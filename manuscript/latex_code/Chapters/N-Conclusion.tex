
\chapter{Conclusion}

\label{ChapterConclusion} 

% Conclusion
% - Résumer les contributions (ce qui était intéressant)
% - Discussions (limites)
% - Perspectives (~3 projets futurs)

% about the fact that we need to transmit knwoledge and reasoning abilities to artificial agents: they cannot learn everything by themselves efficiently
% the fact that we struggle so much for defining the notion of intelligence should not discourage us from trying. 
% Looking at human intelligence, we can see that an important part of this intelligence, from a collective point of view, relies on our ability to share knowledge and reasoning with each other
% Thus, building artificial agents with high levels of intelligence may require tools for transmitting some of our knowledge and experience to these artificial agents
% Language can be such a tool

\subsubsection{Summary}

Throughout this thesis, we have studied techniques to tackle multi-agent robotic problems with deep reinforcement learning. Being anchored in robotics motivates a specific perspective on these learning problems, asking: \textit{how should we design multi-agent deep reinforcement learning algorithms for enabling their applications to robotic settings?} Robotic agents live in complex, dynamic environments, alongside humans and other robots. They need to handle large observation and action spaces, incomplete information about their environment, sparse reward signals, and dynamic settings. Therefore, they require strong representation learning capacities to make sense of the information they receive and the actions they can perform. They need efficient algorithms to learn what behaviour can lead to what outcomes, and how to behave to fulfill their objectives efficiently. They need to generalise their knowledge and experience, to handle changes in their environments and new partners. Finally, for engaging in human-populated environments, they require the right tools for observing, understanding, and interacting with humans. 

Learning such agents with reinforcement learning tools is a challenge. Deep neural networks have allowed expanding the potential of reinforcement learning for training better policies in more complex settings. But, there are still many challenges for reinforcement learners to obtain more general and robust learning abilities. The multi-agent problem is one of them. Having multiple learning agents raises several issues at once, amplifying the limitations of reinforcement learning techniques. To mitigate these issues, research on multi-agent deep reinforcement learning slowly builds a set of tools for improving the training of multi-agent policies and the design of more capable agents. 

To progress in this direction, we first presented an analysis of the field of multi-agent deep reinforcement learning, emphasising its challenges and reviewing its main lines of research. Importantly, we have highlighted the gap between state-of-the-art algorithms and their practical applications in robotics. Because of the great challenges faced when learning multi-agent policies, works in the field often neglect real-world complexities. While this is inevitable for building our way to more capable algorithms, we may also benefit from rethinking some of our approaches and try embracing some of the issues we face. We have identified four crucial matters: benchmarking, exploration, generalisation, and interaction; that all should be addressed for improving the research domain, the algorithms, and the agents that result from them. 

Following this, we have contributed two new methods for improving multi-agent learning. First, by tackling the problem of multi-agent exploration. Similar to previous works, we showed that state-of-the-art multi-agent deep reinforcement learning algorithms suffer from the problem of relative overgeneralisation. Because of poor exploration strategies, they are unable to efficiently find optimal joint policies that require strong coordination of agents to be revealed. To tackle this issue, we introduced the Joint Intrinsic Motivation method to incite agents to explore the space of joint observations. This method demonstrated an important idea: local exploration is inefficient in multi-agent settings because outcomes depend on multiple agents. However, as we have shown, actively exploring the joint-observation space helps for finding new coordinated behaviours. This should motivate further study of joint exploration strategies. 

Finally, we tackled the problem of multi-agent communication. Communication is a crucial skill for agents to face the challenges of multi-agent settings. For human beings, it enables the sharing of local information, the expression of intents, and the transmission of knowledge. We proposed to take inspiration from humans by learning to communicate with a pre-defined language, allowing efficient information transmission and communicating with unknown agents and humans. We intentionally defined agents that learn language during their training, as opposed to using pre-trained language modules, allowing language training to also serve as guidance for learning better representations of meaning from the world. With these language-augmented agents, we gain important qualities for (i) improving learning: by learning faster how the world works, how to communicate efficiently, and how to adapt to changes in the environment; and (ii) improving communication: by allowing communication with new partners and human-agent interactions. 
% Overall, we advocate for more language-augmented agents in multi-agent settings, as a way to improv




\subsubsection{Discussions}

Working and experimenting with deep reinforcement learning methods consistently shows that human-like intelligence, as in reasoning and behaving like a human being would, does not naturally emerge from reinforcement learning training, at least with the tools we have at our disposal. 
Often, we undoubtedly expect agents to converge towards some logical (to us) behaviour, and remain confused by the results of training. \textit{Why would they not go directly to the goal, when it provides them with a great reward?}
We can find two sound examples of this in the present work:
\begin{itemize}
    \item In environments with sparse rewards, we expect agents to quickly recognise the states associated with positive rewards, and we often struggle to predict how hard a given task will be. This is the case with the problem of relative overgeneralisation, where the multi-agent settings exacerbates the sparsity of positive reward signals by requiring coordinated exploration to discover them. 
    \item When training agents with differentiable emergent communication, we can expect them to learn to communicate about elements important for solving the task, to discard redundant or irrelevant information, and to listen to incoming information for improving their policy; experiments show that learning to maximise returns is insufficient to guarantee such features. 
\end{itemize}

Once the unexpected outcomes have been observed, logical explanations often arise by themselves. For sparse environments, one explanation is that reinforcement learning is inefficient. It needs to observe a certain outcome many times to learn efficiently from it. Intrinsic motivation can help by guiding the agents to develop a more active exploratory strategy. But, this is limited as it does not teach agents the concept of exploration -- what it is, when it should be used --, it only induces some behaviour change by essentially modifying the task. To go beyond this, techniques like goal-directed RL and hierarchical learning could help build agents that can switch their strategy momentarily to explore more when required. 

For investigating the failures of differentiable emergent communication, looking at language evolution provides some explanations, as explained in Section~\ref{sec:LAMAC:RW_LanguageDef}. Note that, while we argued that learning language was a preferable solution for multi-agent communication, it should not be seen as a magical solution for teaching communication to reinforcement learning agents. In the algorithm we proposed in Chapter~\ref{ChapterComm}, learning of how information coming from other agents should be used for action-selection is still entrusted to the reinforcement learning algorithm. Thus, we do not control fully how messages will be used. For example, agents may learn to ignore the incoming information -- which was not the case in our setting, but has been observed previously~\citep{Jaques2019_SocialInfluence}. To gain further control over the training of reinforcement learners, we may expand our use of language grounding by adding other auxiliary learning objectives. 


\subsubsection{Future Works}

Studying the fields of multi-agent learning, deep reinforcement learning, robotics, and language at the same time feels both daunting and exciting. These domains are rich of concepts, techniques, and experiments, that should all be understood to be used, merged, and reproduced for advancing the state of research. But, at the same time, at the intersection of these fields appear many avenues for improvements and new approaches to shape the capabilities of agents learning in groups. Looking into each problem sparks the desire to dig deeper and contribute to the collective progress. In future works, there are three main directions I would like to pursue. 

% Expanding language-based agents in social settings
First, because communication is such an important skill in multi-agent interactions, and language plays such a big part in human learning, we should continue working on language-augmented agents in social settings. As described in Section~\ref{sec:LAMAC:RW_LARL}, language can play a central role in human learning, as a way to understand our world and internalise observed behaviour in social interactions. To fully enable this kind of learning in artificial agents, many extensions to our work may be envisioned. Agents should be able to communicate about their actions. They should use their language abilities to learn from their interactions with the world and other agents. Fulfilling these requirements will require thorough studies and experiments on how language can be used to augment learning agents. 



% Hierarchical agents with language
Second, a problem that we have not tackled during this thesis is the learning of hierarchical policies for allowing to reason on different levels of abstraction. Hierarchical agent architectures allow dissociating the learning of high-level strategy -- \textit{what goal should I pursue at the moment?} -- and low-level action-selection -- \textit{given my current objective, how should I command my actuators?}. This has been widely studied in the single-agent case~\citep{Pateria2021_HRLSurvey}, but is also very relevant to multi-agent settings for handling complex social interactions. This can also be related to language learning. Language is used to describe high-level concepts (e.g., objects) that arise from low-level signals (e.g., sensor data). Investigating the mechanisms of these two levels of abstraction separately and their different roles in learning and behaving can help us design more capable social agents. 



% Models in multi-agent learning
Lastly, I believe that model-based multi-agent reinforcement learning should be investigate further. We humans constantly use models of the world and other entities to decide what we want to do -- \textit{what actions should I perform?} --, to understand the situations we are in --\textit{what led to the present state?} --, our role or possibilities in these situations -- \textit{what goal can I, and should I pursue?} --, and to learn from our experience -- \textit{what should I have done differently?}. Of course, all these questions are central in our learning and conducting of social interactions. Thus, artificial agents would benefit from having similar capacities to handle multi-agent learning. The reason why they are mostly not equipped with models is technical: it is extremely difficult to model complex, dynamic systems like multi-agent environments. However, despite this difficulty, the intuitive and observed benefits of models should still motivate us to develop model-based multi-agent approaches. Agent modelling techniques, presented in Section~\ref{sec:MADRL:AgentModelling}, are a step towards this objective. The great progress of single-agent model-based RL, provided by deep learning techniques~\citep{Hafner2023_DreamerV3}, should also be a source of inspiration for experimenting with models in social environments. 


As always, these three avenues of research, and their potential intersections, pave the way for compelling problems to solve, challenges to overcome, and, hopefully, some interesting results along the way. So, let us get to work!



% Finding the source of these misconceptions is often hard, and is not helped by the numerous layers of techniques used in modern deep reinforcement learning. 
% There may always be some explanation hidden somewhere in the gradients. But, there may also be many misguiding assumptions made by researchers. Because the reinforcement learning process is conceptually so similar to learning found in nature, and because we manipulate learning tools that supposedly emulate the mechanisms of our brain -- very poorly, as we all know, but still the semantic has been established this way --, we might take for granted that a seemingly simple problem should be solved accordingly easily. But, as we observe so often, this is not the case. A first step towards understanding this should be to fight our cognitive biases and reiterate that deep reinforcement learning agents, as any deep learning model, have little in common with any form of natural intelligence. While this can sound obvious, this is, in my opinion, an important thing to bear in mind, especially at the age of models trained to replicate human-generated text, images, sound, and videos. Failing to do so may both  

% There is probably a cognitive bias at play when researching and experimenting with RL. We, as humans, are used to learning to solve problems in a way that is very similar to the process of RL. We do this naturally, without thinking about the cognitive processes at play. So, we take for granted that a seemingly simple problem should be solved accordingly easily by a deep RL algorithm. 
% But multiple issues may prevent this from happening without struggle.

% Various algorthmic issues limit RL methods to work better. The problem of exploration that we tackled is one of them. As stated multiple times, RL algorithms have a crucial need for numerous examples of what behaving in their environment looks like. Only with exhaustive knowledge of the environment, will a RL algorithm be able to learn the truly optimal policy. But, lacking some knowledge may end up in catastrophic failures when faced with unseen environmental configurations. Combining this with unavoidable technical limitations of RL training requires agents to learn efficiently during a limited time-frame, or fail completely. This is what is observed in hard exploration problems, where otherwise good RL algorithms fail to find any form of solution and usually end up doing nothing or acting randomly. To succeed in such cases, exploration must be promoted as an explicit goal of the agent. While exploration is a central problem in single-agent RL research, it tends to be set aside in the multi-agent case, in favour of more canonical multi-agent problem such as credit assignment. This is an issue as the problem of exploration remains in the mutli-agent case, expands to new forms, and, as we have shown, exploring locally does not necessarily mean exploring jointly. In this context, intrinsic motivation techniques have a great potential to incite exploratory behaviour. However, they also have important limitations that must be considererd. Adding a reward effectively modifies the initial MDP, which may wrongfully uplift suboptimal policies. In the context of exploration, this is theoretically alleviated by the fact that novelty rewards will mechanically fade as the policy space is explored. But, this also leads to another issue: as the exploration drive eventually dissipates, the agents will progressively transition to less exploratory behaviours, ultimately forgetting the exploration strategy completely. If the agents did not manage to find the optimal strategy during the exploration time-frame, they will likely get stuck in suboptimal behaviour -- which is what we observed in [ChapterJIM], with some runs trained with JIM that did not manage to find the optimal strategy. In other words, such novelty-based intrinsic motivation methods do not produce agents that "know exploration", as in they would know when to explore and what to do if they need to explore. Instead, they only increase the chances of solving the hard exploration problem by boosting exploration during training. To go beyond this, techniques like goal-directed RL and hierarchical learning could help building agents that are able to switch their strategy momentarily to explore more when required. 

% Paradigm issues (problems with neural networks and learning from gradients descent -> can help to induce more knowledge in the agents) 
% we struggle to understand and comprehend how neural networks actually learn

% Conceptual issues (what are our expectations with artificial agents? how do we measure their intelligence?)
% is it only by performance on a given task or benchmark ?
% research shows that intelligence is made of many interleaving aspects
% -> need to go towards embodiment

% JIM requires CTDE: there might be a larger issue behind this. For now, in most algorithm, we don't have tools for agents to view themselves as part of a group. Thus, decentralised agents have no way of internalising the fact that the world is moved by other forces than its own actions. 
% there may be a need for a push of model-based approaches in the multi-agent settings. Learning model-free agents allows learning the best actions to perform for some given objective, and the value of the related behaviour. This is a task-centered approach, that only reason about behaviour and its results. 
% By learning a model of the world, intelligent beings can attempt at understanding what goes on beyond their own action power. This can help learning better behaviour, by enabling better understanding of outcomes and their causes, or by allowing explicit planning of future outcomes. More crucially, this can enable agents to have a sense of the other, as in everything that is not in their control. By learning to understand how the environment functions, and by modelling how other agents behave, what are their current objectives, and what can we expect of their future actions. 
% Coming back to the joint exploration problem, model-free agents can only be guided to explore more from various rewards supplements, without internalising the actual mission they are incited to take part in. By learning models, agents may be able to understand what exploration means for them, how it is currently being carried by the whole multi-agent system, and what future actions will maximise the explorative success of the group.
% Of course, there is a simple explanation to why there is so little model-based approaches in mutli-agent reinforcement learning: learning to model a dynamic, non-stationary, partially observable environment filled with various intelligent beings is extremely difficult. The number of different variables to account for is massive, and the non-stationarity prevents any given model to be confident about its future predictions. 
% But, despite these difficulties, the intuive and observed befenits of models should still motivate us from trying to develop model-based multi-agent approaches. Agent modelling techniques, presented in Section~\ref{}, are a step towards this objective. They allow agents to reason about other agents' behaviour, to allow better action-selection.
% Deep learning will probably be a major tool for learning models conditioned on such variables. Model-based deep RL has made great advancements in the past years, showing how these techniques allow the emergence of complex behaviour~\citep{minecraft dream}. Investigating these approaches for multi-agent contexts should be a priority


% \subsubsection{Future works}

