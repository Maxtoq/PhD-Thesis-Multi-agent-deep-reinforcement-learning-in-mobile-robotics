% Encoding: UTF-8

@InProceedings{NguyenKL18,
  author    = {Duc Thien Nguyen and Akshat Kumar and Hoong Chuin Lau},
  booktitle = {NeurIPS},
  title     = {Credit Assignment For Collective Multiagent RL With Global Rewards},
  year      = {2018},
  pages     = {8113-8124},
  cdate     = {1514764800000},
  crossref  = {conf/nips/2018},
  file      = {:NguyenKL18 - Credit Assignment for Collective Multiagent RL with Global Rewards.pdf:PDF},
  groups    = {Multi-agent RL, Credit Assignment},
  url       = {http://papers.nips.cc/paper/8033-credit-assignment-for-collective-multiagent-rl-with-global-rewards},
}

@InProceedings{AndrychowiczCRS17,
  author    = {Marcin Andrychowicz and Dwight Crow and Alex Ray and Jonas Schneider and Rachel Fong and Peter Welinder and Bob McGrew and Josh Tobin and Pieter Abbeel and Wojciech Zaremba},
  booktitle = {NIPS},
  title     = {Hindsight Experience Replay},
  year      = {2017},
  pages     = {5055-5065},
  cdate     = {1483228800000},
  crossref  = {conf/nips/2017},
  file      = {:AndrychowiczCRS17 - Hindsight Experience Replay.pdf:PDF},
  groups    = {Robotics},
  ranking   = {rank4},
  url       = {http://papers.nips.cc/paper/7090-hindsight-experience-replay},
}

@Article{Haarnoja2018,
  author        = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  year          = {2018},
  month         = jan,
  abstract      = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  comment       = {Soft Actor Critic
- PROBLEM:
	Model-free DRL algos typically suffer from two major challenges: very high sample complexity and brittle convergence properties (=> meticulous hyperparameter tuning)
- SOLUTION:
	Soft Actor-Critic: based on Maximum Entropy RL framework: maximize expected reward while also maximizing entropy = succeed at the task while acting as randomly as possible
- MODEL:
	Double DQN to mitigate overestimation of Q values
	Target network for policy and Q-functions for more stable training
- RESULTS:
	Outperforms sota model-free DRL methods (DDPG and PPO)
	Very stable accross different random seeds},
  eprint        = {1801.01290},
  file          = {:Haarnoja2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF},
  groups        = {Policy based, Soft Actor Critic, Actor Critic},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Fujimoto2018,
  author        = {Scott Fujimoto and Herke van Hoof and David Meger},
  title         = {Addressing Function Approximation Error in Actor-Critic Methods},
  year          = {2018},
  month         = feb,
  abstract      = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Overestimation of Q-values estimates leading to suboptimal policies, even in DDPG
- SOLUTION:
	TD3: Double DQN and Clipped Double-Q trick
	Target networks => more stable training
	Delayed Policy Updates => more stable
	Target Policy smoothing: add noise to choses action  => mitigates overestimation of Q-values
- RESULTS:
	Reduces overestimation of Q-values
	Outperforms all previous policy-based algorithms},
  eprint        = {1802.09477},
  file          = {:Fujimoto2018 - Addressing Function Approximation Error in Actor Critic Methods.pdf:PDF},
  groups        = {Policy based, DDPG, Actor Critic},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
  readstatus    = {read},
}

@Article{Schulman2017,
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  comment       = {PPO},
  eprint        = {1707.06347},
  file          = {:Schulman2017 - Proximal Policy Optimization Algorithms.pdf:PDF},
  groups        = {Policy based, TRPO, Actor Critic},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Schulman2015,
  author        = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  title         = {Trust Region Policy Optimization},
  year          = {2015},
  month         = feb,
  abstract      = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  comment       = {TRPO},
  eprint        = {1502.05477},
  file          = {:Schulman2015 - Trust Region Policy Optimization.pdf:PDF},
  groups        = {Policy based, TRPO, Actor Critic},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Schrittwieser2019,
  author        = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
  title         = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  year          = {2019},
  month         = nov,
  abstract      = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  comment       = {Muzero},
  doi           = {10.1038/s41586-020-03051-4},
  eprint        = {1911.08265},
  file          = {:Schrittwieser2019 - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hafner2019,
  author        = {Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
  title         = {Dream to Control: Learning Behaviors by Latent Imagination},
  year          = {2019},
  month         = dec,
  abstract      = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  archiveprefix = {arXiv},
  comment       = {Dreamer},
  eprint        = {1912.01603},
  file          = {:Hafner2019 - Dream to Control_ Learning Behaviors by Latent Imagination.pdf:PDF},
  groups        = {Model based, Latent Dynamics Model},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Pathak2017,
  author        = {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  title         = {Curiosity-driven Exploration by Self-supervised Prediction},
  year          = {2017},
  month         = may,
  abstract      = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  archiveprefix = {arXiv},
  comment       = {Intrinsinc Curiosity Module},
  eprint        = {1705.05363},
  file          = {:Pathak2017 - Curiosity Driven Exploration by Self Supervised Prediction.pdf:PDF},
  groups        = {Intrinsic goals, Curiosity},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Burda2018,
  author        = {Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
  title         = {Exploration by Random Network Distillation},
  year          = {2018},
  month         = oct,
  abstract      = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  archiveprefix = {arXiv},
  eprint        = {1810.12894},
  file          = {:Burda2018 - Exploration by Random Network Distillation.pdf:PDF},
  groups        = {Intrinsic goals, Curiosity},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Lillicrap2015,
  author        = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  title         = {Continuous control with deep reinforcement learning},
  year          = {2015},
  month         = sep,
  abstract      = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Adapt DQN to continuous action domain
- SOLUTION:
	Combine Actor-Critic with the recent successes of DQN
	Policy network to compute action that maximizes Q
	DQN to learn Q
- RESULTS:
	Achieves learning policy for continuous actions
	Struggles to learn from raw pixels somtimes
	Overestimation of Q values sometimes},
  eprint        = {1509.02971},
  file          = {:Lillicrap2015 - Continuous Control with Deep Reinforcement Learning.pdf:PDF},
  groups        = {Policy based, DDPG, Actor Critic},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{Williams1992,
  author     = {Ronald J. Williams},
  title      = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  year       = {1992},
  comment    = {Reinforce},
  doi        = {10.1007/978-1-4615-3618-5_2},
  file       = {:Williams1992_Article_SimpleStatisticalGradient-foll.pdf:PDF},
  groups     = {Policy based},
  pages      = {5-32},
  readstatus = {read},
}

@Article{Zhang2018,
  author        = {Marvin Zhang and Sharad Vikram and Laura Smith and Pieter Abbeel and Matthew J. Johnson and Sergey Levine},
  title         = {SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning},
  year          = {2018},
  month         = aug,
  abstract      = {Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.},
  archiveprefix = {arXiv},
  eprint        = {1808.09105},
  file          = {:Zhang2018 - SOLAR_ Deep Structured Representations for Model Based Reinforcement Learning.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Article{Sekar2020,
  author        = {Ramanan Sekar and Oleh Rybkin and Kostas Daniilidis and Pieter Abbeel and Danijar Hafner and Deepak Pathak},
  title         = {Planning to Explore via Self-Supervised World Models},
  year          = {2020},
  month         = may,
  abstract      = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Find learning algorithms that are sample efficient and not task specific.
- SOLUTION:
	Plan2Explore: self-supervised exploration and fast adaptation to new tasks. Instead of maximizing an instinsic reward in retrospect, it learns a world model to plan ahead and seek novelty in future situations.
- MODEL:
	Encode images with CNN
	Learn a world model with PlaNet
	Learn policy and value with Dreamer
	Induce exploration by generating an intrinsic reward using ensemble disagreement, like Pathak2019
	No reward from the environment during learning the model
	After learning the world model through exploration, adapts on tasks in zero-shot (only imagination) or few-shot (imagination and few iteraction)},
  eprint        = {2005.05960},
  file          = {:Sekar2020 - Planning to Explore Via Self Supervised World Models.pdf:PDF},
  groups        = {Model based, Disagreement, Model-based exploration, Intrinsic goals},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.NE, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Pathak2019,
  author        = {Deepak Pathak and Dhiraj Gandhi and Abhinav Gupta},
  title         = {Self-Supervised Exploration via Disagreement},
  year          = {2019},
  month         = jun,
  abstract      = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/},
  archiveprefix = {arXiv},
  eprint        = {1906.04161},
  file          = {:Pathak2019 - Self Supervised Exploration Via Disagreement.pdf:PDF},
  groups        = {Disagreement, Intrinsic goals},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hessel2017,
  author        = {Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
  title         = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  year          = {2017},
  month         = oct,
  abstract      = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archiveprefix = {arXiv},
  eprint        = {1710.02298},
  file          = {:Hessel2017 - Rainbow_ Combining Improvements in Deep Reinforcement Learning.pdf:PDF},
  groups        = {Value based},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
}

@Article{Vecerik2017,
  author        = {Mel Vecerik and Todd Hester and Jonathan Scholz and Fumin Wang and Olivier Pietquin and Bilal Piot and Nicolas Heess and Thomas Rothörl and Thomas Lampe and Martin Riedmiller},
  title         = {Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sampling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Normalized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kinesthetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.},
  archiveprefix = {arXiv},
  eprint        = {1707.08817},
  file          = {:Vecerik2017 - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards.pdf:PDF},
  groups        = {DDPG, Robotics, Policy based, Actor Critic},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Eysenbach2018,
  author        = {Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
  title         = {Diversity is All You Need: Learning Skills without a Reward Function},
  year          = {2018},
  month         = feb,
  abstract      = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  archiveprefix = {arXiv},
  eprint        = {1802.06070},
  file          = {:Eysenbach2018 - Diversity Is All You Need_ Learning Skills without a Reward Function.pdf:PDF},
  groups        = {Diversity, Intrinsic goals},
  keywords      = {cs.AI, cs.RO},
  primaryclass  = {cs.AI},
  readstatus    = {skimmed},
}

@Article{Burda2018a,
  author        = {Yuri Burda and Harri Edwards and Deepak Pathak and Amos Storkey and Trevor Darrell and Alexei A. Efros},
  title         = {Large-Scale Study of Curiosity-Driven Learning},
  year          = {2018},
  month         = aug,
  abstract      = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  archiveprefix = {arXiv},
  eprint        = {1808.04355},
  file          = {:Burda2018a - Large Scale Study of Curiosity Driven Learning.pdf:PDF},
  groups        = {Curiosity, Intrinsic goals},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Achiam2017,
  author        = {Joshua Achiam and Shankar Sastry},
  title         = {Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},
  year          = {2017},
  month         = mar,
  abstract      = {Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.},
  archiveprefix = {arXiv},
  eprint        = {1703.01732},
  file          = {:Achiam2017 - Surprise Based Intrinsic Motivation for Deep Reinforcement Learning.pdf:PDF},
  groups        = {Curiosity, Intrinsic goals},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Article{Ha2018,
  author        = {David Ha and Jürgen Schmidhuber},
  title         = {World Models},
  year          = {2018},
  month         = mar,
  abstract      = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Human base their decisions and actions on a mental model of the world, itself based on their senses and predictions of the future.
	RL algos often use small NNs because they iterate faster to learn a good policy, but they would benefit from large RNNs that learn rich spatial and temporal representations of data
- SOLUTION:
	Large RNN to learn a world model in an unsupervised manner
	Small controller to learn to perform task in this world model
- MODEL: 
	VAE to learn abstract, compressed latent representation of obseverd image frame
	Mixture Density Network-RNN to predict future latent representation: model probability density of next latent state (as a mixture of Gaussian distribution) based on current action, latent state and hidden state of the RNN
	Very simple controller trained separately in latent imagination: single layer linear model to map latent state and hidden state to action
- RESULTS:
	Demonstrate the possibility of training agents entirely inside of its simulated latent space dream world},
  doi           = {10.5281/zenodo.1207631},
  eprint        = {1803.10122},
  file          = {:Ha2018 - World Models.pdf:PDF},
  groups        = {Model based, Latent Dynamics Model},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hafner2018,
  author        = {Danijar Hafner and Timothy Lillicrap and Ian Fischer and Ruben Villegas and David Ha and Honglak Lee and James Davidson},
  title         = {Learning Latent Dynamics for Planning from Pixels},
  year          = {2018},
  month         = nov,
  abstract      = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
  archiveprefix = {arXiv},
  comment       = {- MODEL:
	PlaNet: Deep Planning Network, a model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space
- RESULTS:
	Beat A3C and sometimes D4PG
	Way more sample efficient (200x)},
  eprint        = {1811.04551},
  file          = {:Hafner2018 - Learning Latent Dynamics for Planning from Pixels.pdf:PDF},
  groups        = {Model based, Latent Dynamics Model},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Article{Shyam2018,
  author        = {Pranav Shyam and Wojciech Jaśkowski and Faustino Gomez},
  title         = {Model-Based Active Exploration},
  year          = {2018},
  month         = oct,
  abstract      = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Over-commitment: intrinsic exploration bonus have to be unlearned once the novelty of a state's vicinity has worn off, making exploration inefficient
- SOLUTION:
	Model-based Active eXploration (MAX): actively seek out novelty in future states by measuring the amount of conflict between predictions of an ensemble of forward models
- RESULTS:
	Active exploration prevents from getting stuck in a local optimum
	Model-based methods suffer from model-bias: bad model in certain regions of the state space leading to bad policy. MAX would explore more difficult aspects of the environment, thereby improving the quality of the models
	Less computationally efficient than baselines, BUT trading off for data efficiency},
  eprint        = {1810.12162},
  file          = {:Shyam2018 - Model Based Active Exploration.pdf:PDF},
  groups        = {Model-based exploration, Model based, Disagreement},
  keywords      = {cs.LG, cs.AI, cs.IT, cs.NE, math.IT, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Article{Lee2019,
  author        = {Alex X. Lee and Anusha Nagabandi and Pieter Abbeel and Sergey Levine},
  title         = {Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model},
  year          = {2019},
  month         = jul,
  abstract      = {Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	It is difficult to learn directly from high-dimensional image inputs (task learning)
	It is difficult to extract compact representations of the underlying task-relevant information from which to learn (representation learning)
- SOLUTION:
	Treat task learning and representation learning separately: learn a latent representation space with a predictive model, and train a RL agent in that learning latent space
- MODEL:
	Maximize ELBO (model objective + maximum entropy RL objective)
	Sequential latent variable model objective: predict next latent state based on next state, current latent state and current action
	RL objective: Soft Actor-Critic
- RESULTS: 
	Better performance than several sota model-free and model-based approaches
	More sample-efficient},
  eprint        = {1907.00953},
  file          = {:Lee2019 - Stochastic Latent Actor Critic_ Deep Reinforcement Learning with a Latent Variable Model (1).pdf:PDF},
  groups        = {Model based, POMDP, Soft Actor Critic, Latent Dynamics Model},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Zhu2018,
  author        = {Pengfei Zhu and Xin Li and Pascal Poupart and Guanghui Miao},
  title         = {On Improving Deep Reinforcement Learning for POMDPs},
  year          = {2018},
  month         = apr,
  abstract      = {Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.},
  archiveprefix = {arXiv},
  eprint        = {1804.06309},
  file          = {:- On Improving Deep Reinforcement Learning for POMDPs.pdf:PDF},
  groups        = {POMDP},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Bellemare2016,
  author        = {Marc G. Bellemare and Sriram Srinivasan and Georg Ostrovski and Tom Schaul and David Saxton and Remi Munos},
  title         = {Unifying Count-Based Exploration and Intrinsic Motivation},
  year          = {2016},
  month         = jun,
  abstract      = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
  archiveprefix = {arXiv},
  eprint        = {1606.01868},
  file          = {:Bellemare2016 - Unifying Count Based Exploration and Intrinsic Motivation.pdf:PDF},
  groups        = {Visitation count},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
}

@Article{Tampuu2015,
  author        = {Ardi Tampuu and Tambet Matiisen and Dorian Kodelja and Ilya Kuzovkin and Kristjan Korjus and Juhan Aru and Jaan Aru and Raul Vicente},
  title         = {Multiagent Cooperation and Competition with Deep Reinforcement Learning},
  year          = {2015},
  month         = nov,
  abstract      = {Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.},
  archiveprefix = {arXiv},
  eprint        = {1511.08779},
  file          = {:Tampuu2015 - Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:PDF},
  groups        = {MARL, Multi-agent RL},
  keywords      = {cs.AI, cs.LG, q-bio.NC},
  primaryclass  = {cs.AI},
}

@Article{Wang2019,
  author        = {Yixiang Wang and Feng Wu},
  title         = {Multi-Agent Deep Reinforcement Learning with Adaptive Policies},
  year          = {2019},
  month         = nov,
  abstract      = {We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution. We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments.},
  archiveprefix = {arXiv},
  eprint        = {1912.00949},
  file          = {:Wang2019 - Multi Agent Deep Reinforcement Learning with Adaptive Policies.pdf:PDF},
  groups        = {MARL, Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Bard2020,
  author    = {Nolan Bard and Jakob N. Foerster and Sarath Chandar and Neil Burch and Marc Lanctot and H. Francis Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and Hugo Larochelle and Marc G. Bellemare and Michael Bowling},
  journal   = {Artif. Intell.},
  title     = {The Hanabi challenge: {A} new frontier for {AI} research},
  year      = {2020},
  pages     = {103216},
  volume    = {280},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ai/BardFCBLSPDMHDM20.bib},
  doi       = {10.1016/j.artint.2019.103216},
  file      = {:Bard2020 - The Hanabi Challenge_ a New Frontier for AI Research.pdf:PDF},
  groups    = {MARL, Multi-agent learning},
}

@Article{Long2020,
  author        = {Qian Long and Zihan Zhou and Abhibav Gupta and Fei Fang and Yi Wu and Xiaolong Wang},
  title         = {Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning},
  year          = {2020},
  month         = mar,
  abstract      = {In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely, EPC maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets of agents with the best adaptability to the next stage. We implement EPC on a popular MARL algorithm, MADDPG, and empirically show that our approach consistently outperforms baselines by a large margin as the number of agents grows exponentially.},
  archiveprefix = {arXiv},
  eprint        = {2003.10423},
  file          = {:Long2020 - Evolutionary Population Curriculum for Scaling Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {MARL, Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.NE, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Li,
  author  = {Shihui Li and Yi Wu and Xinyue Cui and Honghua Dong and Fei Fang and Stuart Russell},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title   = {Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient},
  year    = {2019},
  issn    = {2374-3468},
  pages   = {4213-4220},
  volume  = {33},
  comment = {M3DDPG},
  doi     = {10.1609/aaai.v33i01.33014213},
  file    = {:Li - Robust Multi Agent Reinforcement Learning Via Minimax Deep Deterministic Policy Gradient.pdf:PDF},
  groups  = {MARL, Multi-agent RL},
  ranking = {rank2},
}

@Article{Hu2021,
  author        = {Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},
  title         = {UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers},
  year          = {2021},
  month         = jan,
  abstract      = {Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).},
  archiveprefix = {arXiv},
  eprint        = {2101.08001},
  file          = {:Hu2021 - UPDeT_ Universal Multi Agent Reinforcement Learning Via Policy Decoupling with Transformers.pdf:PDF},
  groups        = {MARL, Multi-agent RL},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@Article{Rafailov2020,
  author        = {Rafael Rafailov and Tianhe Yu and Aravind Rajeswaran and Chelsea Finn},
  title         = {Offline Reinforcement Learning from Images with Latent Space Models},
  year          = {2020},
  month         = dec,
  abstract      = {Offline reinforcement learning (RL) refers to the problem of learning policies from a static dataset of environment interactions. Offline RL enables extensive use and re-use of historical datasets, while also alleviating safety concerns associated with online exploration, thereby expanding the real-world applicability of RL. Most prior work in offline RL has focused on tasks with compact state representations. However, the ability to learn directly from rich observation spaces like images is critical for real-world applications such as robotics. In this work, we build on recent advances in model-based algorithms for offline RL, and extend them to high-dimensional visual observation spaces. Model-based offline RL algorithms have achieved state of the art results in state based tasks and have strong theoretical guarantees. However, they rely crucially on the ability to quantify uncertainty in the model predictions, which is particularly challenging with image observations. To overcome this challenge, we propose to learn a latent-state dynamics model, and represent the uncertainty in the latent space. Our approach is both tractable in practice and corresponds to maximizing a lower bound of the ELBO in the unknown POMDP. In experiments on a range of challenging image-based locomotion and manipulation tasks, we find that our algorithm significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods. Moreover, we also find that our approach excels on an image-based drawer closing task on a real robot using a pre-existing dataset. All results including videos can be found online at https://sites.google.com/view/lompo/ .},
  archiveprefix = {arXiv},
  eprint        = {2012.11547},
  file          = {:Rafailov2020 - Offline Reinforcement Learning from Images with Latent Space Models.pdf:PDF},
  groups        = {Latent Dynamics Model, Offline RL},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
}

@Article{Laskin2020,
  author        = {Michael Laskin and Kimin Lee and Adam Stooke and Lerrel Pinto and Pieter Abbeel and Aravind Srinivas},
  title         = {Reinforcement Learning with Augmented Data},
  year          = {2020},
  month         = apr,
  abstract      = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
  archiveprefix = {arXiv},
  eprint        = {2004.14990},
  file          = {:Laskin2020 - Reinforcement Learning with Augmented Data.pdf:PDF},
  groups        = {Data Augmentation},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Kostrikov2020,
  author        = {Ilya Kostrikov and Denis Yarats and Rob Fergus},
  title         = {Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels},
  year          = {2020},
  month         = apr,
  abstract      = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  eprint        = {2004.13649},
  file          = {:Kostrikov2020 - Image Augmentation Is All You Need_ Regularizing Deep Reinforcement Learning from Pixels.pdf:PDF},
  groups        = {Data Augmentation},
  keywords      = {cs.LG, cs.CV, eess.IV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Kidambi2020,
  author        = {Rahul Kidambi and Aravind Rajeswaran and Praneeth Netrapalli and Thorsten Joachims},
  title         = {MOReL : Model-Based Offline Reinforcement Learning},
  year          = {2020},
  month         = may,
  abstract      = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.},
  archiveprefix = {arXiv},
  eprint        = {2005.05951},
  file          = {:Kidambi2020 - MOReL _ Model Based Offline Reinforcement Learning.pdf:PDF},
  groups        = {Model based, Offline RL},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Yu2020,
  author        = {Tianhe Yu and Garrett Thomas and Lantao Yu and Stefano Ermon and James Zou and Sergey Levine and Chelsea Finn and Tengyu Ma},
  title         = {MOPO: Model-based Offline Policy Optimization},
  year          = {2020},
  month         = may,
  abstract      = {Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. The code is available at https://github.com/tianheyu927/mopo.},
  archiveprefix = {arXiv},
  eprint        = {2005.13239},
  file          = {:Yu2020 - MOPO_ Model Based Offline Policy Optimization.pdf:PDF},
  groups        = {Model based, Offline RL},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Janner2019,
  author        = {Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
  title         = {When to Trust Your Model: Model-Based Policy Optimization},
  year          = {2019},
  month         = jun,
  abstract      = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  archiveprefix = {arXiv},
  eprint        = {1906.08253},
  file          = {:Janner2019 - When to Trust Your Model_ Model Based Policy Optimization.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Rajeswaran2020,
  author        = {Aravind Rajeswaran and Igor Mordatch and Vikash Kumar},
  title         = {A Game Theoretic Framework for Model Based Reinforcement Learning},
  year          = {2020},
  month         = apr,
  abstract      = {Model-based reinforcement learning (MBRL) has recently gained immense interest due to its potential for sample efficiency and ability to incorporate off-policy data. However, designing stable and efficient MBRL algorithms using rich function approximators have remained challenging. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. For algorithm development, we construct a Stackelberg game between the two players, and show that it can be solved with approximate bi-level optimization. This gives rise to two natural families of algorithms for MBRL based on which player is chosen as the leader in the Stackelberg game. Together, they encapsulate, unify, and generalize many previous MBRL algorithms. Furthermore, our framework is consistent with and provides a clear basis for heuristics known to be important in practice from prior works. Finally, through experiments we validate that our proposed algorithms are highly sample efficient, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation.},
  archiveprefix = {arXiv},
  eprint        = {2004.07804},
  file          = {:Rajeswaran2020 - A Game Theoretic Framework for Model Based Reinforcement Learning.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.AI, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Duan2016,
  author        = {Yan Duan and John Schulman and Xi Chen and Peter L. Bartlett and Ilya Sutskever and Pieter Abbeel},
  title         = {RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning},
  year          = {2016},
  month         = nov,
  abstract      = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  archiveprefix = {arXiv},
  eprint        = {1611.02779},
  file          = {:Duan2016 - RL$^2$_ Fast Reinforcement Learning Via Slow Reinforcement Learning.pdf:PDF},
  groups        = {Meta-Learning},
  keywords      = {cs.AI, cs.LG, cs.NE, stat.ML},
  primaryclass  = {cs.AI},
}

@Article{Mnih2013,
  author        = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  title         = {Playing Atari with Deep Reinforcement Learning},
  year          = {2013},
  month         = dec,
  abstract      = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  eprint        = {1312.5602},
  file          = {:Mnih2013 - Playing Atari with Deep Reinforcement Learning.pdf:PDF},
  groups        = {Deep Q-Network, Value based},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Mnih2015,
  author    = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and Andreas Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  journal   = {Nat.},
  title     = {Human-level control through deep reinforcement learning},
  year      = {2015},
  number    = {7540},
  pages     = {529--533},
  volume    = {518},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/MnihKSRVBGRFOPB15.bib},
  comment   = {DQN},
  doi       = {10.1038/nature14236},
  groups    = {Deep Q-Network, Value based},
}

@InProceedings{Hasselt2016,
  author    = {Hado van Hasselt and Arthur Guez and David Silver},
  booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {USA}},
  title     = {Deep Reinforcement Learning with Double Q-Learning},
  year      = {2016},
  editor    = {Dale Schuurmans and Michael P. Wellman},
  pages     = {2094--2100},
  publisher = {{AAAI} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aaai/HasseltGS16.bib},
  comment   = {Double DQN},
  groups    = {Value based},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389},
}

@InProceedings{Schaul2016,
  author    = {Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  title     = {Prioritized Experience Replay},
  year      = {2016},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/SchaulQAS15.bib},
  groups    = {PER, Value based},
  url       = {http://arxiv.org/abs/1511.05952},
}

@InProceedings{Wang2016,
  author    = {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado van Hasselt and Marc Lanctot and Nando de Freitas},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  year      = {2016},
  editor    = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
  pages     = {1995--2003},
  publisher = {JMLR.org},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/icml/WangSHHLF16.bib},
  groups    = {Dueling DQN, Value based},
  url       = {http://proceedings.mlr.press/v48/wangf16.html},
}

@Article{Conti2017,
  author        = {Edoardo Conti and Vashisht Madhavan and Felipe Petroski Such and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  title         = {Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents},
  year          = {2017},
  month         = dec,
  abstract      = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
  archiveprefix = {arXiv},
  eprint        = {1712.06560},
  file          = {:Conti2017 - Improving Exploration in Evolution Strategies for Deep Reinforcement Learning Via a Population of Novelty Seeking Agents.pdf:PDF},
  groups        = {Diversity},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Ecoffet2019,
  author        = {Adrien Ecoffet and Joost Huizinga and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  title         = {Go-Explore: a New Approach for Hard-Exploration Problems},
  year          = {2019},
  volume        = {abs/1901.10995},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1901-10995.bib},
  eprint        = {1901.10995},
  groups        = {Random Exploration, Imitation},
  url           = {http://arxiv.org/abs/1901.10995},
}

@Article{Ostrovski2017,
  author        = {Georg Ostrovski and Marc G. Bellemare and A{\"{a}}ron van den Oord and R{\'{e}}mi Munos},
  journal       = {CoRR},
  title         = {Count-Based Exploration with Neural Density Models},
  year          = {2017},
  volume        = {abs/1703.01310},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/OstrovskiBOM17.bib},
  eprint        = {1703.01310},
  groups        = {Visitation count},
  url           = {http://arxiv.org/abs/1703.01310},
}

@InProceedings{Bellemare2017,
  author    = {Marc G. Bellemare and Will Dabney and R{\'e}mi Munos},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {A Distributional Perspective on Reinforcement Learning},
  year      = {2017},
  address   = {International Convention Centre, Sydney, Australia},
  editor    = {Doina Precup and Yee Whye Teh},
  month     = {06--11 Aug},
  pages     = {449--458},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  groups    = {Value based},
  pdf       = {http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
  url       = {http://proceedings.mlr.press/v70/bellemare17a.html},
}

@Article{Fortunato2017,
  author        = {Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
  title         = {Noisy Networks for Exploration},
  year          = {2017},
  month         = jun,
  abstract      = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
  archiveprefix = {arXiv},
  eprint        = {1706.10295},
  file          = {:- Noisy Networks for Exploration.pdf:PDF},
  groups        = {Value based},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{FletBerliac2021,
  author        = {Yannis Flet-Berliac and Johan Ferret and Olivier Pietquin and Philippe Preux and Matthieu Geist},
  title         = {Adversarially Guided Actor-Critic},
  year          = {2021},
  month         = feb,
  abstract      = {Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks.},
  archiveprefix = {arXiv},
  eprint        = {2102.04376},
  file          = {:FletBerliac2021 - Adversarially Guided Actor Critic.pdf:PDF},
  groups        = {Adversary Guidance},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Yu2021,
  author        = {Tianhe Yu and Aviral Kumar and Rafael Rafailov and Aravind Rajeswaran and Sergey Levine and Chelsea Finn},
  title         = {COMBO: Conservative Offline Model-Based Policy Optimization},
  year          = {2021},
  month         = feb,
  abstract      = {Model-based algorithms, which learn a dynamics model from logged experience and perform some sort of pessimistic planning under the learned model, have emerged as a promising paradigm for offline reinforcement learning (offline RL). However, practical variants of such model-based algorithms rely on explicit uncertainty quantification for incorporating pessimism. Uncertainty estimation with complex models, such as deep neural networks, can be difficult and unreliable. We overcome this limitation by developing a new model-based offline RL algorithm, COMBO, that regularizes the value function on out-of-support state-action tuples generated via rollouts under the learned model. This results in a conservative estimate of the value function for out-of-support state-action tuples, without requiring explicit uncertainty estimation. We theoretically show that our method optimizes a lower bound on the true policy value, that this bound is tighter than that of prior methods, and our approach satisfies a policy improvement guarantee in the offline setting. Through experiments, we find that COMBO consistently performs as well or better as compared to prior offline model-free and model-based methods on widely studied offline RL benchmarks, including image-based tasks.},
  archiveprefix = {arXiv},
  eprint        = {2102.08363},
  file          = {:Yu2021 - COMBO_ Conservative Offline Model Based Policy Optimization.pdf:PDF},
  groups        = {Offline RL},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
}

@Article{Kumar2020,
  author        = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
  title         = {Conservative Q-Learning for Offline Reinforcement Learning},
  year          = {2020},
  month         = jun,
  abstract      = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  archiveprefix = {arXiv},
  eprint        = {2006.04779},
  file          = {:Kumar2020 - Conservative Q Learning for Offline Reinforcement Learning.pdf:PDF},
  groups        = {Offline RL},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Silver2016,
  author    = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  journal   = {Nat.},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  year      = {2016},
  number    = {7587},
  pages     = {484--489},
  volume    = {529},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  comment   = {AlphaGo},
  doi       = {10.1038/nature16961},
  file      = {:Silver2016 - Mastering the Game of Go with Deep Neural Networks and Tree Search.pdf:PDF},
  groups    = {Model based},
}

@Article{Silver2017,
  author    = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy P. Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  journal   = {Nat.},
  title     = {Mastering the game of Go without human knowledge},
  year      = {2017},
  number    = {7676},
  pages     = {354--359},
  volume    = {550},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/SilverSSAHGHBLB17.bib},
  comment   = {AlphaGo Zero},
  doi       = {10.1038/nature24270},
  file      = {:Silver2017 - Mastering the Game of Go without Human Knowledge.pdf:PDF},
  groups    = {Model based},
}

@Article{Silver2017a,
  author        = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy P. Lillicrap and Karen Simonyan and Demis Hassabis},
  journal       = {CoRR},
  title         = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  year          = {2017},
  volume        = {abs/1712.01815},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  comment       = {AlphaZero},
  eprint        = {1712.01815},
  file          = {:Silver2017a - Mastering Chess and Shogi by Self Play with a General Reinforcement Learning Algorithm.pdf:PDF},
  groups        = {Model based},
  url           = {http://arxiv.org/abs/1712.01815},
}

@Article{Schulman2015a,
  author        = {John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
  journal       = {ICLR 2016},
  title         = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  year          = {2015},
  month         = jun,
  abstract      = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archiveprefix = {arXiv},
  comment       = {General Advantage Estimator},
  eprint        = {1506.02438},
  file          = {:Schulman2015a - High Dimensional Continuous Control Using Generalized Advantage Estimation.pdf:PDF},
  groups        = {TRPO},
  keywords      = {cs.LG, cs.RO, cs.SY},
  primaryclass  = {cs.LG},
}

@Article{Heess2017,
  author        = {Nicolas Heess and Dhruva TB and Srinivasan Sriram and Jay Lemmon and Josh Merel and Greg Wayne and Yuval Tassa and Tom Erez and Ziyu Wang and S. M. Ali Eslami and Martin Riedmiller and David Silver},
  title         = {Emergence of Locomotion Behaviours in Rich Environments},
  year          = {2017},
  month         = jul,
  abstract      = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx_bgoTF7bs .},
  archiveprefix = {arXiv},
  eprint        = {1707.02286},
  file          = {:Heess2017 - Emergence of Locomotion Behaviours in Rich Environments.pdf:PDF},
  groups        = {TRPO},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Leibo2019,
  author        = {Joel Z. Leibo and Edward Hughes and Marc Lanctot and Thore Graepel},
  title         = {Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research},
  year          = {2019},
  month         = mar,
  abstract      = {Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.},
  archiveprefix = {arXiv},
  eprint        = {1903.00742},
  file          = {:Leibo2019 - Autocurricula and the Emergence of Innovation from Social Interaction_ a Manifesto for Multi Agent Intelligence Research.pdf:PDF},
  groups        = {Multi-agent Systems},
  keywords      = {cs.AI, cs.GT, cs.MA, cs.NE, q-bio.NC},
  primaryclass  = {cs.AI},
}

@Article{Baker2019,
  author        = {Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
  title         = {Emergent Tool Use From Multi-Agent Autocurricula},
  year          = {2019},
  month         = sep,
  abstract      = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
  archiveprefix = {arXiv},
  eprint        = {1909.07528},
  file          = {:Baker2019 - Emergent Tool Use from Multi Agent Autocurricula.pdf:PDF},
  groups        = {MARL, Multi-agent learning},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Bansal2017,
  author        = {Trapit Bansal and Jakub Pachocki and Szymon Sidor and Ilya Sutskever and Igor Mordatch},
  journal       = {ICLR 2018},
  title         = {Emergent Complexity via Multi-Agent Competition},
  year          = {2017},
  month         = oct,
  abstract      = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
  archiveprefix = {arXiv},
  eprint        = {1710.03748},
  file          = {:Bansal2017 - Emergent Complexity Via Multi Agent Competition.pdf:PDF},
  groups        = {MARL, Multi-agent learning},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Jaderberg2018,
  author        = {Max Jaderberg and Wojciech M. Czarnecki and Iain Dunning and Luke Marris and Guy Lever and Antonio Garcia Castaneda and Charles Beattie and Neil C. Rabinowitz and Ari S. Morcos and Avraham Ruderman and Nicolas Sonnerat and Tim Green and Louise Deason and Joel Z. Leibo and David Silver and Demis Hassabis and Koray Kavukcuoglu and Thore Graepel},
  journal       = {Science, 364(6443):859–865, 2019},
  title         = {Human-level performance in first-person multiplayer games with population-based deep reinforcement learning},
  year          = {2018},
  month         = jul,
  abstract      = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
  archiveprefix = {arXiv},
  doi           = {10.1126/science.aau6249},
  eprint        = {1807.01281},
  file          = {:Jaderberg2018 - Human Level Performance in First Person Multiplayer Games with Population Based Deep Reinforcement Learning.pdf:PDF},
  groups        = {MARL, Multi-agent RL},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Liu2019,
  author        = {Siqi Liu and Guy Lever and Josh Merel and Saran Tunyasuvunakool and Nicolas Heess and Thore Graepel},
  title         = {Emergent Coordination Through Competition},
  year          = {2019},
  month         = feb,
  abstract      = {We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics. We demonstrate that decentralized, population-based training with co-play can lead to a progression in agents' behaviors: from random, to simple ball chasing, and finally showing evidence of cooperation. Our study highlights several of the challenges encountered in large scale multi-agent training in continuous control. In particular, we demonstrate that the automatic optimization of simple shaping rewards, not themselves conducive to co-operative behavior, can lead to long-horizon team behavior. We further apply an evaluation scheme, grounded by game theoretic principals, that can assess agent performance in the absence of pre-defined evaluation tasks or human baselines.},
  archiveprefix = {arXiv},
  eprint        = {1902.07151},
  file          = {:Liu2019 - Emergent Coordination through Competition.pdf:PDF},
  groups        = {MARL, Multi-agent RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Miikkulainen2004,
  author        = {R. Miikkulainen and K. O. Stanley},
  journal       = {Journal Of Artificial Intelligence Research, Volume 21, pages 63-100, 2004},
  title         = {Competitive Coevolution through Evolutionary Complexification},
  year          = {2004},
  abstract      = {Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure, complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.},
  archiveprefix = {arXiv},
  comment       = {following NEAT},
  doi           = {10.1613/jair.1338},
  eprint        = {1107.0037},
  file          = {:Miikkulainen2011 - Competitive Coevolution through Evolutionary Complexification.pdf:PDF},
  groups        = {Multi-agent learning},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Rosin1997,
  author = {Christopher D. Rosin and Richard K. Belew},
  title  = {New Methods for Competitive Coevolution},
  year   = {1997},
  issn   = {1063-6560},
  pages  = {1-29},
  volume = {5},
  doi    = {10.1162/evco.1997.5.1.1},
  groups = {Multi-agent learning},
}

@Article{Sukhbaatar2016,
  author        = {Sainbayar Sukhbaatar and Arthur Szlam and Rob Fergus},
  journal       = {Advances in Neural Information Processing Systems, pp. 2244–2252},
  title         = {Learning Multiagent Communication with Backpropagation},
  year          = {2016},
  month         = may,
  abstract      = {Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.},
  archiveprefix = {arXiv},
  comment       = {CommNet},
  eprint        = {1605.07736},
  file          = {:Sukhbaatar2016 - Learning Multiagent Communication with Backpropagation.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  ranking       = {rank4},
}

@InProceedings{Mordatch2018,
  author    = {Igor Mordatch and Pieter Abbeel},
  booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence},
  title     = {Emergence of Grounded Compositional Language in Multi-Agent Populations},
  year      = {2018},
  editor    = {Sheila A. McIlraith and Kilian Q. Weinberger},
  pages     = {1495--1502},
  publisher = {{AAAI} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aaai/MordatchA18.bib},
  file      = {:Mordatch2017 - Emergence of Grounded Compositional Language in Multi Agent Populations.pdf:PDF},
  groups    = {MARL, Communication},
  priority  = {prio1},
  ranking   = {rank4},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17007},
}

@Article{Jaques2019,
  author        = {Natasha Jaques and Angeliki Lazaridou and Edward Hughes and Caglar Gulcehre and Pedro A. Ortega and DJ Strouse and Joel Z. Leibo and Nando de Freitas},
  journal       = {Proceedings of the 36th International Conference on Machine Learning, PMLR 97:3040-3049},
  title         = {Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning},
  year          = {2019},
  abstract      = {We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.},
  archiveprefix = {arXiv},
  eprint        = {1810.08647},
  file          = {:Jaques2018 - Social Influence As Intrinsic Motivation for Multi Agent Deep Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL, Communication},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
}

@Article{Leibo2019,
  author        = {Joel Z. Leibo and Julien Perolat and Edward Hughes and Steven Wheelwright and Adam H. Marblestone and Edgar Duéñez-Guzmán and Peter Sunehag and Iain Dunning and Thore Graepel},
  journal       = {Proceedings of the 18th International Conference on Autonomous Agents and Multi-Agent Systems, pp. 1099–1107.},
  title         = {Malthusian Reinforcement Learning},
  year          = {2019},
  month         = dec,
  abstract      = {Here we explore a new algorithmic framework for multi-agent reinforcement learning, called Malthusian reinforcement learning, which extends self-play to include fitness-linked population size dynamics that drive ongoing innovation. In Malthusian RL, increases in a subpopulation's average return drive subsequent increases in its size, just as Thomas Malthus argued in 1798 was the relationship between preindustrial income levels and population growth. Malthusian reinforcement learning harnesses the competitive pressures arising from growing and shrinking population size to drive agents to explore regions of state and policy spaces that they could not otherwise reach. Furthermore, in environments where there are potential gains from specialization and division of labor, we show that Malthusian reinforcement learning is better positioned to take advantage of such synergies than algorithms based on self-play.},
  archiveprefix = {arXiv},
  eprint        = {1812.07019},
  file          = {:Leibo2018 - Malthusian Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.NE, cs.MA, q-bio.PE},
  primaryclass  = {cs.NE},
}

@InProceedings{pmlr-v48-mniha16,
  author    = {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  year      = {2016},
  address   = {New York, New York, USA},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  month     = {20--22 Jun},
  pages     = {1928--1937},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {48},
  abstract  = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  comment   = {A3C},
  file      = {:pmlr-v48-mniha16 - Asynchronous Methods for Deep Reinforcement Learning.pdf:PDF},
  groups    = {Value based},
  pdf       = {http://proceedings.mlr.press/v48/mniha16.pdf},
  url       = {http://proceedings.mlr.press/v48/mniha16.html},
}

@InProceedings{10.5555/3091125.3091194,
  author    = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Multi-Agent Reinforcement Learning in Sequential Social Dilemmas},
  year      = {2017},
  address   = {Richland, SC},
  pages     = {464–473},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '17},
  abstract  = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
  file      = {:10.5555_3091125.3091194 - Multi Agent Reinforcement Learning in Sequential Social Dilemmas.pdf:PDF},
  groups    = {Multi-agent RL},
  keywords  = {agent-based social simulation, cooperation, markov games, non-cooperative games, social dilemmas},
  location  = {S\~{a}o Paulo, Brazil},
  numpages  = {10},
  ranking   = {rank2},
  url       = {https://arxiv.org/pdf/1702.03037.pdf},
}

@Article{SHOHAM2007365,
  author   = {Yoav Shoham and Rob Powers and Trond Grenager},
  journal  = {Artificial Intelligence},
  title    = {If multi-agent learning is the answer, what is the question?},
  year     = {2007},
  issn     = {0004-3702},
  note     = {Foundations of Multi-Agent Learning},
  number   = {7},
  pages    = {365-377},
  volume   = {171},
  abstract = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.11This article has a long history and owes many debts. A first version was presented at the NIPS workshop, Multi-Agent Learning: Theory and Practice, in 2002. A later version was presented at the AAAI Fall Symposium in 2004 [Y. Shoham, R. Powers, T. Grenager, On the agenda(s) of research on multi-agent learning, in: AAAI 2004 Symposium on Artificial Multi-Agent Learning (FS-04-02), AAAI Press, 2004]. Over time it has gradually evolved into the current form, as a result of our own work in the area as well as the feedback of many colleagues. We thank them all collectively, with special thanks to members of the multi-agent group at Stanford in the past three years. Rakesh Vohra and Michael Wellman provided detailed comments on the latest draft which resulted in substantive improvements, although we alone are responsible for the views put forward. This work was supported by NSF ITR grant IIS-0205633 and DARPA grant HR0011-05-1.},
  doi      = {https://doi.org/10.1016/j.artint.2006.02.006},
  file     = {:SHOHAM2007365 - If Multi Agent Learning Is the Answer, What Is the Question_ (2).pdf:PDF},
  groups   = {MARL, MAS Reviews},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370207000495},
}

@Article{Ecoffet2020,
  author        = {Adrien Ecoffet and Joost Huizinga and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  journal       = {Nature},
  title         = {First return then explore},
  year          = {2021},
  month         = feb,
  number        = {590},
  pages         = {580–586},
  abstract      = {The promise of reinforcement learning is to solve complex sequential decision problems by specifying a high-level reward function only. However, RL algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but despite substantial investments by the community, creating algorithms that can do so remains one of the central challenges of the field. We hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states ("detachment") and from failing to first return to a state before exploring from it ("derailment"). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before exploring. Go-Explore solves all heretofore unsolved Atari games (those for which algorithms could not previously outperform humans when evaluated following current community standards) and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a challenging and extremely sparse-reward robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The striking contrast between the substantial performance gains from Go-Explore and the simplicity of its mechanisms suggests that remembering promising states, returning to them, and exploring from them is a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.},
  archiveprefix = {arXiv},
  doi           = {https://doi.org/10.1038/s41586-020-03157-9},
  eprint        = {2004.12919},
  file          = {:Ecoffet2020 - First Return Then Explore.pdf:PDF},
  groups        = {Imitation},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Hester_Vecerik_Pietquin_Lanctot_Schaul_Piot_Horgan_Quan_Sendonaris_Osband_Dulac-Arnold_Agapiou_Leibo_Gruslys_2018,
  author       = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and Dulac-Arnold, Gabriel and Agapiou, John and Leibo, Joel and Gruslys, Audrunas},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Deep Q-learning From Demonstrations},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator’s actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD’s performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN. &lt;/p&gt;},
  file         = {:Hester_Vecerik_Pietquin_Lanctot_Schaul_Piot_Horgan_Quan_Sendonaris_Osband_Dulac-Arnold_Agapiou_Leibo_Gruslys_2018 - Deep Q Learning from Demonstrations.pdf:PDF},
  groups       = {Imitation},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11757},
}

@Article{Panait2005,
  author     = {Liviu Panait and Sean Luke},
  journal    = {Autonomous Agents and Multi-Agent Systems},
  title      = {Cooperative Multi-Agent Learning: The State of the Art},
  year       = {2005},
  month      = {nov},
  number     = {3},
  pages      = {387--434},
  volume     = {11},
  doi        = {10.1007/s10458-005-2631-2},
  file       = {:Panait2005 - Cooperative Multi Agent Learning_ the State of the Art.pdf:PDF},
  groups     = {MAS Reviews},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{Lowe2017,
  author     = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  booktitle  = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title      = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  year       = {2017},
  address    = {Red Hook, NY, USA},
  pages      = {6382–6393},
  publisher  = {Curran Associates Inc.},
  series     = {NIPS'17},
  abstract   = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  comment    = {MADDPG},
  file       = {:10.5555_3295222.3295385 - Multi Agent Actor Critic for Mixed Cooperative Competitive Environments.pdf:PDF},
  groups     = {Multi-agent RL},
  isbn       = {9781510860964},
  location   = {Long Beach, California, USA},
  numpages   = {12},
  ranking    = {rank5},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.5555/3295222.3295385},
}

@InBook{doi:10.1142/9789812777263_0020,
  author    = {David H. Wolpert and Kagan Tumer},
  pages     = {355-369},
  title     = {Optimal Payoff Functions for Members of Collectives},
  year      = {2002},
  abstract  = {Abstract We consider the problem of designing (perhaps massively distributed) collectives of computational processes to maximize a provided “world utility” function. We consider this problem when the behavior of each process in the collective can be cast as striving to maximize its own payoff utility function. For such cases the central design issue is how to initialize/update those payoff utility functions of the individual processes so as to induce behavior of the entire collective having good values of the world utility. Traditional “team game” approaches to this problem simply assign to each process the world utility as its payoff utility function. In previous work we used the “Collective Intelligence” (COIN) framework to derive a better choice of payoff utility functions, one that results in world utility performance up to orders of magnitude superior to that ensuing from the use of the team game utility. In this paper, we extend these results using a novel mathematical framework. Under that new framework we review the derivation of the general class of payoff utility functions that both (i) are easy for the individual processes to try to maximize, and (ii) have the property that if good values of them are achieved, then we are assured a high value of world utility. These are the “Aristocrat Utility” and a new variant of the “Wonderful Life Utility” that was introduced in the previous COIN work. We demonstrate experimentally that using these new utility functions can result in significantly improved performance over that of previously investigated COIN payoff utilities, over and above those previous utilities’ superiority to the conventional team game utility. These results also illustrate the substantial superiority of these payoff functions to perhaps the most natural version of the economics technique of “endogenizing externalities.”},
  booktitle = {Modeling Complexity in Economic and Social Systems},
  doi       = {10.1142/9789812777263_0020},
  eprint    = {https://www.worldscientific.com/doi/pdf/10.1142/9789812777263_0020},
  file      = {:doi_10.1142_9789812777263_0020 - Optimal Payoff Functions for Members of Collectives.pdf:PDF},
  groups    = {Multi-agent learning, Wonderful Life Utility},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/9789812777263_0020},
}

@Article{Michalak2014,
  author        = {Tomasz Pawel Michalak and Karthik V Aadithya and Piotr L. Szczepanski and Balaraman Ravindran and Nicholas R. Jennings},
  journal       = {Journal Of Artificial Intelligence Research, Volume 46, pages 607-650, 2013},
  title         = {Efficient Computation of the Shapley Value for Game-Theoretic Network Centrality},
  year          = {2014},
  month         = feb,
  abstract      = {The Shapley value---probably the most important normative payoff division scheme in coalitional games---has recently been advocated as a useful measure of centrality in networks. However, although this approach has a variety of real-world applications (including social and organisational networks, biological networks and communication networks), its computational properties have not been widely studied. To date, the only practicable approach to compute Shapley value-based centrality has been via Monte Carlo simulations which are computationally expensive and not guaranteed to give an exact answer. Against this background, this paper presents the first study of the computational aspects of the Shapley value for network centralities. Specifically, we develop exact analytical formulae for Shapley value-based centrality in both weighted and unweighted networks and develop efficient (polynomial time) and exact algorithms based on them. We empirically evaluate these algorithms on two real-life examples (an infrastructure network representing the topology of the Western States Power Grid and a collaboration network from the field of astrophysics) and demonstrate that they deliver significant speedups over the Monte Carlo approach. For instance, in the case of unweighted networks our algorithms are able to return the exact solution about 1600 times faster than the Monte Carlo approximation, even if we allow for a generous 10% error margin for the latter method.},
  archiveprefix = {arXiv},
  doi           = {10.1613/jair.3806},
  eprint        = {1402.0567},
  file          = {:Michalak2014 - Efficient Computation of the Shapley Value for Game Theoretic Network Centrality (2).pdf:PDF},
  groups        = {Shapley value},
  keywords      = {cs.GT},
  primaryclass  = {cs.GT},
}

@Article{Anshelevich2008,
  author     = {Elliot Anshelevich and Anirban Dasgupta and Jon Kleinberg and {\'{E}}va Tardos and Tom Wexler and Tim Roughgarden},
  journal    = {{SIAM} Journal on Computing},
  title      = {The Price of Stability for Network Design with Fair Cost Allocation},
  year       = {2008},
  month      = {jan},
  number     = {4},
  pages      = {1602--1623},
  volume     = {38},
  comment    = {Use Shapley value in potential games for allocating network cost to users.},
  doi        = {10.1137/070680096},
  file       = {:Anshelevich2008 - The Price of Stability for Network Design with Fair Cost Allocation.pdf:PDF},
  groups     = {Shapley value},
  publisher  = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  readstatus = {skimmed},
}

@Article{10013542751,
  author    = {Shapley, L. S.},
  journal   = {Contributions to the Theory of Games},
  title     = {A value for n-person games},
  year      = {1953},
  number    = {28},
  pages     = {307-317},
  volume    = {II},
  file      = {:10013542751 - A Value for N Person Games.pdf:PDF},
  groups    = {Shapley value},
  publisher = {Princeton University Press},
  url       = {https://ci.nii.ac.jp/naid/10013542751/en/},
}

@Article{Wang2020a,
  author     = {Jianhong Wang and Yuan Zhang and Tae-Kyun Kim and Yunjie Gu},
  journal    = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title      = {Shapley Q-Value: A Local Reward Approach to Solve Global Reward Games},
  year       = {2020},
  month      = {apr},
  number     = {05},
  pages      = {7285--7292},
  volume     = {34},
  doi        = {10.1609/aaai.v34i05.6220},
  file       = {:Wang2020 - Shapley Q Value_ a Local Reward Approach to Solve Global Reward Games.pdf:PDF},
  groups     = {Shapley value},
  publisher  = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  readstatus = {skimmed},
}

@Article{Foerster_Farquhar_Afouras_Nardelli_Whiteson_2018,
  author       = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Counterfactual Multi-Agent Policy Gradients},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state. &lt;/p&gt;},
  comment      = {COMA},
  file         = {:Foerster_Farquhar_Afouras_Nardelli_Whiteson_2018 - Counterfactual Multi Agent Policy Gradients.pdf:PDF},
  groups       = {Multi-agent RL},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11794},
}

@InProceedings{kim2018learning,
  author    = {Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},
  booktitle = {International Conference on Learning Representations},
  title     = {Learning to Schedule Communication in Multi-agent Reinforcement Learning},
  year      = {2019},
  comment   = {SchedNet},
  file      = {:kim2018learning - Learning to Schedule Communication in Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=SJxu5iR9KQ},
}

@InProceedings{Das2019,
  author     = {Das, Abhishek and Gervet, Th{\'e}ophile and Romoff, Joshua and Batra, Dhruv and Parikh, Devi and Rabbat, Mike and Pineau, Joelle},
  booktitle  = {Proceedings of the 36th International Conference on Machine Learning},
  title      = {{T}ar{MAC}: Targeted Multi-Agent Communication},
  year       = {2019},
  editor     = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  month      = {09--15 Jun},
  pages      = {1538--1546},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {97},
  abstract   = {We propose a targeted communication architecture for multi-agent reinforcement learning, where agents learn both <em>what</em> messages to send and <em>whom</em> to address them to while performing cooperative tasks in partially-observable environments. This targeting behavior is learnt solely from downstream task-specific reward without any communication supervision. We additionally augment this with a multi-round communication approach where agents coordinate via multiple rounds of communication before taking actions in the environment. We evaluate our approach on a diverse set of cooperative multi-agent tasks, of varying difficulties, with varying number of agents, in a variety of environments ranging from 2D grid layouts of shapes and simulated traffic junctions to 3D indoor environments, and demonstrate the benefits of targeted and multi-round communication. Moreover, we show that the targeted communication strategies learned by agents are interpretable and intuitive. Finally, we show that our architecture can be easily extended to mixed and competitive environments, leading to improved performance and sample complexity over recent state-of-the-art approaches.},
  file       = {:Das2019 - TarMAC_ Targeted Multi Agent Communication.pdf:PDF},
  groups     = {Communication},
  pdf        = {http://proceedings.mlr.press/v97/das19a/das19a.pdf},
  ranking    = {rank3},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v97/das19a.html},
}

@InProceedings{Iqbal2019,
  author    = {Iqbal, Shariq and Sha, Fei},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  month     = {09--15 Jun},
  pages     = {2961--2970},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.},
  file      = {:Iqbal2019 - Actor Attention Critic for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v97/iqbal19a.html},
}

@InProceedings{Lanctot2017,
  author    = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audr\={u}nas and Lazaridou, Angeliki and Tuyls, Karl and P\'{e}rolat, Julien and Silver, David and Graepel, Thore},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {4193–4206},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'17},
  abstract  = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
  comment   = {PSRO},
  file      = {:Lanctot2017 - A Unified Game Theoretic Approach to Multiagent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {14},
  ranking   = {rank3},
  url       = {https://dl.acm.org/doi/pdf/10.5555/3294996.3295174},
}

@InProceedings{cao2018emergent,
  author    = {Kris Cao and Angeliki Lazaridou and Marc Lanctot and Joel Z Leibo and Karl Tuyls and Stephen Clark},
  booktitle = {International Conference on Learning Representations},
  title     = {Emergent Communication through Negotiation},
  year      = {2018},
  file      = {:cao2018emergent - Emergent Communication through Negotiation.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=Hk6WhagRW},
}

@Article{Eccles2019,
  author        = {Tom Eccles and Yoram Bachrach and Guy Lever and Angeliki Lazaridou and Thore Graepel},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {Biases for Emergent Communication in Multi-agent Reinforcement Learning},
  year          = {2019},
  month         = dec,
  pages         = {13111–13121},
  abstract      = {We study the problem of emergent communication, in which language arises because speakers and listeners must communicate information in order to solve tasks. In temporally extended reinforcement learning domains, it has proved hard to learn such communication without centralized training of agents, due in part to a difficult joint exploration problem. We introduce inductive biases for positive signalling and positive listening, which ease this problem. In a simple one-step environment, we demonstrate how these biases ease the learning problem. We also apply our methods to a more extended environment, showing that agents with these inductive biases achieve better performance, and analyse the resulting communication protocols.},
  archiveprefix = {arXiv},
  eprint        = {1912.05676},
  file          = {:Eccles2019 - Biases for Emergent Communication in Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.MA, cs.CL, cs.LG},
  primaryclass  = {cs.MA},
}

@Article{BACHRACH2020103356,
  author   = {Yoram Bachrach and Richard Everett and Edward Hughes and Angeliki Lazaridou and Joel Z. Leibo and Marc Lanctot and Michael Johanson and Wojciech M. Czarnecki and Thore Graepel},
  journal  = {Artificial Intelligence},
  title    = {Negotiating team formation using deep reinforcement learning},
  year     = {2020},
  issn     = {0004-3702},
  pages    = {103356},
  volume   = {288},
  abstract = {When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory. Additionally, we investigate how the physical location of agents influences negotiation outcomes.},
  doi      = {https://doi.org/10.1016/j.artint.2020.103356},
  file     = {:BACHRACH2020103356 - Negotiating Team Formation Using Deep Reinforcement Learning.pdf:PDF},
  groups   = {Shapley value},
  keywords = {Multi-agent systems, Team formation, Coalition formation, Reinforcement learning, Deep learning, Cooperative games, Shapley value},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370220301077},
}

@Book{wooldridge2009introduction,
  author    = {Wooldridge, Michael},
  publisher = {John wiley \& sons},
  title     = {An introduction to multiagent systems},
  year      = {2009},
  groups    = {Multi-agent Systems},
}

@TechReport{wolpert99a,
  author      = {David H. Wolpert and Kagan Tumer},
  institution = {NASA},
  title       = {An Introduction to Collective Intelligence},
  year        = {1999},
  note        = {NASA-ARC-IC-99-63},
  abstract    = {This paper surveys the emerging science of how to
                  design a ``COllective INtelligence'' (COIN). A COIN
                  is a large multi-agent system where: (i) There is
                  little to no centralized communication or control;
                  and (ii) There is a provided world utility function
                  that rates the possible histories of the full
                  system. In particular, we are interested in COINs in
                  which each agent runs a reinforcement learning (RL)
                  algorithm. Rather than use a conventional modeling
                  approach (e.g., model the system dynamics, and
                  hand-tune agents to cooperate), we aim to solve the
                  COIN design problem implicitly, via the ``adaptive''
                  character of the RL algorithms of each of the
                  agents. This approach introduces an entirely new,
                  profound design problem: Assuming the RL algorithms
                  are able to achieve high rewards, what reward
                  functions for the individual agents will, when
                  pursued by those agents, result in high world
                  utility? In other words, what reward functions will
                  best ensure that we do not have phenomena like the
                  tragedy of the commons, Braess's paradox, or the
                  liquidity trap? Although still very young, research
                  specifically concentrating on the COIN design
                  problem has already resulted in successes in
                  artificial domains, in particular in packet-routing,
                  the leader-follower problem, and in variants of
                  Arthur's El Farol bar problem. It is expected that
                  as it matures and draws upon other disciplines
                  related to COINs, this research will greatly expand
                  the range of tasks addressable by human
                  engineers. Moreover, in addition to drawing on them,
                  such a fully developed science of COIN design may
                  provide much insight into other already established
                  scientific fields, such as economics, game theory,
                  and population biology},
  arxiv       = {cs.LG/9908014},
  comment     = {Shows how to set the agents' reward functions (using the "wonderful life" reward) so that the global utility is maximized. Extensive related work section.},
  file        = {:wolpert99a - An Introduction to Collective Intelligence.pdf:PDF},
  googleid    = {89pRsJDcBLUJ:scholar.google.com/},
  groups      = {Wonderful Life Utility},
  keywords    = {multiagent learning},
  url         = {http://jmvidal.cse.sc.edu/library/wolpert99a.pdf},
}

@Article{HernandezLeal2019,
  author    = {Pablo Hernandez-Leal and Bilal Kartal and Matthew E. Taylor},
  journal   = {Autonomous Agents and Multi-Agent Systems},
  title     = {A survey and critique of multiagent deep reinforcement learning},
  year      = {2019},
  month     = {oct},
  number    = {6},
  pages     = {750--797},
  volume    = {33},
  doi       = {10.1007/s10458-019-09421-1},
  file      = {:HernandezLeal2019 - A Survey and Critique of Multiagent Deep Reinforcement Learning.pdf:PDF},
  groups    = {MAS Reviews},
  publisher = {Springer Science and Business Media {LLC}},
}

@Misc{Vidal07,
  author = {José M Vidal},
  title  = {Fundamentals of Multiagent Systems},
  year   = {2007},
  file   = {:Vidal06 - Fundamentals of Multiagent Systems.pdf:PDF},
  groups = {MAS Reviews},
}

@InBook{Nowé2012,
  author     = {Now{\'e}, Ann and Vrancx, Peter and De Hauwere, Yann-Micha{\"e}l},
  editor     = {Wiering, Marco and van Otterlo, Martijn},
  pages      = {441--470},
  publisher  = {Springer Berlin Heidelberg},
  title      = {Game Theory and Multi-agent Reinforcement Learning},
  year       = {2012},
  address    = {Berlin, Heidelberg},
  isbn       = {978-3-642-27645-3},
  abstract   = {Reinforcement Learning was originally developed for Markov Decision Processes (MDPs). It allows a single agent to learn a policy that maximizes a possibly delayed reward signal in a stochastic stationary environment. It guarantees convergence to the optimal policy, provided that the agent can sufficiently experiment and the environment in which it is operating is Markovian. However, when multiple agents apply reinforcement learning in a shared environment, this might be beyond the MDP model. In such systems, the optimal policy of an agent depends not only on the environment, but on the policies of the other agents as well. These situations arise naturally in a variety of domains, such as: robotics, telecommunications, economics, distributed control, auctions, traffic light control, etc. In these domains multi-agent learning is used, either because of the complexity of the domain or because control is inherently decentralized. In such systems it is important that agents are capable of discovering good solutions to the problem at hand either by coordinating with other learners or by competing with them. This chapter focuses on the application reinforcement learning techniques in multi-agent systems. We describe a basic learning framework based on the economic research into game theory, and illustrate the additional complexity that arises in such systems. We also described a representative selection of algorithms for the different areas of multi-agent reinforcement learning research.},
  booktitle  = {Reinforcement Learning: State-of-the-Art},
  doi        = {10.1007/978-3-642-27645-3_14},
  file       = {:Nowé2012 - Game Theory and Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {MAS Reviews},
  readstatus = {read},
  url        = {https://doi.org/10.1007/978-3-642-27645-3_14},
}

@Article{6303906,
  author   = {Y. {Cao} and W. {Yu} and W. {Ren} and G. {Chen}},
  journal  = {IEEE Transactions on Industrial Informatics},
  title    = {An Overview of Recent Progress in the Study of Distributed Multi-Agent Coordination},
  year     = {2013},
  issn     = {1941-0050},
  month    = {Feb},
  number   = {1},
  pages    = {427-438},
  volume   = {9},
  abstract = {This paper reviews some main results and progress in distributed multi-agent coordination, focusing on papers published in major control systems and robotics journals since 2006. Distributed coordination of multiple vehicles, including unmanned aerial vehicles, unmanned ground vehicles, and unmanned underwater vehicles, has been a very active research subject studied extensively by the systems and control community. The recent results in this area are categorized into several directions, such as consensus, formation control, optimization, and estimation. After the review, a short discussion section is included to summarize the existing research and to propose several promising research directions along with some open problems that are deemed important for further investigations.},
  doi      = {10.1109/TII.2012.2219061},
  file     = {:6303906 - An Overview of Recent Progress in the Study of Distributed Multi Agent Coordination.pdf:PDF},
  groups   = {MAS Reviews},
  keywords = {distributed control;multi-robot systems;distributed multiagent coordination;control systems;robotics journals;distributed coordination;multiple vehicles;unmanned aerial vehicles;unmanned ground vehicles;unmanned underwater vehicles;systems and control community;Delay effects;Network topology;Heuristic algorithms;Vehicle dynamics;Vehicles;Algorithm design and analysis;Delay;Distributed coordination;formation control;multi-agent system;sensor network},
  ranking  = {rank4},
}

@Article{Oliehoek2008,
  author    = {F. A. Oliehoek and M. T. J. Spaan and N. Vlassis},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {Optimal and Approximate Q-value Functions for Decentralized {POMDPs}},
  year      = {2008},
  month     = {may},
  pages     = {289--353},
  volume    = {32},
  doi       = {10.1613/jair.2447},
  file      = {:Oliehoek2008 - Optimal and Approximate Q Value Functions for Decentralized POMDPs.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {{AI} Access Foundation},
  ranking   = {rank2},
}

@Article{KRAEMER201682,
  author   = {Landon Kraemer and Bikramjit Banerjee},
  journal  = {Neurocomputing},
  title    = {Multi-agent reinforcement learning as a rehearsal for decentralized planning},
  year     = {2016},
  issn     = {0925-2312},
  pages    = {82-94},
  volume   = {190},
  abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are a powerful tool for modeling multi-agent planning and decision-making under uncertainty. Prevalent Dec-POMDP solution techniques require centralized computation given full knowledge of the underlying model. Multi-agent reinforcement learning (MARL) based approaches have been recently proposed for distributed solution of Dec-POMDPs without full prior knowledge of the model, but these methods assume that conditions during learning and policy execution are identical. In some practical scenarios this may not be the case. We propose a novel MARL approach in which agents are allowed to rehearse with information that will not be available during policy execution. The key is for the agents to learn policies that do not explicitly rely on these rehearsal features. We also establish a weak convergence result for our algorithm, RLaR, demonstrating that RLaR converges in probability when certain conditions are met. We show experimentally that incorporating rehearsal features can enhance the learning rate compared to non-rehearsal-based learners, and demonstrate fast, (near) optimal performance on many existing benchmark Dec-POMDP problems. We also compare RLaR against an existing approximate Dec-POMDP solver which, like RLaR, does not assume a priori knowledge of the model. While RLaR׳s policy representation is not as scalable, we show that RLaR produces higher quality policies for most problems and horizons studied.},
  comment  = {RLaR},
  doi      = {https://doi.org/10.1016/j.neucom.2016.01.031},
  file     = {:KRAEMER201682 - Multi Agent Reinforcement Learning As a Rehearsal for Decentralized Planning.pdf:PDF},
  groups   = {Multi-agent RL},
  keywords = {Multi-agent reinforcement learning, Decentralized planning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231216000783},
}

@Article{Jorge2016,
  author        = {Emilio Jorge and Mikael Kågebäck and Fredrik D. Johansson and Emil Gustavsson},
  title         = {Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence},
  year          = {2016},
  month         = nov,
  abstract      = {Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most natural language processing today. As an alternative, we propose the use of situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks for evolving a shared language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that the agents learn not only to encode physical concepts in their words, i.e. grounding, but also that the agents learn to hold a multi-step dialogue remembering the state of the dialogue from step to step.},
  archiveprefix = {arXiv},
  eprint        = {1611.03218},
  file          = {:Jorge2016 - Learning to Play Guess Who_ and Inventing a Grounded Language As a Consequence.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.AI, cs.CL, cs.LG, cs.MA},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
}

@Article{10.1371/journal.pone.0172395,
  author    = {Tampuu, Ardi AND Matiisen, Tambet AND Kodelja, Dorian AND Kuzovkin, Ilya AND Korjus, Kristjan AND Aru, Juhan AND Aru, Jaan AND Vicente, Raul},
  journal   = {PLOS ONE},
  title     = {Multiagent cooperation and competition with deep reinforcement learning},
  year      = {2017},
  month     = {04},
  number    = {4},
  pages     = {1-15},
  volume    = {12},
  abstract  = {Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.},
  comment   = {Independent Deep-Q-learning},
  doi       = {10.1371/journal.pone.0172395},
  file      = {:10.1371_journal.pone.0172395 - Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {Public Library of Science},
  ranking   = {rank3},
  url       = {https://doi.org/10.1371/journal.pone.0172395},
}

@InProceedings{Tan1993,
  author     = {Tan, Ming},
  booktitle  = {Proceedings of the Tenth International Conference on Machine Learning},
  title      = {Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents},
  year       = {1993},
  address    = {San Francisco, CA, USA},
  pages      = {330–337},
  publisher  = {Morgan Kaufmann Publishers Inc.},
  series     = {ICML'93},
  file       = {:tan1993.pdf:PDF},
  groups     = {Multi-agent RL},
  isbn       = {1558603077},
  location   = {Amherst, MA, USA},
  numpages   = {8},
  ranking    = {rank3},
  readstatus = {read},
}

@InProceedings{Das_2017_ICCV,
  author    = {Das, Abhishek and Kottur, Satwik and Moura, Jose M. F. and Lee, Stefan and Batra, Dhruv},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {Learning Cooperative Visual Dialog Agents With Deep Reinforcement Learning},
  year      = {2017},
  month     = {Oct},
  file      = {:Das_2017_ICCV - Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank3},
}

@Article{Lazaridou2016,
  author        = {Angeliki Lazaridou and Alexander Peysakhovich and Marco Baroni},
  title         = {Multi-Agent Cooperation and the Emergence of (Natural) Language},
  year          = {2016},
  month         = dec,
  abstract      = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.},
  archiveprefix = {arXiv},
  eprint        = {1612.07182},
  file          = {:Lazaridou2016 - Multi Agent Cooperation and the Emergence of (Natural) Language.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.CL, cs.CV, cs.GT, cs.LG, cs.MA},
  primaryclass  = {cs.CL},
  ranking       = {rank2},
}

@InCollection{Gupta2017,
  author    = {Jayesh K. Gupta and Maxim Egorov and Mykel Kochenderfer},
  booktitle = {Autonomous Agents and Multiagent Systems},
  publisher = {Springer International Publishing},
  title     = {Cooperative Multi-agent Control Using Deep Reinforcement Learning},
  year      = {2017},
  pages     = {66--83},
  comment   = {centralised actor-critic algorithm with per-agent critics
sharing of policy parameters},
  doi       = {10.1007/978-3-319-71682-4_5},
  file      = {:Gupta2017 - Cooperative Multi Agent Control Using Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank3},
}

@InProceedings{foerster17b,
  author     = {Jakob Foerster and Nantas Nardelli and Gregory Farquhar and Triantafyllos Afouras and Philip H. S. Torr and Pushmeet Kohli and Shimon Whiteson},
  booktitle  = {Proceedings of the 34th International Conference on Machine Learning},
  title      = {Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning},
  year       = {2017},
  address    = {International Convention Centre, Sydney, Australia},
  editor     = {Doina Precup and Yee Whye Teh},
  month      = {06--11 Aug},
  pages      = {1146--1155},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {70},
  abstract   = {Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent’s value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.},
  comment    = {Replay stabilisation for Independent Q-learning},
  file       = {:foerster17b - Stabilising Experience Replay for Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Multi-agent RL},
  pdf        = {http://proceedings.mlr.press/v70/foerster17b/foerster17b.pdf},
  ranking    = {rank2},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v70/foerster17b.html},
}

@InProceedings{rashid18a,
  author     = {Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle  = {Proceedings of the 35th International Conference on Machine Learning},
  title      = {{QMIX}: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  year       = {2018},
  address    = {Stockholmsmässan, Stockholm Sweden},
  editor     = {Jennifer Dy and Andreas Krause},
  month      = {10--15 Jul},
  pages      = {4295--4304},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {80},
  abstract   = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
  file       = {:rashid18a - QMIX_ Monotonic Value Function Factorisation for Deep Multi Agent Reinforcement Learning.pdf:PDF;:rashid18a - QMIX_ Monotonic Value Function Factorisation for Deep Multi Agent Reinforcement Learning (1).pdf:PDF},
  groups     = {Multi-agent RL},
  pdf        = {http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf},
  ranking    = {rank4},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v80/rashid18a.html},
}

@InProceedings{omidshafiei17a,
  author    = {Shayegan Omidshafiei and Jason Pazis and Christopher Amato and Jonathan P. How and John Vian},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability},
  year      = {2017},
  address   = {International Convention Centre, Sydney, Australia},
  editor    = {Doina Precup and Yee Whye Teh},
  month     = {06--11 Aug},
  pages     = {2681--2690},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.},
  comment   = {Multi-task MARL
Dec-Histeretic Deep Recurrent Q-Network
Concurrent Experience Replay Trajectories (CERT)},
  file      = {:omidshafiei17a - Deep Decentralized Multi Task Multi Agent Reinforcement Learning under Partial Observability.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v70/omidshafiei17a/omidshafiei17a.pdf},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v70/omidshafiei17a.html},
}

@Article{Usunier2016,
  author        = {Nicolas Usunier and Gabriel Synnaeve and Zeming Lin and Soumith Chintala},
  title         = {Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks},
  year          = {2016},
  month         = sep,
  abstract      = {We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the state-action space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, {\epsilon}-greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.},
  archiveprefix = {arXiv},
  eprint        = {1609.02993},
  file          = {:Usunier2016 - Episodic Exploration for Deep Deterministic Policies_ an Application to StarCraft Micromanagement Tasks.pdf:PDF},
  groups        = {DDPG},
  keywords      = {cs.AI, cs.LG, I.2.1; I.2.6},
  primaryclass  = {cs.AI},
  ranking       = {rank2},
}

@InCollection{LITTMAN1994157,
  author    = {Michael L. Littman},
  booktitle = {Machine Learning Proceedings 1994},
  publisher = {Morgan Kaufmann},
  title     = {Markov games as a framework for multi-agent reinforcement learning},
  year      = {1994},
  address   = {San Francisco (CA)},
  editor    = {William W. Cohen and Haym Hirsh},
  isbn      = {978-1-55860-335-6},
  pages     = {157-163},
  abstract  = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
  doi       = {https://doi.org/10.1016/B978-1-55860-335-6.50027-1},
  file      = {:LITTMAN1994157 - Markov Games As a Framework for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank3},
  url       = {https://www.sciencedirect.com/science/article/pii/B9781558603356500271},
}

@Article{Hu2021a,
  author        = {Jian Hu and Haibin Wu and Seth Austin Harding and Siyang Jiang and Shih-wei Liao},
  title         = {RIIT: Rethinking the Importance of Implementation Tricks in Multi-Agent Reinforcement Learning},
  year          = {2021},
  month         = feb,
  abstract      = {In recent years, Multi-Agent Deep Reinforcement Learning (MADRL) has been successfully applied to various complex scenarios such as computer games and robot swarms. We investigate the impact of "implementation tricks" of state-of-the-art (SOTA) QMIX-based algorithms. Firstly, we find that such tricks, described as auxiliary details to the core algorithm, seemingly of secondary importance, have a major impact. Our finding demonstrates that, after minimal tuning, QMIX attains extraordinarily high win rates and achieves SOTA in the StarCraft Multi-Agent Challenge (SMAC). Furthermore, we find QMIX's monotonicity condition improves sample efficiency in some cooperative tasks. We propose a new policy-based algorithm, called RIIT, to verify the importance of the monotonicity condition. RIIT also achieves SOTA in policy-based algorithms. At last, we prove theoretically that the Purely Cooperative Tasks can be represented by the monotonic mixing networks. We open-sourced the code at \url{https://github.com/hijkzzz/pymarl2}.},
  archiveprefix = {arXiv},
  eprint        = {2102.03479},
  file          = {:Hu2021a - RIIT_ Rethinking the Importance of Implementation Tricks in Multi Agent Reinforcement Learning (1).pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.MA},
  primaryclass  = {cs.LG},
}

@Article{Brown2020,
  author        = {Noam Brown and Anton Bakhtin and Adam Lerer and Qucheng Gong},
  title         = {Combining Deep Reinforcement Learning and Search for Imperfect-Information Games},
  year          = {2020},
  month         = jul,
  abstract      = {The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.},
  archiveprefix = {arXiv},
  comment       = {ReBeL},
  eprint        = {2007.13544},
  file          = {:Brown2020 - Combining Deep Reinforcement Learning and Search for Imperfect Information Games.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.GT, cs.AI, cs.LG},
  primaryclass  = {cs.GT},
}

@InProceedings{Rashid2020,
  author     = {Rashid, T and Farquhar, G and Peng, B and Whiteson, S},
  title      = {Weighted QMIX: Expanding monotonic value function factorisation for deep multi−agent reinforcement learning},
  year       = {2020},
  publisher  = {NeurIPS},
  groups     = {Multi-agent RL},
  journal    = {NeurIPS Proceedings 2020},
  organizer  = {34th Annual Conference on Neural Information Processing Systems (NeurIPS 2020)},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{Witt2020,
  author        = {Schroeder de Witt, Christian and Tarun Gupta and Denys Makoviichuk and Viktor Makoviychuk and Philip H. S. Torr and Mingfei Sun and Shimon Whiteson},
  title         = {Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  year          = {2020},
  month         = nov,
  abstract      = {Most recently developed approaches to cooperative multi-agent reinforcement learning in the \emph{centralized training with decentralized execution} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent PPO (IPPO), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite SMAC with little hyperparameter tuning. We also compare IPPO to several variants; the results suggest that IPPO's strong performance may be due to its robustness to some forms of environment non-stationarity.},
  archiveprefix = {arXiv},
  comment       = {IPPO},
  eprint        = {2011.09533},
  file          = {:Witt2020 - Is Independent Learning All You Need in the StarCraft Multi Agent Challenge_.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
  readstatus    = {read},
}

@Article{Li2021,
  author        = {Wenhao Li and Xiangfeng Wang and Bo Jin and Junjie Sheng and Hongyuan Zha},
  title         = {Dealing with Non-Stationarity in Multi-Agent Reinforcement Learning via Trust Region Decomposition},
  year          = {2021},
  month         = feb,
  abstract      = {Non-stationarity is one thorny issue in multi-agent reinforcement learning, which is caused by the policy changes of agents during the learning procedure. Current works to solve this problem have their own limitations in effectiveness and scalability, such as centralized critic and decentralized actor (CCDA), population-based self-play, modeling of others and etc. In this paper, we novelly introduce a $\delta$-stationarity measurement to explicitly model the stationarity of a policy sequence, which is theoretically proved to be proportional to the joint policy divergence. However, simple policy factorization like mean-field approximation will mislead to larger policy divergence, which can be considered as trust region decomposition dilemma. We model the joint policy as a general Markov random field and propose a trust region decomposition network based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established with the purpose to satisfy $\delta$-stationarity. MAMT can adjust the trust region of the local policies adaptively in an end-to-end manner, thereby approximately constraining the divergence of joint policy to alleviate the non-stationary problem. Our method can bring noticeable and stable performance improvement compared with baselines in coordination tasks of different complexity.},
  archiveprefix = {arXiv},
  eprint        = {2102.10616},
  file          = {:Li2021 - Dealing with Non Stationarity in Multi Agent Reinforcement Learning Via Trust Region Decomposition.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.GT, cs.MA},
  primaryclass  = {cs.LG},
}

@InProceedings{Liu2020,
  author    = {Liu, Iou-Jen and Yeh, Raymond A. and Schwing, Alexander G.},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {PIC: Permutation Invariant Critic for Multi-Agent Deep Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {590--602},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Sample efficiency and scalability to a large number of agents are two important goals for multi-agent reinforcement learning systems. Recent works got us closer to those goals, addressing non-stationarity of the environment from a single agent’s perspective by utilizing a deep net critic which depends on all observations and actions. The critic input concatenates agent observations and actions in a user-specified order. However, since deep nets aren’t permutation invariant, a permuted input changes the critic output despite the environment remaining identical. To avoid this inefficiency, we propose a ‘permutation invariant critic’ (PIC), which yields identical output irrespective of the agent permutation. This consistent representation enables our model to scale to 30 times more agents and to achieve improvements of test episode reward between 15% to 50% on the challenging multi-agent particle environment (MPE).},
  file      = {:Liu2020 - PIC_ Permutation Invariant Critic for Multi Agent Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v100/liu20a/liu20a.pdf},
  url       = {http://proceedings.mlr.press/v100/liu20a.html},
}

@Article{hu2003nash,
  author  = {Hu, Junling and Wellman, Michael P},
  journal = {Journal of machine learning research},
  title   = {Nash Q-learning for general-sum stochastic games},
  year    = {2003},
  number  = {Nov},
  pages   = {1039--1069},
  volume  = {4},
  file    = {:hu2003nash - Nash Q Learning for General Sum Stochastic Games.pdf:PDF},
  groups  = {Multi-agent RL},
  ranking = {rank3},
}

@Misc{yu2021benchmarking,
  author = {Chao Yu and Akash Velu and Eugene Vinitsky and Yu Wang and Alexandre Bayen and Yi Wu},
  title  = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms},
  year   = {2021},
  file   = {:NeurIPS_DRL_Workshop_BenchmarkingMARL.pdf:PDF},
  groups = {MAS Reviews},
  url    = {https://openreview.net/forum?id=t5lNr0Lw84H},
}

@Article{Papoudakis2020,
  author        = {Georgios Papoudakis and Filippos Christianos and Lukas Schäfer and Stefano V. Albrecht},
  title         = {Comparative Evaluation of Multi-Agent Deep Reinforcement Learning Algorithms},
  year          = {2020},
  month         = jun,
  abstract      = {Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learners, centralised training with decentralised execution, and value decomposition) in a diverse range of multi-agent learning tasks. Our results show that (1) algorithm performance depends strongly on environment properties and no algorithm learns efficiently across all learning tasks; (2) independent learners often achieve equal or better performance than more complex algorithms; (3) tested algorithms struggle to solve multi-agent tasks with sparse rewards. We report detailed empirical data, including a reliability analysis, and provide insights into the limitations of the tested algorithms.},
  archiveprefix = {arXiv},
  eprint        = {2006.07869},
  file          = {:Papoudakis2020 - Comparative Evaluation of Multi Agent Deep Reinforcement Learning Algorithms.pdf:PDF},
  groups        = {MAS Reviews},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
}

@InProceedings{Claus1998,
  author  = {Claus, Caroline and Boutilier, Craig},
  title   = {The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems},
  year    = {1998},
  month   = {07},
  file    = {:Claus1998 - The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf:PDF},
  groups  = {Multi-agent RL},
  journal = {The National Conference on Artificial Intelligence (AAAI 1998)},
  ranking = {rank2},
}

@InProceedings{littman2001friend,
  author    = {Littman, Michael L},
  booktitle = {ICML},
  title     = {Friend-or-foe Q-learning in general-sum games},
  year      = {2001},
  pages     = {322--328},
  volume    = {1},
  file      = {:littman2001friend - Friend or Foe Q Learning in General Sum Games.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank2},
}

@InProceedings{greenwald2003correlated,
  author    = {Greenwald, Amy and Hall, Keith and Serrano, Roberto},
  booktitle = {ICML},
  title     = {Correlated Q-learning},
  year      = {2003},
  pages     = {242--249},
  volume    = {3},
  file      = {:greenwald2003correlated - Correlated Q Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank1},
}

@InBook{Buşoniu2010,
  author     = {Bu{\c{s}}oniu, Lucian and Babu{\v{s}}ka, Robert and De Schutter, Bart},
  editor     = {Srinivasan, Dipti and Jain, Lakhmi C.},
  pages      = {183--221},
  publisher  = {Springer Berlin Heidelberg},
  title      = {Multi-agent Reinforcement Learning: An Overview},
  year       = {2010},
  address    = {Berlin, Heidelberg},
  isbn       = {978-3-642-14435-6},
  abstract   = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of multi-agent reinforcement learning algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of multi-agent reinforcement learning are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where multi-agent reinforcement learning techniques have been applied are briefly discussed. Several multi-agent reinforcement learning algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the multi-agent reinforcement learning field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
  booktitle  = {Innovations in Multi-Agent Systems and Applications - 1},
  doi        = {10.1007/978-3-642-14435-6_7},
  file       = {:Buşoniu2010 - Multi Agent Reinforcement Learning_ an Overview.pdf:PDF},
  groups     = {MAS Reviews},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://doi.org/10.1007/978-3-642-14435-6_7},
}

@Article{bowling2005convergence,
  author  = {Bowling, Michael},
  journal = {Advances in neural information processing systems},
  title   = {Convergence and no-regret in multiagent learning},
  year    = {2005},
  pages   = {209--216},
  volume  = {17},
  comment = {GIGA-WoLF},
  file    = {:bowling2005convergence - Convergence and No Regret in Multiagent Learning.pdf:PDF},
  groups  = {Multi-agent RL},
  ranking = {rank2},
}

@InProceedings{Kok2004,
  author    = {Jelle R. Kok and Nikos Vlassis},
  booktitle = {21st International Conference on Machine learning - {ICML} {\textquotesingle}04},
  title     = {Sparse cooperative Q-learning},
  year      = {2004},
  publisher = {{ACM} Press},
  doi       = {10.1145/1015330.1015410},
  file      = {:Kok2004 - Sparse Cooperative Q Learning.pdf:PDF},
  groups    = {Multi-agent RL},
}

@InProceedings{guestrin2002coordinated,
  author       = {Guestrin, Carlos and Lagoudakis, Michail and Parr, Ronald},
  booktitle    = {ICML},
  title        = {Coordinated reinforcement learning},
  year         = {2002},
  organization = {Citeseer},
  pages        = {227--234},
  volume       = {2},
  file         = {:guestrin2002coordinated - Coordinated Reinforcement Learning.pdf:PDF},
  groups       = {Multi-agent RL},
}

@Article{KokV06,
  author    = {Jelle R. Kok and Nikos A. Vlassis},
  journal   = {J. Mach. Learn. Res.},
  title     = {Collaborative Multiagent Reinforcement Learning by Payoff Propagation},
  year      = {2006},
  pages     = {1789--1828},
  volume    = {7},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/jmlr/KokV06.bib},
  file      = {:KokV06 - Collaborative Multiagent Reinforcement Learning by Payoff Propagation.pdf:PDF},
  groups    = {Multi-agent RL},
  timestamp = {Wed, 10 Jul 2019 15:28:11 +0200},
  url       = {http://jmlr.org/papers/v7/kok06a.html},
}

@InProceedings{bowling2001rational,
  author       = {Bowling, Michael and Veloso, Manuela},
  booktitle    = {International joint conference on artificial intelligence},
  title        = {Rational and convergent learning in stochastic games},
  year         = {2001},
  number       = {1},
  organization = {Citeseer},
  pages        = {1021--1026},
  volume       = {17},
  comment      = {WoLF},
  file         = {:bowling2001rational - Rational and Convergent Learning in Stochastic Games.pdf:PDF},
  groups       = {Multi-agent RL},
}

@Article{BOWLING2002215,
  author   = {Michael Bowling and Manuela Veloso},
  journal  = {Artificial Intelligence},
  title    = {Multiagent learning using a variable learning rate},
  year     = {2002},
  issn     = {0004-3702},
  number   = {2},
  pages    = {215-250},
  volume   = {136},
  abstract = {Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents' policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, “Win or Learn Fast”, for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method.},
  comment  = {WoLF},
  doi      = {https://doi.org/10.1016/S0004-3702(02)00121-2},
  file     = {:BOWLING2002215 - Multiagent Learning Using a Variable Learning Rate.pdf:PDF},
  groups   = {Multi-agent RL},
  keywords = {Multiagent learning, Reinforcement learning, Game theory},
  ranking  = {rank3},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370202001212},
}

@Article{LITTMAN200155,
  author   = {Michael L. Littman},
  journal  = {Cognitive Systems Research},
  title    = {Value-function reinforcement learning in Markov games},
  year     = {2001},
  issn     = {1389-0417},
  number   = {1},
  pages    = {55-66},
  volume   = {2},
  abstract = {Markov games are a model of multiagent environments that are convenient for studying multiagent reinforcement learning. This paper describes a set of reinforcement-learning algorithms based on estimating value functions and presents convergence theorems for these algorithms. The main contribution of this paper is that it presents the convergence theorems in a way that makes it easy to reason about the behavior of simultaneous learners in a shared environment.},
  comment  = {Team Q-learning},
  doi      = {https://doi.org/10.1016/S1389-0417(01)00015-8},
  file     = {:LITTMAN200155 - Value Function Reinforcement Learning in Markov Games.pdf:PDF},
  groups   = {Multi-agent RL},
  keywords = {Reinforcement learning, Temporal difference learning, Value functions, Game theory, Markov games, -learning, Nash equilibria},
  url      = {https://www.sciencedirect.com/science/article/pii/S1389041701000158},
}

@InProceedings{Lauer00analgorithm,
  author    = {Martin Lauer and Martin Riedmiller},
  booktitle = {In Proceedings of the Seventeenth International Conference on Machine Learning},
  title     = {An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems},
  year      = {2000},
  pages     = {535--542},
  publisher = {Morgan Kaufmann},
  comment   = {Distributed Q-learning},
  file      = {:Lauer-Riedmiller2000.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank2},
}

@InProceedings{hu1998multiagent,
  author       = {Hu, Junling and Wellman, Michael P},
  booktitle    = {ICML},
  title        = {Multiagent reinforcement learning: theoretical framework and an algorithm},
  year         = {1998},
  organization = {Citeseer},
  pages        = {242--250},
  volume       = {98},
  comment      = {Nash Q learning},
  file         = {:hu1998multiagent - Multiagent Reinforcement Learning_ Theoretical Framework and an Algorithm (2).pdf:PDF},
  groups       = {Multi-agent RL},
  ranking      = {rank3},
}

@Article{Gmytrasiewicz2005,
  author    = {P. J. Gmytrasiewicz and P. Doshi},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {A Framework for Sequential Planning in Multi-Agent Settings},
  year      = {2005},
  month     = {jul},
  pages     = {49--79},
  volume    = {24},
  doi       = {10.1613/jair.1579},
  file      = {:Gmytrasiewicz2005 - A Framework for Sequential Planning in Multi Agent Settings.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {{AI} Access Foundation},
  ranking   = {rank2},
}

@InProceedings{zinkevich2003online,
  author    = {Zinkevich, Martin},
  booktitle = {Proceedings of the 20th international conference on machine learning (icml-03)},
  title     = {Online convex programming and generalized infinitesimal gradient ascent},
  year      = {2003},
  pages     = {928--936},
  comment   = {GIGA},
  file      = {:zinkevich2003online - Online Convex Programming and Generalized Infinitesimal Gradient Ascent.pdf:PDF},
  groups    = {Multi-agent RL},
}

@Article{conitzer2007awesome,
  author    = {Conitzer, Vincent and Sandholm, Tuomas},
  journal   = {Machine Learning},
  title     = {AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents},
  year      = {2007},
  number    = {1-2},
  pages     = {23--43},
  volume    = {67},
  file      = {:conitzer2007awesome - AWESOME_ a General Multiagent Learning Algorithm That Converges in Self Play and Learns a Best Response against Stationary Opponents.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {Springer},
  ranking   = {rank1},
}

@Article{foerster2017learning,
  author     = {Foerster, Jakob N and Chen, Richard Y and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
  journal    = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Learning with opponent-learning awareness},
  year       = {2018},
  comment    = {LOLA},
  file       = {:foerster2017learning - Learning with Opponent Learning Awareness.pdf:PDF},
  groups     = {Multi-agent RL},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{Mahajan2019,
  author        = {Anuj Mahajan and Tabish Rashid and Mikayel Samvelyan and Shimon Whiteson},
  journal       = {Advances in Neural Information Processing Systems, 32, 2019, 7611-7622},
  title         = {MAVEN: Multi-Agent Variational Exploration},
  year          = {2019},
  month         = oct,
  abstract      = {Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43].},
  archiveprefix = {arXiv},
  eprint        = {1910.07483},
  file          = {:Mahajan2019 - MAVEN_ Multi Agent Variational Exploration.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
  readstatus    = {read},
}

@Article{ShalevShwartz2016,
  author        = {Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
  title         = {Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},
  year          = {2016},
  month         = oct,
  abstract      = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.},
  archiveprefix = {arXiv},
  eprint        = {1610.03295},
  file          = {:ShalevShwartz2016 - Safe, Multi Agent, Reinforcement Learning for Autonomous Driving.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
}

@Article{doi:10.1177/0278364913495721,
  author   = {Jens Kober and J. Andrew Bagnell and Jan Peters},
  journal  = {The International Journal of Robotics Research},
  title    = {Reinforcement learning in robotics: A survey},
  year     = {2013},
  number   = {11},
  pages    = {1238-1274},
  volume   = {32},
  abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
  doi      = {10.1177/0278364913495721},
  eprint   = {https://doi.org/10.1177/0278364913495721},
  file     = {:doi_10.1177_0278364913495721 - Reinforcement Learning in Robotics_ a Survey.pdf:PDF},
  groups   = {Robotics},
  ranking  = {rank5},
  url      = {https://doi.org/10.1177/0278364913495721},
}

@TechReport{yang2004multiagent,
  author      = {Yang, Erfu and Gu, Dongbing},
  institution = {tech. rep},
  title       = {Multiagent reinforcement learning for multi-robot systems: A survey},
  year        = {2004},
  file        = {:yang2004multiagent - Multiagent Reinforcement Learning for Multi Robot Systems_ a Survey.pdf:PDF},
  groups      = {Multi-robot},
}

@Article{Peng2017,
  author        = {Peng Peng and Ying Wen and Yaodong Yang and Quan Yuan and Zhenkun Tang and Haitao Long and Jun Wang},
  title         = {Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games},
  year          = {2017},
  month         = mar,
  abstract      = {Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.},
  archiveprefix = {arXiv},
  comment       = {BiCNet},
  eprint        = {1703.10069},
  file          = {:Peng2017 - Multiagent Bidirectionally Coordinated Nets_ Emergence of Human Level Coordination in Learning to Play StarCraft Combat Games.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
}

@InProceedings{FSS1511673,
  author     = {Matthew Hausknecht and Peter Stone},
  booktitle  = {Sequential Decision Making for Intelligent Agents Papers from the AAAI 2015 Fall Symposium},
  title      = {Deep Recurrent Q-Learning for Partially Observable MDPs},
  year       = {2015},
  abstract   = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  conference = {AAAI Fall Symposium Series},
  file       = {:FSS1511673 - Deep Recurrent Q Learning for Partially Observable MDPs (1).pdf:PDF},
  groups     = {POMDP},
  keywords   = {Deep Reinforcement Learning; LSTM; Deep Networks; Reinforcement Learning; Atari; Deep Q-Learning},
  ranking    = {rank4},
  readstatus = {skimmed},
  url        = {https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/11673},
}

@InProceedings{45823,
  author    = {David Ha and Andrew Dai and Quoc V. Le},
  booktitle = {ICLR 2017},
  title     = {HyperNetworks},
  year      = {2017},
  file      = {:45823 - HyperNetworks.pdf:PDF},
  ranking   = {rank3},
  url       = {https://openreview.net/pdf?id=rkpACe1lx},
}

@Article{rizk2019cooperative,
  author     = {Rizk, Yara and Awad, Mariette and Tunstel, Edward W},
  journal    = {ACM Computing Surveys (CSUR)},
  title      = {Cooperative heterogeneous multi-robot systems: A survey},
  year       = {2019},
  number     = {2},
  pages      = {1--31},
  volume     = {52},
  file       = {:rizk2019cooperative - Cooperative Heterogeneous Multi Robot Systems_ a Survey.pdf:PDF},
  groups     = {Multi-robot, Heterogeneous},
  publisher  = {ACM New York, NY, USA},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@InProceedings{kapturowski2018recurrent,
  author    = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle = {International conference on learning representations},
  title     = {Recurrent experience replay in distributed reinforcement learning},
  year      = {2018},
  comment   = {R2D2},
  file      = {:kapturowski2018recurrent - Recurrent Experience Replay in Distributed Reinforcement Learning.pdf:PDF},
  groups    = {Value based},
}

@PhdThesis{hausknecht2016cooperation,
  author = {Hausknecht, Matthew John},
  title  = {Cooperation and communication in multiagent deep reinforcement learning},
  year   = {2016},
  file   = {:HAUSKNECHT-DISSERTATION-2016.pdf:PDF},
  groups = {Communication},
}

@InProceedings{son2019qtran,
  author       = {Son, Kyunghwan and Kim, Daewoo and Kang, Wan Ju and Hostallero, David Earl and Yi, Yung},
  booktitle    = {International Conference on Machine Learning},
  title        = {Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning},
  year         = {2019},
  month        = may,
  organization = {PMLR},
  pages        = {5887--5896},
  file         = {:son2019qtran - Qtran_ Learning to Factorize with Transformation for Cooperative Multi Agent Reinforcement Learning.pdf:PDF},
  groups       = {Multi-agent RL},
  ranking      = {rank2},
  readstatus   = {read},
}

@InProceedings{Foerster2016a,
  author     = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
  booktitle  = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  title      = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  year       = {2016},
  address    = {Red Hook, NY, USA},
  pages      = {2145–2153},
  publisher  = {Curran Associates Inc.},
  series     = {NIPS'16},
  abstract   = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
  comment    = {DIAL},
  file       = {:10.5555_3157096.3157336 - Learning to Communicate with Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Communication},
  isbn       = {9781510838819},
  location   = {Barcelona, Spain},
  numpages   = {9},
  priority   = {prio1},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{schroeder2019multi,
  author  = {Schroeder de Witt, Christian and Foerster, Jakob and Farquhar, Gregory and Torr, Philip and Boehmer, Wendelin and Whiteson, Shimon},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Multi-agent common knowledge reinforcement learning},
  year    = {2019},
  pages   = {9927--9939},
  volume  = {32},
  comment = {MACKRL},
  file    = {:schroeder2019multi - Multi Agent Common Knowledge Reinforcement Learning (1).pdf:PDF},
  groups  = {Multi-agent RL},
  ranking = {rank1},
}

@Article{Witt2020a,
  author        = {Schroeder de Witt, Christian and Bei Peng and Pierre-Alexandre Kamienny and Philip Torr and Wendelin Böhmer and Shimon Whiteson},
  title         = {Deep Multi-Agent Reinforcement Learning for Decentralized Continuous Cooperative Control},
  year          = {2020},
  month         = mar,
  abstract      = {Centralised training with decentralised execution (CTDE) is an important learning paradigm in multi-agent reinforcement learning (MARL). To make progress in CTDE, we introduce Multi-Agent MuJoCo (MAMuJoCo), a novel benchmark suite that, unlike StarCraft Multi-Agent Challenge (SMAC), the predominant benchmark environment, applies to continuous robotic control tasks. To demonstrate the utility of MAMuJoCo, we present a range of benchmark results on this new suite, including comparing the state-of-the-art actor-critic method MADDPG against two novel variants of existing methods. These new methods outperform MADDPG on a number of MAMuJoCo tasks. In addition, we show that, in these continuous cooperative MAMuJoCo tasks, value factorisation plays a greater role in performance than the underlying algorithmic choices. This motivates the necessity of extending the study of value factorisations from $Q$-learning to actor-critic algorithms.},
  archiveprefix = {arXiv},
  comment       = {COMIX
FacMADDPG},
  eprint        = {2003.06709},
  file          = {:Witt2020a - Deep Multi Agent Reinforcement Learning for Decentralized Continuous Cooperative Control.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{levine2016end,
  author    = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal   = {The Journal of Machine Learning Research},
  title     = {End-to-end training of deep visuomotor policies},
  year      = {2016},
  number    = {1},
  pages     = {1334--1373},
  volume    = {17},
  file      = {:levine2016end - End to End Training of Deep Visuomotor Policies.pdf:PDF},
  groups    = {Robotics},
  publisher = {JMLR. org},
  ranking   = {rank5},
}

@InProceedings{matignon2007hysteretic,
  author       = {Matignon, La{\"e}titia and Laurent, Guillaume J and Le Fort-Piat, Nadine},
  booktitle    = {2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title        = {Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams},
  year         = {2007},
  organization = {IEEE},
  pages        = {64--69},
  file         = {:matignon2007hysteretic - Hysteretic Q Learning_ an Algorithm for Decentralized Reinforcement Learning in Cooperative Multi Agent Teams.pdf:PDF},
  groups       = {Multi-agent RL},
}

@Article{Sukhbaatar2017,
  author        = {Sainbayar Sukhbaatar and Zeming Lin and Ilya Kostrikov and Gabriel Synnaeve and Arthur Szlam and Rob Fergus},
  title         = {Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},
  year          = {2017},
  month         = mar,
  abstract      = {We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.},
  archiveprefix = {arXiv},
  eprint        = {1703.05407},
  file          = {:Sukhbaatar2017 - Intrinsic Motivation and Automatic Curricula Via Asymmetric Self Play.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
}

@InProceedings{He2016,
  author    = {He, He and Boyd-Graber, Jordan and Kwok, Kevin and Daum\'e, III, Hal},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  title     = {Opponent Modeling in Deep Reinforcement Learning},
  year      = {2016},
  address   = {New York, New York, USA},
  editor    = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  month     = {20--22 Jun},
  pages     = {1804--1813},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {48},
  abstract  = {Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because of strategies’ complex interaction and the non-stationary nature. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent’s action, we encode observation of the opponents into a deep Q-Network (DQN), while retaining explicit modeling under multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents even without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.},
  comment   = {DRON},
  file      = {:He2016 - Opponent Modeling in Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v48/he16.pdf},
  ranking   = {rank1},
  url       = {http://proceedings.mlr.press/v48/he16.html},
}

@InProceedings{10.5555/3237383.3238080,
  author     = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
  booktitle  = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
  year       = {2018},
  address    = {Richland, SC},
  pages      = {2085–2087},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '18},
  abstract   = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
  file       = {:10.5555_3237383.3238080 - Value Decomposition Networks for Cooperative Multi Agent Learning Based on Team Reward.pdf:PDF},
  groups     = {Multi-agent RL},
  keywords   = {neural networks, reinforcement learning, collaborative, q-learning, dqn, multi-agent, value-decomposition},
  location   = {Stockholm, Sweden},
  numpages   = {3},
  readstatus = {read},
}

@InProceedings{10.5555/3306127.3332052,
  author    = {Samvelyan, Mikayel and Rashid, Tabish and Schroeder de Witt, Christian and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
  booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {The StarCraft Multi-Agent Challenge},
  year      = {2019},
  address   = {Richland, SC},
  pages     = {2186–2188},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '19},
  abstract  = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.},
  file      = {:10.5555_3306127.3332052 - The StarCraft Multi Agent Challenge (1).pdf:PDF},
  groups    = {MAS Reviews},
  isbn      = {9781450363099},
  keywords  = {multi-agent learning, starcraft, reinforcement learning},
  location  = {Montreal QC, Canada},
  numpages  = {3},
}

@InProceedings{Boehmer2020,
  author    = {Boehmer, Wendelin and Kurin, Vitaly and Whiteson, Shimon},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Deep Coordination Graphs},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  month     = {13--18 Jul},
  pages     = {980--991},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {This paper introduces the deep coordination graph (DCG) for collaborative multi-agent reinforcement learning. DCG strikes a flexible trade-off between representational capacity and generalization by factoring the joint value function of all agents according to a coordination graph into payoffs between pairs of agents. The value can be maximized by local message passing along the graph, which allows training of the value function end-to-end with Q-learning. Payoff functions are approximated with deep neural networks that employ parameter sharing and low-rank approximations to significantly improve sample efficiency. We show that DCG can solve predator-prey tasks that highlight the relative overgeneralization pathology, as well as challenging StarCraft II micromanagement tasks.},
  file      = {:Boehmer2020 - Deep Coordination Graphs.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v119/boehmer20a/boehmer20a.pdf},
  ranking   = {rank1},
  url       = {http://proceedings.mlr.press/v119/boehmer20a.html},
}

@Article{Wei2018,
  author        = {Ermo Wei and Drew Wicke and David Freelan and Sean Luke},
  title         = {Multiagent Soft Q-Learning},
  year          = {2018},
  month         = apr,
  abstract      = {Policy gradient methods are often applied to reinforcement learning in continuous multiagent games. These methods perform local search in the joint-action space, and as we show, they are susceptable to a game-theoretic pathology known as relative overgeneralization. To resolve this issue, we propose Multiagent Soft Q-learning, which can be seen as the analogue of applying Q-learning to continuous controls. We compare our method to MADDPG, a state-of-the-art approach, and show that our method achieves better coordination in multiagent cooperative tasks, converging to better local optima in the joint action space.},
  archiveprefix = {arXiv},
  eprint        = {1804.09817},
  file          = {:Wei2018 - Multiagent Soft Q Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
}

@Article{wei2016lenient,
  author    = {Wei, Ermo and Luke, Sean},
  journal   = {The Journal of Machine Learning Research},
  title     = {Lenient learning in independent-learner stochastic cooperative games},
  year      = {2016},
  number    = {1},
  pages     = {2914--2955},
  volume    = {17},
  file      = {:wei2016lenient - Lenient Learning in Independent Learner Stochastic Cooperative Games.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {JMLR. org},
}

@PhdThesis{wiegand2003analysis,
  author  = {Wiegand, R Paul},
  school  = {Citeseer},
  title   = {An analysis of cooperative coevolutionary algorithms},
  year    = {2003},
  file    = {:wiegand2003analysis - An Analysis of Cooperative Coevolutionary Algorithms.pdf:PDF},
  groups  = {Multi-agent RL},
  ranking = {rank3},
}

@Article{WANG202068,
  author   = {Di Wang and Hongbin Deng and Zhenhua Pan},
  journal  = {Neurocomputing},
  title    = {MRCDRL: Multi-robot coordination with deep reinforcement learning},
  year     = {2020},
  issn     = {0925-2312},
  pages    = {68-76},
  volume   = {406},
  abstract = {This paper proposes a multi-robot cooperative algorithm based on deep reinforcement learning (MRCDRL). We use end-to-end methods to train directly from each robot-centered, relative perspective-generated image, and each robot’s reward as the input. During training, it is not necessary to specify the target position and movement path of each robot. MRCDRL learns the actions of each robot by training the neural network. MRCDRL uses the neural network structure that was modified from the Duel neural network structure. In the Duel network structure, there are two streams that each represents the state value function and the state-dependent action advantage function, and the results of the two streams are merged. The proposed method can solve the resource competition problem on the one hand and can solve the static and dynamic obstacle avoidance problems between multi-robot in real time on the other hand. Our new MRCDRL algorithm has higher accuracy and robustness than DQN and DDQN and can be effectively applied to multi-robot collaboration.},
  doi      = {https://doi.org/10.1016/j.neucom.2020.04.028},
  file     = {:1-s2.0-S0925231220305932-main.pdf:PDF},
  groups   = {Multi-robot},
  keywords = {Cooperative control, Machine learning, Image processing},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231220305932},
}

@Article{doi:10.1177/0278364917719333,
  author   = {Javier Alonso-Mora and Stuart Baker and Daniela Rus},
  journal  = {The International Journal of Robotics Research},
  title    = {Multi-robot formation control and object transport in dynamic environments via constrained optimization},
  year     = {2017},
  number   = {9},
  pages    = {1000-1021},
  volume   = {36},
  abstract = {We present a constrained optimization method for multi-robot formation control in dynamic environments, where the robots adjust the parameters of the formation, such as size and three-dimensional orientation, to avoid collisions with static and moving obstacles, and to make progress towards their goal. We describe two variants of the algorithm, one for local motion planning and one for global path planning. The local planner first computes a large obstacle-free convex region in a neighborhood of the robots, embedded in position-time space. Then, the parameters of the formation are optimized therein by solving a constrained optimization, via sequential convex programming. The robots navigate towards the optimized formation with individual controllers that account for their dynamics. The idea is extended to global path planning by sampling convex regions in free position space and connecting them if a transition in formation is possible - computed via the constrained optimization. The path of lowest cost to the goal is then found via graph search. The method applies to ground and aerial vehicles navigating in two- and three-dimensional environments among static and dynamic obstacles, allows for reconfiguration, and is efficient and scalable with the number of robots. In particular, we consider two applications, a team of aerial vehicles navigating in formation, and a small team of mobile manipulators that collaboratively carry an object. The approach is verified in experiments with a team of three mobile manipulators and in simulations with a team of up to sixteen Micro Air Vehicles (quadrotors).},
  doi      = {10.1177/0278364917719333},
  eprint   = {https://doi.org/10.1177/0278364917719333},
  file     = {:doi_10.1177_0278364917719333 - Multi Robot Formation Control and Object Transport in Dynamic Environments Via Constrained Optimization.pdf:PDF},
  groups   = {Multi-robot},
  url      = {https://doi.org/10.1177/0278364917719333},
}

@InProceedings{pmlr-v100-lin20a,
  author    = {Lin, Juntong and Yang, Xuyun and Zheng, Peiwei and Cheng, Hui},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Connectivity Guaranteed Multi-robot Navigation via Deep Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {661--670},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {This paper considers the multi-robot navigation problem where the geometric center of a multi-robot team aims to efficiently reach the waypoint without collisions in unknown complex environments while maintaining connectivity during the navigation. A novel Deep Reinforcement Learning (DRL)-based approach is proposed to derive end-to-end policies for the multi-robot navigation problem. In order to guarantee the connectivity during the navigation, a constraint satisfying parametric function (CSPF) is proposed to represent the navigation policy. Virtual policy extended environment (VP2E), an implementation framework of the CSPF is accompanied so as to make CSPF compatible with existing DRL techniques which rely on differentiable parametric functions. Both simulations and real-world experiments of a team of 3 holonomic robots are conducted to verify the effectiveness of the proposed DRL-based navigation method.},
  file      = {:pmlr-v100-lin20a - Connectivity Guaranteed Multi Robot Navigation Via Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-robot},
  pdf       = {http://proceedings.mlr.press/v100/lin20a/lin20a.pdf},
  url       = {http://proceedings.mlr.press/v100/lin20a.html},
}

@InProceedings{8461113,
  author    = {Long, Pinxin and Fan, Tingxiang and Liao, Xinyi and Liu, Wenxi and Zhang, Hao and Pan, Jia},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning},
  year      = {2018},
  pages     = {6252-6259},
  doi       = {10.1109/ICRA.2018.8461113},
  file      = {:8461113 - Towards Optimally Decentralized Multi Robot Collision Avoidance Via Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-robot},
  ranking   = {rank4},
}

@Article{8744599,
  author  = {Viseras, Alberto and Garcia, Ricardo},
  journal = {IEEE Robotics and Automation Letters},
  title   = {DeepIG: Multi-Robot Information Gathering With Deep Reinforcement Learning},
  year    = {2019},
  number  = {3},
  pages   = {3059-3066},
  volume  = {4},
  doi     = {10.1109/LRA.2019.2924839},
  file    = {:8744599 - DeepIG_ Multi Robot Information Gathering with Deep Reinforcement Learning.pdf:PDF},
  groups  = {Multi-robot, Information gathering},
}

@InProceedings{9288300,
  author    = {Yao, Shunyi and Chen, Guangda and Pan, Lifan and Ma, Jun and Ji, Jianmin and Chen, Xiaoping},
  booktitle = {2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)},
  title     = {Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning},
  year      = {2020},
  pages     = {532-539},
  doi       = {10.1109/ICTAI50040.2020.00088},
  file      = {:9288300 - Multi Robot Collision Avoidance with Map Based Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-robot},
}

@Article{Ma2020,
  author  = {Ma, Junchong and Lu, Huimin and Xiao, Junhao and Zeng, Zhiwen and Zheng, Zhiqiang},
  journal = {Journal of Intelligent & Robotic Systems},
  title   = {Multi-robot Target Encirclement Control with Collision Avoidance via Deep Reinforcement Learning},
  year    = {2020},
  month   = {08},
  volume  = {99},
  doi     = {10.1007/s10846-019-01106-x},
  file    = {:Ma2020 - Multi Robot Target Encirclement Control with Collision Avoidance Via Deep Reinforcement Learning.pdf:PDF},
  groups  = {Multi-robot},
}

@Article{7812634,
  author  = {Long, Pinxin and Liu, Wenxi and Pan, Jia},
  journal = {IEEE Robotics and Automation Letters},
  title   = {Deep-Learned Collision Avoidance Policy for Distributed Multiagent Navigation},
  year    = {2017},
  number  = {2},
  pages   = {656-663},
  volume  = {2},
  doi     = {10.1109/LRA.2017.2651371},
  file    = {:7812634 - Deep Learned Collision Avoidance Policy for Distributed Multiagent Navigation.pdf:PDF},
  groups  = {Multi-robot},
}

@Article{pierson2017deep,
  author    = {Pierson, Harry A and Gashler, Michael S},
  journal   = {Advanced Robotics},
  title     = {Deep learning in robotics: a review of recent research},
  year      = {2017},
  number    = {16},
  pages     = {821--835},
  volume    = {31},
  file      = {:pierson2017deep - Deep Learning in Robotics_ a Review of Recent Research.pdf:PDF},
  groups    = {Multi-robot},
  publisher = {Taylor \& Francis},
}

@InProceedings{elfakharany2020towards,
  author       = {Elfakharany, A and Yusof, R and Ismail, Z},
  booktitle    = {Journal of Physics: Conference Series},
  title        = {Towards Multi Robot Task Allocation and Navigation using Deep Reinforcement Learning},
  year         = {2020},
  number       = {1},
  organization = {IOP Publishing},
  pages        = {012045},
  volume       = {1447},
  file         = {:elfakharany2020towards - Towards Multi Robot Task Allocation and Navigation Using Deep Reinforcement Learning.pdf:PDF},
  groups       = {Multi-robot},
}

@InProceedings{7989250,
  author    = {Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Learning modular neural network policies for multi-task and multi-robot transfer},
  year      = {2017},
  pages     = {2169-2176},
  doi       = {10.1109/ICRA.2017.7989250},
  file      = {:7989250 - Learning Modular Neural Network Policies for Multi Task and Multi Robot Transfer.pdf:PDF},
  groups    = {Multi-robot, Multi-task},
  ranking   = {rank2},
}

@InProceedings{8460655,
  author    = {Kahn, Gregory and Villaflor, Adam and Ding, Bosen and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation},
  year      = {2018},
  pages     = {5129-5136},
  doi       = {10.1109/ICRA.2018.8460655},
  file      = {:8460655 - Self Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation.pdf:PDF},
  groups    = {Navigation},
}

@InProceedings{pmlr-v100-yu20a,
  author    = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {1094--1100},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multitask learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.1.},
  file      = {:pmlr-v100-yu20a - Meta World_ a Benchmark and Evaluation for Multi Task and Meta Reinforcement Learning.pdf:PDF},
  groups    = {Multi-task},
  pdf       = {http://proceedings.mlr.press/v100/yu20a/yu20a.pdf},
  url       = {http://proceedings.mlr.press/v100/yu20a.html},
}

@InProceedings{8202141,
  author    = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Collective robot reinforcement learning with distributed asynchronous guided policy search},
  year      = {2017},
  pages     = {79-86},
  doi       = {10.1109/IROS.2017.8202141},
  file      = {:8202141 - Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:PDF},
  groups    = {Multi-robot},
}

@InProceedings{pmlr-v100-dasari20a,
  author    = {Dasari, Sudeep and Ebert, Frederik and Tian, Stephen and Nair, Suraj and Bucher, Bernadette and Schmeckpeper, Karl and Singh, Siddharth and Levine, Sergey and Finn, Chelsea},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {RoboNet: Large-Scale Multi-Robot Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {885--897},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms’ ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data.1},
  file      = {:pmlr-v100-dasari20a - RoboNet_ Large Scale Multi Robot Learning.pdf:PDF},
  groups    = {Multi-robot},
  pdf       = {http://proceedings.mlr.press/v100/dasari20a/dasari20a.pdf},
  url       = {http://proceedings.mlr.press/v100/dasari20a.html},
}

@InProceedings{pmlr-v100-ahn20a,
  author    = {Ahn, Michael and Zhu, Henry and Hartikainen, Kristian and Ponte, Hugo and Gupta, Abhishek and Levine, Sergey and Kumar, Vikash},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {1300--1313},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {ROBEL is an open-source platform of cost-effective robots designed for reinforcement learning in the real world. ROBEL introduces two robots, each aimed to accelerate reinforcement learning research in different task domains: D’Claw is a three-fingered hand robot that facilitates learning dexterous manipulation tasks, and D’Kitty is a four-legged robot that facilitates learning agile legged locomotion tasks. These low-cost, modular robots are easy to maintain and are robust enough to sustain on-hardware reinforcement learning from scratch with over 14000 training hours registered on them to date. To leverage this platform, we propose an extensible set of continuous control benchmark tasks for each robot. These tasks feature dense and sparse task objectives, and additionally introduce score metrics for hardware-safety. We provide benchmark scores on an initial set of tasks using a variety of learning-based methods. Furthermore, we show that these results can be replicated across copies of the robots located in different institutions. Code, documentation, design files, detailed assembly instructions, trained policies, baseline details, task videos, and all supplementary materials required to reproduce the results are available at www.roboticsbenchmarks.org.},
  file      = {:pmlr-v100-ahn20a - ROBEL_ Robotics Benchmarks for Learning with Low Cost Robots.pdf:PDF},
  groups    = {Robotics},
  pdf       = {http://proceedings.mlr.press/v100/ahn20a/ahn20a.pdf},
  url       = {http://proceedings.mlr.press/v100/ahn20a.html},
}

@InProceedings{pmlr-v87-kahn18a,
  author    = {Kahn, Gregory and Villaflor, Adam and Abbeel, Pieter and Levine, Sergey},
  booktitle = {Proceedings of The 2nd Conference on Robot Learning},
  title     = {Composable Action-Conditioned Predictors: Flexible Off-Policy Learning for Robot Navigation},
  year      = {2018},
  editor    = {Aude Billard and Anca Dragan and Jan Peters and Jun Morimoto},
  month     = {29--31 Oct},
  pages     = {806--816},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {87},
  abstract  = {A general-purpose intelligent robot must be able to learn autonomously and be able to accomplish multiple tasks in order to be deployed in the real world. However, standard reinforcement learning approaches learn separate task-specific policies and assume the reward function for each task is known a priori. We propose a framework that learns event cues from off-policy data, and can flexibly combine these event cues at test time to accomplish different tasks. These event cue labels are not assumed to be known a priori, but are instead labeled using learned models, such as computer vision detectors, and then “backed up” in time using an action-conditioned predictive model. We show that a simulated robotic car and a real-world RC car can gather data and train fully autonomously without any human-provided labels beyond those needed to train the detectors, and then at test-time be able to accomplish a variety of different tasks. Videos of the experiments and code can be found at github.com/gkahn13/CAPs},
  file      = {:pmlr-v87-kahn18a - Composable Action Conditioned Predictors_ Flexible off Policy Learning for Robot Navigation.pdf:PDF},
  groups    = {Navigation},
  pdf       = {http://proceedings.mlr.press/v87/kahn18a/kahn18a.pdf},
  url       = {http://proceedings.mlr.press/v87/kahn18a.html},
}

@Article{Ibarz2021,
  author     = {Julian Ibarz and Jie Tan and Chelsea Finn and Mrinal Kalakrishnan and Peter Pastor and Sergey Levine},
  journal    = {The International Journal of Robotics Research},
  title      = {How to train your robot with deep reinforcement learning: lessons we have learned},
  year       = {2021},
  number     = {0},
  pages      = {0278364920987859},
  volume     = {0},
  abstract   = {Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low-level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time, real-world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn: as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.},
  doi        = {10.1177/0278364920987859},
  eprint     = {https://doi.org/10.1177/0278364920987859},
  file       = {:0278364920987859.pdf:PDF},
  groups     = {Robotics},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1177/0278364920987859},
}

@InProceedings{7487175,
  author    = {Zhang, Tianhao and Kahn, Gregory and Levine, Sergey and Abbeel, Pieter},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search},
  year      = {2016},
  pages     = {528-535},
  doi       = {10.1109/ICRA.2016.7487175},
  file      = {:7487175 - Learning Deep Control Policies for Autonomous Aerial Vehicles with MPC Guided Policy Search.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank4},
}

@InProceedings{charrow2015information,
  author       = {Charrow, Benjamin and Kahn, Gregory and Patil, Sachin and Liu, Sikang and Goldberg, Ken and Abbeel, Pieter and Michael, Nathan and Kumar, Vijay},
  booktitle    = {Robotics: Science and Systems},
  title        = {Information-Theoretic Planning with Trajectory Optimization for Dense 3D Mapping},
  year         = {2015},
  organization = {Rome},
  pages        = {3--12},
  volume       = {11},
  file         = {:charrow2015information - Information Theoretic Planning with Trajectory Optimization for Dense 3D Mapping..pdf:PDF},
  groups       = {Information gathering},
}

@InProceedings{7989379,
  author    = {Kahn, Gregory and Zhang, Tianhao and Levine, Sergey and Abbeel, Pieter},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {PLATO: Policy learning using adaptive trajectory optimization},
  year      = {2017},
  month     = {May},
  pages     = {3342-3349},
  abstract  = {Policy search can in principle acquire complex strategies for control of robots and other autonomous systems. When the policy is trained to process raw sensory inputs, such as images and depth maps, it can also acquire a strategy that combines perception and control. However, effectively processing such complex inputs requires an expressive policy class, such as a large neural network. These high-dimensional policies are difficult to train, especially when learning to control safety-critical systems. We propose PLATO, a continuous, reset-free reinforcement learning algorithm that trains complex control policies with supervised learning, using model-predictive control (MPC) to generate the supervision, hence never in need of running a partially trained and potentially unsafe policy. PLATO uses an adaptive training method to modify the behavior of MPC to gradually match the learned policy in order to generate training samples at states that are likely to be visited by the learned policy. PLATO also maintains the MPC cost as an objective to avoid highly undesirable actions that would result from strictly following the learned policy before it has been fully trained. We prove that this type of adaptive MPC expert produces supervision that leads to good long-horizon performance of the resulting policy. We also empirically demonstrate that MPC can still avoid dangerous on-policy actions in unexpected situations during training. Our empirical results on a set of challenging simulated aerial vehicle tasks demonstrate that, compared to prior methods, PLATO learns faster, experiences substantially fewer catastrophic failures (crashes) during training, and often converges to a better policy.},
  doi       = {10.1109/ICRA.2017.7989379},
  file      = {:7989379 - PLATO_ Policy Learning Using Adaptive Trajectory Optimization.pdf:PDF},
  groups    = {Navigation},
}

@InProceedings{8793735,
  author    = {Kang, Katie and Belkhale, Suneel and Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title     = {Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight},
  year      = {2019},
  month     = {May},
  pages     = {6008-6014},
  abstract  = {Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS.},
  doi       = {10.1109/ICRA.2019.8793735},
  file      = {:8793735 - Generalization through Simulation_ Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision Based Autonomous Flight.pdf:PDF},
  groups    = {Navigation, Sim to Real},
  issn      = {2577-087X},
  ranking   = {rank1},
}

@Article{9345970,
  author   = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {BADGR: An Autonomous Self-Supervised Learning-Based Navigation System},
  year     = {2021},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {1312-1319},
  volume   = {6},
  abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our reinforcement learning approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with autonomously-labeled off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr.},
  doi      = {10.1109/LRA.2021.3057023},
  file     = {:9345970 - BADGR_ an Autonomous Self Supervised Learning Based Navigation System.pdf:PDF},
  groups   = {Navigation},
}

@Article{Queralta2020,
  author        = {Jorge Peña Queralta and Jussi Taipalmaa and Bilge Can Pullinen and Victor Kathan Sarker and Tuan Nguyen Gia and Hannu Tenhunen and Moncef Gabbouj and Jenni Raitoharju and Tomi Westerlund},
  title         = {Collaborative Multi-Robot Systems for Search and Rescue: Coordination and Perception},
  year          = {2020},
  month         = aug,
  abstract      = {Autonomous or teleoperated robots have been playing increasingly important roles in civil applications in recent years. Across the different civil domains where robots can support human operators, one of the areas where they can have more impact is in search and rescue (SAR) operations. In particular, multi-robot systems have the potential to significantly improve the efficiency of SAR personnel with faster search of victims, initial assessment and mapping of the environment, real-time monitoring and surveillance of SAR operations, or establishing emergency communication networks, among other possibilities. SAR operations encompass a wide variety of environments and situations, and therefore heterogeneous and collaborative multi-robot systems can provide the most advantages. In this paper, we review and analyze the existing approaches to multi-robot SAR support, from an algorithmic perspective and putting an emphasis on the methods enabling collaboration among the robots as well as advanced perception through machine vision and multi-agent active perception. Furthermore, we put these algorithms in the context of the different challenges and constraints that various types of robots (ground, aerial, surface or underwater) encounter in different SAR environments (maritime, urban, wilderness or other post-disaster scenarios). This is, to the best of our knowledge, the first review considering heterogeneous SAR robots across different environments, while giving two complimentary points of view: control mechanisms and machine perception. Based on our review of the state-of-the-art, we discuss the main open research questions, and outline our insights on the current approaches that have potential to improve the real-world performance of multi-robot SAR systems.},
  archiveprefix = {arXiv},
  eprint        = {2008.12610},
  file          = {:Queralta2020 - Collaborative Multi Robot Systems for Search and Rescue_ Coordination and Perception.pdf:PDF},
  groups        = {Multi-robot, Information gathering, Heterogeneous},
  keywords      = {cs.RO, cs.LG, cs.MA},
  primaryclass  = {cs.RO},
  ranking       = {rank2},
}

@Article{9392290,
  author   = {Thananjeyan, Brijen and Balakrishna, Ashwin and Nair, Suraj and Luo, Michael and Srinivasan, Krishnan and Hwang, Minho and Gonzalez, Joseph E. and Ibarz, Julian and Finn, Chelsea and Goldberg, Ken},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones},
  year     = {2021},
  issn     = {2377-3766},
  month    = {July},
  number   = {3},
  pages    = {4915-4922},
  volume   = {6},
  abstract = {Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2–20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.},
  doi      = {10.1109/LRA.2021.3070252},
  file     = {:9392290 - Recovery RL_ Safe Reinforcement Learning with Learned Recovery Zones.pdf:PDF},
  groups   = {Robotics},
}

@Article{almadhoun2019survey,
  author    = {Almadhoun, Randa and Taha, Tarek and Seneviratne, Lakmal and Zweiri, Yahya},
  journal   = {SN Applied Sciences},
  title     = {A survey on multi-robot coverage path planning for model reconstruction and mapping},
  year      = {2019},
  number    = {8},
  pages     = {1--24},
  volume    = {1},
  file      = {:almadhoun2019survey - A Survey on Multi Robot Coverage Path Planning for Model Reconstruction and Mapping.pdf:PDF},
  groups    = {Multi-robot, Coverage Path Planning},
  publisher = {Springer},
}

@InProceedings{8202312,
  author    = {Chen, Yu Fan and Everett, Michael and Liu, Miao and How, Jonathan P.},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Socially aware motion planning with deep reinforcement learning},
  year      = {2017},
  month     = {Sep.},
  pages     = {1343-1350},
  abstract  = {For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules (e.g., passing on the right). However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.},
  doi       = {10.1109/IROS.2017.8202312},
  file      = {:8202312 - Socially Aware Motion Planning with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation, Multi-robot},
  issn      = {2153-0866},
}

@InProceedings{7989037,
  author    = {Chen, Yu Fan and Liu, Miao and Everett, Michael and How, Jonathan P.},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning},
  year      = {2017},
  month     = {May},
  pages     = {285-292},
  abstract  = {Finding feasible, collision-free paths for multiagent systems can be challenging, particularly in non-communicating scenarios where each agent's intent (e.g. goal) is unobservable to the others. In particular, finding time efficient paths often requires anticipating interaction with neighboring agents, the process of which can be computationally prohibitive. This work presents a decentralized multiagent collision avoidance algorithm based on a novel application of deep reinforcement learning, which effectively offloads the online computation (for predicting interaction patterns) to an offline learning procedure. Specifically, the proposed approach develops a value network that encodes the estimated time to the goal given an agent's joint configuration (positions and velocities) with its neighbors. Use of the value network not only admits efficient (i.e., real-time implementable) queries for finding a collision-free velocity vector, but also considers the uncertainty in the other agents' motion. Simulation results show more than 26% improvement in paths quality (i.e., time to reach the goal) when compared with optimal reciprocal collision avoidance (ORCA), a state-of-the-art collision avoidance strategy.},
  doi       = {10.1109/ICRA.2017.7989037},
  file      = {:7989037 - Decentralized Non Communicating Multiagent Collision Avoidance with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation, Multi-robot},
}

@InProceedings{7989179,
  author    = {Paull, Liam and Tani, Jacopo and Ahn, Heejin and Alonso-Mora, Javier and Carlone, Luca and Cap, Michal and Chen, Yu Fan and Choi, Changhyun and Dusek, Jeff and Fang, Yajun and Hoehener, Daniel and Liu, Shih-Yuan and Novitzky, Michael and Okuyama, Igor Franzoni and Pazis, Jason and Rosman, Guy and Varricchio, Valerio and Wang, Hsueh-Cheng and Yershov, Dmitry and Zhao, Hang and Benjamin, Michael and Carr, Christopher and Zuber, Maria and Karaman, Sertac and Frazzoli, Emilio and Del Vecchio, Domitilla and Rus, Daniela and How, Jonathan and Leonard, John and Censi, Andrea},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Duckietown: An open, inexpensive and flexible platform for autonomy education and research},
  year      = {2017},
  month     = {May},
  pages     = {1497-1504},
  abstract  = {Duckietown is an open, inexpensive and flexible platform for autonomy education and research. The platform comprises small autonomous vehicles (“Duckiebots”) built from off-the-shelf components, and cities (“Duckietowns”) complete with roads, signage, traffic lights, obstacles, and citizens (duckies) in need of transportation. The Duckietown platform offers a wide range of functionalities at a low cost. Duckiebots sense the world with only one monocular camera and perform all processing onboard with a Raspberry Pi 2, yet are able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots, localize within a global map, navigate a city, and coordinate with other Duckiebots to avoid collisions. Duckietown is a useful tool since educators and researchers can save money and time by not having to develop all of the necessary supporting infrastructure and capabilities. All materials are available as open source, and the hope is that others in the community will adopt the platform for education and research.},
  doi       = {10.1109/ICRA.2017.7989179},
  file      = {:7989179 - Duckietown_ an Open, Inexpensive and Flexible Platform for Autonomy Education and Research.pdf:PDF},
  groups    = {Platforms},
}

@InProceedings{7989677,
  author    = {Lopez, Brett T. and How, Jonathan P.},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Aggressive 3-D collision avoidance for high-speed navigation},
  year      = {2017},
  month     = {May},
  pages     = {5759-5765},
  abstract  = {Autonomous robot navigation through unknown, cluttered environments at high-speeds is still an open problem. Quadrotor platforms with this capability have only begun to emerge with the advancements in light-weight, small form factor sensing and computing. Many of the existing platforms, however, require excessive computation time to perform collision avoidance, which ultimately limits the vehicle's top speed. This work presents an efficient perception and planning approach that significantly reduces the computation time by using instantaneous perception data for collision avoidance. Minimum-time, state and input constrained motion primitives are generated by sampling terminal states until a collision-free path is found. The worst case performance of the Triple Integrator Planner (TIP) is nearly an order of magnitude faster than the state-of-the-art. Experimental results demonstrate the algorithm's ability to plan and execute aggressive collision avoidance maneuvers in highly cluttered environments.},
  doi       = {10.1109/ICRA.2017.7989677},
  file      = {:7989677 - Aggressive 3 D Collision Avoidance for High Speed Navigation.pdf:PDF},
  groups    = {Navigation},
}

@Article{doi:10.1177/0278364917692864,
  author   = {Shayegan Omidshafiei and Ali–Akbar Agha–Mohammadi and Christopher Amato and Shih–Yuan Liu and Jonathan P How and John Vian},
  journal  = {The International Journal of Robotics Research},
  title    = {Decentralized control of multi-robot partially observable Markov decision processes using belief space macro-actions},
  year     = {2017},
  number   = {2},
  pages    = {231-258},
  volume   = {36},
  abstract = {This work focuses on solving general multi-robot planning problems in continuous spaces with partial observability given a high-level domain description. Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are general models for multi-robot coordination problems. However, representing and solving Dec-POMDPs is often intractable for large problems. This work extends the Dec-POMDP model to the Decentralized Partially Observable Semi-Markov Decision Process (Dec-POSMDP) to take advantage of the high-level representations that are natural for multi-robot problems and to facilitate scalable solutions to large discrete and continuous problems. The Dec-POSMDP formulation uses task macro-actions created from lower-level local actions that allow for asynchronous decision-making by the robots, which is crucial in multi-robot domains. This transformation from Dec-POMDPs to Dec-POSMDPs with a finite set of automatically-generated macro-actions allows use of efficient discrete-space search algorithms to solve them. The paper presents algorithms for solving Dec-POSMDPs, which are more scalable than previous methods since they can incorporate closed-loop belief space macro-actions in planning. These macro-actions are automatically constructed to produce robust solutions. The proposed algorithms are then evaluated on a complex multi-robot package delivery problem under uncertainty, showing that our approach can naturally represent realistic problems and provide high-quality solutions for large-scale problems.},
  doi      = {10.1177/0278364917692864},
  eprint   = {https://doi.org/10.1177/0278364917692864},
  file     = {:doi_10.1177_0278364917692864 - Decentralized Control of Multi Robot Partially Observable Markov Decision Processes Using Belief Space Macro Actions.pdf:PDF},
  groups   = {Multi-robot},
  priority = {prio2},
  url      = {https://doi.org/10.1177/0278364917692864},
}

@Article{amato2019modeling,
  author     = {Amato, Christopher and Konidaris, George and Kaelbling, Leslie P and How, Jonathan P},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {Modeling and planning with macro-actions in decentralized POMDPs},
  year       = {2019},
  pages      = {817--859},
  volume     = {64},
  file       = {:amato2019modeling - Modeling and Planning with Macro Actions in Decentralized POMDPs (1).pdf:PDF},
  groups     = {Multi-robot},
  readstatus = {skimmed},
}

@Article{9001226,
  author   = {Semnani, Samaneh Hosseini and Liu, Hugh and Everett, Michael and de Ruiter, Anton and How, Jonathan P.},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Multi-Agent Motion Planning for Dense and Dynamic Environments via Deep Reinforcement Learning},
  year     = {2020},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {3221-3226},
  volume   = {5},
  abstract = {This letter introduces a hybrid algorithm of deep reinforcement learning (RL) and Force-based motion planning (FMP) to solve distributed motion planning problem in dense and dynamic environments. Individually, RL and FMP algorithms each have their own limitations. FMP is not able to produce time-optimal paths and existing RL solutions are not able to produce collision free paths in dense environments. Therefore, we first tried improving the performance of recent RL approaches by introducing a new reward function that not only eliminates the requirement of a pre supervised learning (SL) step but also decreases the chance of collision in crowded environments. That improved things, but there were still a lot of failure cases. So, we developed a hybrid approach to leverage the simpler FMP approach in stuck, simple and high-risk cases, and continue using RL for normal cases in which FMP can't produce optimal path. Also, we extend GA3CCADRL algorithm to 3D environment. Simulation results show that the proposed algorithm outperforms both deep RL and FMP algorithms and produces up to 50% more successful scenarios than deep RL and up to 75% less extra time to reach goal than FMP.},
  comment  = {GA3C-CADRL / GA3C-CADRL-NSL},
  doi      = {10.1109/LRA.2020.2974695},
  file     = {:9001226 - Multi Agent Motion Planning for Dense and Dynamic Environments Via Deep Reinforcement Learning.pdf:PDF},
  groups   = {Multi-robot},
  ranking  = {rank3},
}

@InProceedings{8593871,
  author    = {Everett, Michael and Chen, Yu Fan and How, Jonathan P.},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning},
  year      = {2018},
  month     = {Oct},
  pages     = {3052-3059},
  abstract  = {Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efficient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents' behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a fixed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed.},
  comment   = {GA3C-CADRL},
  doi       = {10.1109/IROS.2018.8593871},
  file      = {:8593871 - Motion Planning among Dynamic, Decision Making Agents with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation},
  issn      = {2153-0866},
  ranking   = {rank3},
}

@Article{9317723,
  author   = {Everett, Michael and Chen, Yu Fan and How, Jonathan P.},
  journal  = {IEEE Access},
  title    = {Collision Avoidance in Pedestrian-Rich Environments With Deep Reinforcement Learning},
  year     = {2021},
  issn     = {2169-3536},
  pages    = {10357-10377},
  volume   = {9},
  abstract = {Collision avoidance algorithms are essential for safe and efficient robot operation among pedestrians. This work proposes using deep reinforcement (RL) learning as a framework to model the complex interactions and cooperation with nearby, decision-making agents, such as pedestrians and other robots. Existing RL-based works assume homogeneity of agent properties, use specific motion models over short timescales, or lack a principled method to handle a large, possibly varying number of agents. Therefore, this work develops an algorithm that learns collision avoidance among a variety of heterogeneous, non-communicating, dynamic agents without assuming they follow any particular behavior rules. It extends our previous work by introducing a strategy using Long Short-Term Memory (LSTM) that enables the algorithm to use observations of an arbitrary number of other agents, instead of a small, fixed number of neighbors. The proposed algorithm is shown to outperform a classical collision avoidance algorithm, another deep RL-based algorithm, and scales with the number of agents better (fewer collisions, shorter time to goal) than our previously published learning-based approach. Analysis of the LSTM provides insights into how observations of nearby agents affect the hidden state and quantifies the performance impact of various agent ordering heuristics. The learned policy generalizes to several applications beyond the training scenarios: formation control (arrangement into letters), demonstrations on a fleet of four multirotors and on a fully autonomous robotic vehicle capable of traveling at human walking speed among pedestrians.},
  doi      = {10.1109/ACCESS.2021.3050338},
  file     = {:09317723.pdf:PDF},
  groups   = {Navigation},
}

@Article{Kahn2017a,
  author        = {Gregory Kahn and Adam Villaflor and Vitchyr Pong and Pieter Abbeel and Sergey Levine},
  title         = {Uncertainty-Aware Reinforcement Learning for Collision Avoidance},
  year          = {2017},
  month         = feb,
  abstract      = {Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a real-world RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll.},
  archiveprefix = {arXiv},
  eprint        = {1702.01182},
  file          = {:- Uncertainty Aware Reinforcement Learning for Collision Avoidance.pdf:PDF},
  groups        = {Navigation},
  keywords      = {cs.LG, cs.RO},
  primaryclass  = {cs.LG},
}

@InProceedings{8202134,
  author    = {Tai, Lei and Paolo, Giuseppe and Liu, Ming},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation},
  year      = {2017},
  month     = {Sep.},
  pages     = {31-36},
  abstract  = {We present a learning-based mapless motion planner by taking the sparse 10-dimensional range findings and the target position with respect to the mobile robot coordinate frame as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the obstacle map of the navigation environment where both the highly precise laser sensor and the obstacle map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.},
  doi       = {10.1109/IROS.2017.8202134},
  file      = {:8202134 - Virtual to Real Deep Reinforcement Learning_ Continuous Control of Mobile Robots for Mapless Navigation.pdf:PDF},
  groups    = {Navigation, Sim to Real},
  issn      = {2153-0866},
  ranking   = {rank2},
}

@InProceedings{Tobin2017,
  author    = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Domain randomization for transferring deep neural networks from simulation to the real world},
  year      = {2017},
  month     = {Sep.},
  pages     = {23-30},
  abstract  = {Bridging the `reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  doi       = {10.1109/IROS.2017.8202133},
  file      = {:8202133 - Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.pdf:PDF},
  groups    = {Sim to Real},
  issn      = {2153-0866},
  ranking   = {rank5},
}

@InProceedings{7989381,
  author    = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Target-driven visual navigation in indoor scenes using deep reinforcement learning},
  year      = {2017},
  month     = {May},
  pages     = {3357-3364},
  abstract  = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.},
  doi       = {10.1109/ICRA.2017.7989381},
  file      = {:7989381 - Target Driven Visual Navigation in Indoor Scenes Using Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank5},
}

@Article{Doncieux2020,
  author        = {Stephane Doncieux and Nicolas Bredeche and Léni Le Goff and Benoît Girard and Alexandre Coninx and Olivier Sigaud and Mehdi Khamassi and Natalia Díaz-Rodríguez and David Filliat and Timothy Hospedales and A. Eiben and Richard Duro},
  title         = {DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics},
  year          = {2020},
  month         = may,
  abstract      = {Robots are still limited to controlled conditions, that the robot designer knows with enough details to endow the robot with the appropriate models or behaviors. Learning algorithms add some flexibility with the ability to discover the appropriate behavior given either some demonstrations or a reward to guide its exploration with a reinforcement learning algorithm. Reinforcement learning algorithms rely on the definition of state and action spaces that define reachable behaviors. Their adaptation capability critically depends on the representations of these spaces: small and discrete spaces result in fast learning while large and continuous spaces are challenging and either require a long training period or prevent the robot from converging to an appropriate behavior. Beside the operational cycle of policy execution and the learning cycle, which works at a slower time scale to acquire new policies, we introduce the redescription cycle, a third cycle working at an even slower time scale to generate or adapt the required representations to the robot, its environment and the task. We introduce the challenges raised by this cycle and we present DREAM (Deferred Restructuring of Experience in Autonomous Machines), a developmental cognitive architecture to bootstrap this redescription process stage by stage, build new state representations with appropriate motivations, and transfer the acquired knowledge across domains or tasks or even across robots. We describe results obtained so far with this approach and end up with a discussion of the questions it raises in Neuroscience.},
  archiveprefix = {arXiv},
  eprint        = {2005.06223},
  file          = {:Doncieux2020 - DREAM Architecture_ a Developmental Approach to Open Ended Learning in Robotics.pdf:PDF},
  groups        = {Multi-task},
  keywords      = {cs.AI, cs.LG, cs.NE, cs.RO},
  primaryclass  = {cs.AI},
}

@Article{JUN2016325,
  author   = {Jae-Yun Jun and Jean-Philippe Saut and Faïz Benamar},
  journal  = {Robotics and Autonomous Systems},
  title    = {Pose estimation-based path planning for a tracked mobile robot traversing uneven terrains},
  year     = {2016},
  issn     = {0921-8890},
  pages    = {325-339},
  volume   = {75},
  abstract = {A novel path-planning algorithm is proposed for a tracked mobile robot to traverse uneven terrains, which can efficiently search for stability sub-optimal paths. This algorithm consists of combining two RRT-like algorithms (the Transition-based RRT (T-RRT) and the Dynamic-Domain RRT (DD-RRT) algorithms) bidirectionally and of representing the robot–terrain interaction with the robot’s quasi-static tip-over stability measure (assuming that the robot traverses uneven terrains at low speed for safety). The robot’s stability is computed by first estimating the robot’s pose, which in turn is interpreted as a contact problem, formulated as a linear complementarity problem (LCP), and solved using the Lemke’s method (which guarantees a fast convergence). The present work compares the performance of the proposed algorithm to other RRT-like algorithms (in terms of planning time, rate of success in finding solutions and the associated cost values) over various uneven terrains and shows that the proposed algorithm can be advantageous over its counterparts in various aspects of the planning performance.},
  doi      = {https://doi.org/10.1016/j.robot.2015.09.014},
  file     = {:JUN2016325 - Pose Estimation Based Path Planning for a Tracked Mobile Robot Traversing Uneven Terrains.pdf:PDF},
  groups   = {Navigation},
  keywords = {Path planning, Rough terrain, Sampling-based motion planning, Mobile robot, Tip-over stability},
  url      = {https://www.sciencedirect.com/science/article/pii/S092188901500202X},
}

@Article{Sunderhauf2018,
  author     = {Niko Sünderhauf and Oliver Brock and Walter Scheirer and Raia Hadsell and Dieter Fox and Jürgen Leitner and Ben Upcroft and Pieter Abbeel and Wolfram Burgard and Michael Milford and Peter Corke},
  journal    = {The International Journal of Robotics Research},
  title      = {The limits and potentials of deep learning for robotics},
  year       = {2018},
  number     = {4-5},
  pages      = {405-420},
  volume     = {37},
  abstract   = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.},
  doi        = {10.1177/0278364918770733},
  eprint     = {https://doi.org/10.1177/0278364918770733},
  file       = {:doi_10.1177_0278364918770733 - The Limits and Potentials of Deep Learning for Robotics.pdf:PDF},
  groups     = {Robotics},
  ranking    = {rank3},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1177/0278364918770733},
}

@Article{charrow2014approximate,
  author    = {Charrow, Benjamin and Kumar, Vijay and Michael, Nathan},
  journal   = {Autonomous Robots},
  title     = {Approximate representations for multi-robot control policies that maximize mutual information},
  year      = {2014},
  number    = {4},
  pages     = {383--400},
  volume    = {37},
  file      = {:charrow2014approximate - Approximate Representations for Multi Robot Control Policies That Maximize Mutual Information.pdf:PDF},
  groups    = {Information gathering, Multi-robot},
  publisher = {Springer},
}

@InCollection{schwager2017multi,
  author    = {Schwager, Mac and Dames, Philip and Rus, Daniela and Kumar, Vijay},
  booktitle = {Robotics research},
  publisher = {Springer},
  title     = {A multi-robot control policy for information gathering in the presence of unknown hazards},
  year      = {2017},
  pages     = {455--472},
  file      = {:schwager2017multi - A Multi Robot Control Policy for Information Gathering in the Presence of Unknown Hazards.pdf:PDF},
  groups    = {Information gathering, Multi-robot},
}

@InProceedings{7139865,
  author    = {Charrow, Benjamin and Liu, Sikang and Kumar, Vijay and Michael, Nathan},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Information-theoretic mapping using Cauchy-Schwarz Quadratic Mutual Information},
  year      = {2015},
  month     = {May},
  pages     = {4791-4798},
  abstract  = {We develop a computationally efficient control policy for active perception that incorporates explicit models of sensing and mobility to build 3D maps with ground and aerial robots. Like previous work, our policy maximizes an information-theoretic objective function between the discrete occupancy belief distribution (e.g., voxel grid) and future measurements that can be made by mobile sensors. However, our work is unique in three ways. First, we show that by using Cauchy-Schwarz Quadratic Mutual Information (CSQMI), we get significant gains in efficiency. Second, while most previous methods adopt a myopic, gradient-following approach that yields poor convergence properties, our algorithm searches over a set of paths and is less susceptible to local minima. In doing so, we explicitly incorporate models of sensors, and model the dependence (and independence) of measurements over multiple time steps in a path. Third, because we consider models of sensing and mobility, our method naturally applies to both ground and aerial vehicles. The paper describes the basic models, the problem formulation and the algorithm, and demonstrates applications via simulation and experimentation.},
  doi       = {10.1109/ICRA.2015.7139865},
  file      = {:7139865 - Information Theoretic Mapping Using Cauchy Schwarz Quadratic Mutual Information.pdf:PDF},
  groups    = {Information gathering},
  issn      = {1050-4729},
}

@InProceedings{Roberts_2017_ICCV,
  author    = {Roberts, Mike and Dey, Debadeepta and Truong, Anh and Sinha, Sudipta and Shah, Shital and Kapoor, Ashish and Hanrahan, Pat and Joshi, Neel},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {Submodular Trajectory Optimization for Aerial 3D Scanning},
  year      = {2017},
  month     = {Oct},
  file      = {:Roberts_2017_ICCV - Submodular Trajectory Optimization for Aerial 3D Scanning.pdf:PDF},
  groups    = {Information gathering},
}

@Article{smith2018distributed,
  author    = {Smith, Andrew J and Hollinger, Geoffrey A},
  journal   = {Autonomous Robots},
  title     = {Distributed inference-based multi-robot exploration},
  year      = {2018},
  number    = {8},
  pages     = {1651--1668},
  volume    = {42},
  file      = {:auro-si-distributed__1_.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Springer},
}

@Article{8968434,
  author   = {Schmid, Lukas and Pantic, Michael and Khanna, Raghav and Ott, Lionel and Siegwart, Roland and Nieto, Juan},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {An Efficient Sampling-Based Method for Online Informative Path Planning in Unknown Environments},
  year     = {2020},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {1500-1507},
  volume   = {5},
  abstract = {The ability to plan informative paths online is essential to robot autonomy. In particular, sampling-based approaches are often used as they are capable of using arbitrary information gain formulations. However, they are prone to local minima, resulting in sub-optimal trajectories, and sometimes do not reach global coverage. In this letter, we present a new RRT*-inspired online informative path planning algorithm. Our method continuously expands a single tree of candidate trajectories and rewires nodes to maintain the tree and refine intermediate paths. This allows the algorithm to achieve global coverage and maximize the utility of a path in a global context, using a single objective function. We demonstrate the algorithm's capabilities in the applications of autonomous indoor exploration as well as accurate Truncated Signed Distance Field (TSDF)-based 3D reconstruction on-board a Micro Aerial Vehicle (MAV). We study the impact of commonly used information gain and cost formulations in these scenarios and propose a novel TSDF-based 3D reconstruction gain and cost-utility formulation. Detailed evaluation in realistic simulation environments show that our approach outperforms sampling-based state of the art methods in these tasks. Experiments on a real MAV demonstrate the ability of our method to robustly plan in real-time, exploring an indoor environment with on-board sensing and computation. We make our framework available for future research.},
  doi      = {10.1109/LRA.2020.2969191},
  file     = {:8968434 - An Efficient Sampling Based Method for Online Informative Path Planning in Unknown Environments.pdf:PDF},
  groups   = {Information gathering},
}

@Article{8276241,
  author   = {Oleynikova, Helen and Taylor, Zachary and Siegwart, Roland and Nieto, Juan},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Safe Local Exploration for Replanning in Cluttered Unknown Environments for Microaerial Vehicles},
  year     = {2018},
  issn     = {2377-3766},
  month    = {July},
  number   = {3},
  pages    = {1474-1481},
  volume   = {3},
  abstract = {In order to enable microaerial vehicles (MAVs) to assist in complex, unknown, unstructured environments, they must be able to navigate with guaranteed safety, even when faced with a cluttered environment they have no prior knowledge of. While trajectory-optimization-based local planners have been shown to perform well in these cases, prior work either does not address how to deal with local minima in the optimization problem or solves it by using an optimistic global planner. We present a conservative trajectory-optimization-based local planner, coupled with a local exploration strategy that selects intermediate goals. We perform extensive simulations to show that this system performs better than the standard approach of using an optimistic global planner and also outperforms doing a single exploration step when the local planner is stuck. The method is validated through experiments in a variety of highly cluttered environments including a dense forest. These experiments show the complete system running in real time fully onboard an MAV, mapping and replanning at 4 Hz.},
  doi      = {10.1109/LRA.2018.2800109},
  file     = {:8276241 - Safe Local Exploration for Replanning in Cluttered Unknown Environments for Microaerial Vehicles.pdf:PDF},
  groups   = {Information gathering},
}

@Article{9293348,
  author   = {Li, Juncheng and Ran, Maopeng and Xie, Lihua},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Efficient Trajectory Planning for Multiple Non-Holonomic Mobile Robots via Prioritized Trajectory Optimization},
  year     = {2021},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {405-412},
  volume   = {6},
  abstract = {In this letter, we present a novel approach to efficiently generate collision-free optimal trajectories for multiple non-holonomic mobile robots in obstacle-rich environments. Our approach first employs a graph-based multi-agent path planner to find an initial discrete solution, and then refines this solution into smooth trajectories using nonlinear optimization. We divide the robot team into small groups and propose a prioritized trajectory optimization method to improve the scalability of the algorithm. Infeasible sub-problems may arise in some scenarios because of the decoupled optimization framework. To handle this problem, a novel grouping and priority assignment strategy is developed to increase the probability of finding feasible trajectories. Compared to the coupled trajectory optimization, the proposed approach reduces the computation time considerably with a small impact on the optimality of the plans. Simulations and hardware experiments verified the effectiveness and superiority of the proposed approach.},
  doi      = {10.1109/LRA.2020.3044834},
  file     = {:9293348 - Efficient Trajectory Planning for Multiple Non Holonomic Mobile Robots Via Prioritized Trajectory Optimization.pdf:PDF},
  groups   = {Navigation},
}

@Article{Nelson2017,
  author    = {Erik Nelson and Micah Corah and Nathan Michael},
  journal   = {Autonomous Robots},
  title     = {Environment model adaptation for mobile robot exploration},
  year      = {2017},
  month     = {nov},
  number    = {2},
  pages     = {257--272},
  volume    = {42},
  doi       = {10.1007/s10514-017-9669-2},
  file      = {:Nelson2017 - Environment Model Adaptation for Mobile Robot Exploration.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{bircher2018receding,
  author    = {Bircher, Andreas and Kamel, Mina and Alexis, Kostas and Oleynikova, Helen and Siegwart, Roland},
  journal   = {Autonomous Robots},
  title     = {Receding horizon path planning for 3D exploration and surface inspection},
  year      = {2018},
  number    = {2},
  pages     = {291--306},
  volume    = {42},
  file      = {:bircher2018receding - Receding Horizon Path Planning for 3D Exploration and Surface Inspection.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Springer},
}

@InProceedings{10.1007/978-981-15-9460-1_21,
  author    = {Goel, Kshitij and Corah, Micah and Boirum, Curtis and Michael, Nathan},
  booktitle = {Field and Service Robotics},
  title     = {Fast Exploration Using Multirotors: Analysis, Planning, and Experimentation},
  year      = {2021},
  address   = {Singapore},
  editor    = {Ishigami, Genya and Yoshida, Kazuya},
  pages     = {291--305},
  publisher = {Springer Singapore},
  abstract  = {This work presents a system and approach for the rapid exploration of unknown environments using aerial robots. High-speed flight with multirotor air vehicles is challenging due to limited sensing range, use of onboard computation, and constrained dynamics. For robots operating in unknown environments, the control system must guarantee collision-free operation, and for exploration tasks, the system should also select sensing actions to maximize information gain with respect to the environment. To this end, we present a motion primitive-based, receding-horizon planning approach that maximizes information gain, accounts for platform dynamics, and ensures safe operation. Analysis of motions parallel and perpendicular to frontiers given constraints on sensing and dynamics leads to bounds on safe velocities for exploration. This analysis and the bounds obtained inform the design of the motion primitive approach. Simulation experiments in a complex 3D environment demonstrate the utility of the motion primitive actions for rapid exploration and provide a comparison to a reduced motion primitive library that is appropriate for online planning. Experimental results on a hexarotor robot with the reduced library demonstrate rapid exploration at speeds above 2.25 m/s under a varying clutter in an outdoor environment which is comparable to and exceeding the existing state-of-the-art results.},
  file      = {:EasyChair-Preprint-1454.pdf:PDF},
  groups    = {Information gathering},
  isbn      = {978-981-15-9460-1},
}

@Article{vago2017habitability,
  author    = {Vago, Jorge L and Westall, Frances and Coates, Andrew J and Jaumann, Ralf and Korablev, Oleg and Ciarletti, Val{\'e}rie and Mitrofanov, Igor and Josset, Jean-Luc and De Sanctis, Maria Cristina and Bibring, Jean-Pierre and others},
  journal   = {Astrobiology},
  title     = {Habitability on early Mars and the search for biosignatures with the ExoMars Rover},
  year      = {2017},
  number    = {6-7},
  pages     = {471--510},
  volume    = {17},
  groups    = {Space Exploration},
  publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
  url       = {https://www.liebertpub.com/doi/pdfplus/10.1089/ast.2016.1533},
}

@InProceedings{sonsalla2017field,
  author    = {Sonsalla, Roland and Cordes, Florian and Christensen, Leif and Roehr, Thomas M and Stark, Tobias and Planthaber, Steffen and Maurus, Michael and Mallwitz, Martin and Kirchner, Elsa A},
  booktitle = {Proceedings of the 14th symposium on advanced space technologies in robotics and automation (ASTRA)},
  title     = {Field testing of a cooperative multi-robot sample return mission in mars analogue environment},
  year      = {2017},
  file      = {:sonsalla2017field - Field Testing of a Cooperative Multi Robot Sample Return Mission in Mars Analogue Environment.pdf:PDF},
  groups    = {Space Exploration},
}

@Article{Corah2021,
  author        = {Micah Corah and Nathan Michael},
  title         = {Volumetric Objectives for Multi-Robot Exploration of Three-Dimensional Environments},
  year          = {2021},
  month         = mar,
  abstract      = {Volumetric objectives for exploration and perception tasks seek to capture a sense of value (or reward) for hypothetical observations at one or more camera views for robots operating in unknown environments. For example, a volumetric objective may reward robots proportionally to the expected volume of unknown space to be observed. We identify connections between existing information-theoretic and coverage objectives in terms of expected coverage, particularly that mutual information without noise is a special case of expected coverage. Likewise, we provide the first comparison, of which we are aware, between information-based approximations and coverage objectives for exploration, and we find, perhaps surprisingly, that coverage objectives can significantly outperform information-based objectives in practice. Additionally, the analysis for information and coverage objectives demonstrates that Randomized Sequential Partitions -- a method for efficient distributed sensor planning -- applies for both classes of objectives, and we provide simulation results in a variety of environments for as many as 32 robots.},
  archiveprefix = {arXiv},
  eprint        = {2103.11625},
  file          = {:Corah2021 - Volumetric Objectives for Multi Robot Exploration of Three Dimensional Environments.pdf:PDF},
  groups        = {Information gathering},
  keywords      = {cs.RO},
  primaryclass  = {cs.RO},
}

@InProceedings{Sonsalla2014,
  author = {Sonsalla, Roland and Cordes, Florian and Christensen, Leif and Planthaber, Steffen and Albiez, Jan and Scholz, Ingo and Kirchner, Frank},
  title  = {Towards a Heterogeneous Modular Robotic Team in a Logistic Chain for Extraterrestrial Exploration},
  year   = {2014},
  month  = {01},
  file   = {:Sonsalla2014 - Towards a Heterogeneous Modular Robotic Team in a Logistic Chain for Extraterrestrial Exploration.pdf:PDF},
  groups = {Space Exploration, Heterogeneous},
}

@Article{Roehr_2018,
  author       = {Roehr, Thomas M},
  journal      = {Inteligencia Artificial},
  title        = {A Constraint-based Mission Planning Approach for Reconfigurable Multi-Robot Systems},
  year         = {2018},
  month        = {Sep.},
  number       = {62},
  pages        = {25–39},
  volume       = {21},
  abstractnote = {&amp;lt;p&amp;gt;The application of reconfigurable multi-robot systems introduces additional degrees of freedom to design robotic missions compared to classical multi-robot systems. To allow for autonomous operation of such systems, planning approaches have to be investigated that cannot only cope with the combinatorial challenge arising from the increased flexibility of modular systems, but also exploit this flexibility to improve for example the safety of operation. While the problem originates from the domain of robotics it is of general nature and significantly intersects with operations research. This paper suggests a constraint-based mission planning approach, and presents a set of revised definitions for reconfigurable multi-robot systems including the representation of the planning problem using spatially and temporally qualified resource constraints. Planning is performed using a multi-stage approach and a combined use of knowledge-based reasoning, constraint-based programming and integer linear programming. The paper concludes with the illustration of the solution of a planned example mission.&amp;lt;/p&amp;gt;},
  doi          = {10.4114/intartif.vol21iss62pp25-39},
  file         = {:editor-journal-editor-216.pdf:PDF},
  groups       = {Space Exploration},
  url          = {https://www.journal.iberamia.org/index.php/intartif/article/view/216},
}

@InProceedings{govindaraj2019pro,
  author    = {Govindaraj, Shashank and Gancet, Jeremi and Urbina, Diego and Brinkmann, Wiebke and Aouf, Nabil and Lacroix, Simon and Wolski, Mateusz and Colmenero, Francisco and Walshe, Michael and Ortega, Cristina and others},
  booktitle = {Proceedings of the 15th Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA-2019)},
  title     = {PRO-ACT: Planetary Robots Deployed for Assembly and Construction of Future Lunar ISRU and Supporting Infrastructures},
  year      = {2019},
  file      = {:govindaraj2019pro - PRO ACT_ Planetary Robots Deployed for Assembly and Construction of Future Lunar ISRU and Supporting Infrastructures.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{Govindaraj2020,
  author = {Govindaraj, Shashank and Nieto, Irene and But, Alexandru and Brinkmann, Wiebke and Dettmann, Alexander and Danter, Leon and Aouf, Nabil and Sotoodeh Bahraini, Masoud and Zenati, Abdelhafid and Savino, Heitor and Stelmachowski, Jakub and Colmenero, Francisco and Heredia Aguado, Enrique and Alonso, Mercedes and Purnell, Joe and Picton, Kevin and Lopes, Luís},
  title  = {Multi-Robot Cooperation for Lunar Base Assembly And Construction},
  year   = {2020},
  month  = {10},
  file   = {:11170_i-SAIRAS2020_PRO-ACT-Consortial-Paper.pdf:PDF},
  groups = {Space Exploration},
}

@InProceedings{brinkmann2019space,
  author    = {Brinkmann, Wiebke and Cordes, Florian and Koch, Christian Ernst Siegfried and Wirkus, Malte and Dominguez, Raul and Dettmann, Alexander and V{\"o}gele, Thomas and Kirchner, Frank},
  booktitle = {proceedings of 2019 Space Tech Industry Conference, Bremen, Germany},
  title     = {Space robotic systems and artificial intelligence in the context of the European space technology roadmap},
  year      = {2019},
  file      = {:brinkmann2019space - Space Robotic Systems and Artificial Intelligence in the Context of the European Space Technology Roadmap.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{waldmann2018europa,
  author    = {Waldmann, C and Winebrenner, D and Bachmayer, R and Hanff, H and Funke, O},
  booktitle = {Lunar and Planetary Science Conference},
  title     = {EUROPA Calling--A comprehensive technical concept for exploring ocean worlds},
  year      = {2018},
  number    = {2083},
  pages     = {2162},
  groups    = {Space Exploration},
}

@Article{8633953,
  author   = {Corah, Micah and O’Meadhra, Cormac and Goel, Kshitij and Michael, Nathan},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Communication-Efficient Planning and Mapping for Multi-Robot Exploration in Large Environments},
  year     = {2019},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {1715-1721},
  volume   = {4},
  abstract = {This letter presents a framework for planning and perception for multi-robot exploration in large and unstructured three-dimensional environments. We employ a Gaussian mixture model for global mapping to model complex environment geometries while maintaining a small memory footprint which enables distributed operation with a low volume of communication. We then generate a local occupancy grid for use in planning from the Gaussian mixture model using Monte Carlo ray tracing. Then, a finite-horizon, information-based planner uses this local map and optimizes sequences of observations locally while accounting for the global distribution of information in the robot state space which we model using a library of informative views. Simulation results demonstrate that the proposed system is able to maintain efficiency and completeness in exploration while only requiring a low rate of communication.},
  doi      = {10.1109/LRA.2019.2897368},
  file     = {:8633953 - Communication Efficient Planning and Mapping for Multi Robot Exploration in Large Environments.pdf:PDF},
  groups   = {Information gathering, Multi-robot},
}

@Article{9152817,
  author   = {Ni, Jianjun and Wang, Xiaotian and Tang, Min and Cao, Weidong and Shi, Pengfei and Yang, Simon X.},
  journal  = {IEEE Access},
  title    = {An Improved Real-Time Path Planning Method Based on Dragonfly Algorithm for Heterogeneous Multi-Robot System},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {140558-140568},
  volume   = {8},
  abstract = {Heterogeneous multi-robot system is one of the most important research directions in the robotic field. Real-time path planning for heterogeneous multi-robot system under unknown 3D environment is a new challenging research and a hot spot in this field. In this paper, an improved real-time path planning method is proposed for a heterogeneous multi-robot system, which is composed of many unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). In the proposed method, the 3D environment is modelled as a neuron topology map, based on the grid method combined with the bio-inspired neural network. Then a new 3D dynamic movement model for multi-robots is established based on an improved Dragonfly Algorithm (DA). Thus, the movements of the robots are optimized according to the activities of the neurons in the bio-inspired neural network to realize the real-time path planning. Furthermore, some simulations have been carried out. The results show that the proposed method can effectively guide the heterogeneous UAV/UGV system to the target, and has better performance than traditional methods in the real-time path planning tasks.},
  doi      = {10.1109/ACCESS.2020.3012886},
  file     = {:09152817.pdf:PDF},
  groups   = {Heterogeneous},
}

@Article{fabisch2020bolero,
  author    = {Fabisch, Alexander and Langosz, Malte and Kirchner, Frank},
  journal   = {International Journal of Advanced Robotic Systems},
  title     = {BOLeRo: Behavior optimization and learning for robots},
  year      = {2020},
  number    = {3},
  pages     = {1729881420913741},
  volume    = {17},
  file      = {:fabisch2020bolero - BOLeRo_ Behavior Optimization and Learning for Robots.pdf:PDF},
  groups    = {Robotics, Platforms},
  publisher = {SAGE Publications Sage UK: London, England},
}

@PhdThesis{hidalgo2018adaptive,
  author = {Hidalgo-Carri{\'o}, Javier},
  school = {Universit{\"a}t Bremen},
  title  = {Adaptive Localization and Mapping for Planetary Rovers},
  year   = {2018},
  file   = {:hidalgo2018adaptive - Adaptive Localization and Mapping for Planetary Rovers.pdf:PDF},
  groups = {Space Exploration},
}

@InProceedings{schwendnerentern,
  author    = {Schwendner, Jakob and Hidalgo-Carrio, Javier and Dom{\i}nguez, Ra{\'u}l and Planthaber, Steffen and Yoo, Yong-Ho and Asadi, Behnam and Machowinski, Janosch and Rauch, Christian and Kirchner, Frank},
  booktitle = {Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA). ESA/Estec Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA), 13th, May},
  title     = {Entern--environment modelling and navigation for robotic space-exploration},
  year      = {2015},
  pages     = {11--13},
  file      = {:schwendnerentern - Entern Environment Modelling and Navigation for Robotic Space Exploration.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{carrio2016envire,
  author    = {Carri{\'o}, Javier Hidalgo and Arnold, Sascha and B{\"o}ckmann, Arne and Born, Anna and Dom{\'\i}nguez, Ra{\'u}l and Kirchner, F},
  booktitle = {AI for Long-term Autonomy Workshop of the Int. Conf. on Robotics and Automation (ICRA)},
  title     = {EnviRe-Environment Representation for Long-term Autonomy},
  year      = {2016},
  file      = {:carrio2016envire - EnviRe Environment Representation for Long Term Autonomy.pdf:PDF},
  groups    = {Space Exploration},
}

@InBook{Starek2016,
  author    = {Starek, Joseph A. and A{\c{c}}{\i}kme{\c{s}}e, Beh{\c{c}}et and Nesnas, Issa A. and Pavone, Marco},
  editor    = {Feron, Eric},
  pages     = {1--48},
  publisher = {Springer Berlin Heidelberg},
  title     = {Spacecraft Autonomy Challenges for Next-Generation Space Missions},
  year      = {2016},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-662-47694-9},
  abstract  = {In early 2011, NASA's Office of the Chief Technologist (OCT) released a set of technology roadmaps with the aim of fostering the development of concepts and cross-cutting technologies addressing NASA's needs for the 2011--2021 decade and beyond. In an attempt to engage the external technical community and enhance the development program in light of scarce resources, NASA reached out to the National Research Council (NRC) to review the program's objectives and prioritize its list of technologies. In January 2012, the NRC released its report entitled ``Restoring NASA's Technological Edge and Paving the Way for a New Era in Space.'' While the NRC report provides a systematic and thorough ranking of the future technology needs for NASA, it does not discuss in detail the technical aspects of the prioritized technologies (which clearly lie beyond its scope). This chapter, building upon the NRC report and an earlier assessment of NASA's needs in terms of guidance, navigation, and control technologies, aims at providing such technical details for a selected number of high-priority technologies in the autonomous systems area. Specifically, this chapter focuses on technology area TA04 ``Robotics, Tele-Robotics, and Autonomous Systems'' and discusses in some detail the technical aspects and challenges associated with three high-priority TA04 technologies: ``Relative Guidance Algorithms,'' ``Extreme Terrain Mobility,'' and ``Small Body/Microgravity Mobility.'' The result is a unified presentation of key autonomy challenges for next-generation space missions.},
  booktitle = {Advances in Control System Technology for Aerospace Applications},
  doi       = {10.1007/978-3-662-47694-9_1},
  file      = {:Starek2016 - Spacecraft Autonomy Challenges for Next Generation Space Missions.pdf:PDF},
  groups    = {Space Exploration},
  ranking   = {rank4},
  url       = {https://doi.org/10.1007/978-3-662-47694-9_1},
}

@InProceedings{Dominguez2014,
  author    = {Raúl Domínguez, Jakob Schwendner, Frank Kirchner},
  booktitle = {Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA)},
  title     = {On-Board Simulator for Autonomy Enhancement in Robotic Space Missions},
  year      = {2015},
  file      = {:Dominguez2014 - On Board Simulator for Autonomy Enhancement in Robotic Space Missions.pdf:PDF},
  groups    = {Space Exploration},
}

@Article{Corah_2018,
  author    = {Micah Corah and Nathan Michael},
  journal   = {Autonomous Robots},
  title     = {Distributed matroid-constrained submodular maximization for multi-robot exploration: theory and practice},
  year      = {2018},
  month     = {jul},
  number    = {2},
  pages     = {485--501},
  volume    = {43},
  doi       = {10.1007/s10514-018-9778-6},
  file      = {:corah2018.pdf:PDF},
  groups    = {Information gathering, Multi-robot},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Wang2019a,
  author    = {Chaoqun Wang and Wenzheng Chi and Yuxiang Sun and Max Q.-H. Meng},
  journal   = {{IEEE} Transactions on Automation Science and Engineering},
  title     = {Autonomous Robotic Exploration by Incremental Road Map Construction},
  year      = {2019},
  month     = {oct},
  number    = {4},
  pages     = {1720--1731},
  volume    = {16},
  doi       = {10.1109/tase.2019.2894748},
  file      = {:wang2019.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Pergola2016,
  author    = {Pierpaolo Pergola and Vittorio Cipolla},
  journal   = {International Journal of Intelligent Unmanned Systems},
  title     = {Mission architecture for Mars exploration based on small satellites and planetary drones},
  year      = {2016},
  month     = {jul},
  number    = {3},
  pages     = {142--162},
  volume    = {4},
  doi       = {10.1108/ijius-12-2015-0014},
  file      = {:pergola2016.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {Emerald},
}

@Article{Roehr2013,
  author    = {Thomas M. Roehr and Florian Cordes and Frank Kirchner},
  journal   = {Journal of Field Robotics},
  title     = {Reconfigurable Integrated Multirobot Exploration System ({RIMRES}): Heterogeneous Modular Reconfigurable Robots for Space Exploration},
  year      = {2013},
  month     = {aug},
  number    = {1},
  pages     = {3--34},
  volume    = {31},
  doi       = {10.1002/rob.21477},
  file      = {:roehr2013.pdf:PDF},
  groups    = {Space Exploration, Heterogeneous},
  publisher = {Wiley},
}

@Article{Zheng2011,
  author    = {Y. Zheng and L. Wang and Y. Zhu},
  journal   = {{IET} Control Theory {\&} Applications},
  title     = {Consensus of heterogeneous multi-agent systems},
  year      = {2011},
  month     = {nov},
  number    = {16},
  pages     = {1881--1888},
  volume    = {5},
  doi       = {10.1049/iet-cta.2011.0033},
  file      = {:Zheng2011 - Consensus of Heterogeneous Multi Agent Systems.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institution of Engineering and Technology ({IET})},
  ranking   = {rank2},
}

@Article{Kim2011,
  author    = {Hongkeun Kim and Hyungbo Shim and Jin Heon Seo},
  journal   = {{IEEE} Transactions on Automatic Control},
  title     = {Output Consensus of Heterogeneous Uncertain Linear Multi-Agent Systems},
  year      = {2011},
  month     = {jan},
  number    = {1},
  pages     = {200--206},
  volume    = {56},
  doi       = {10.1109/tac.2010.2088710},
  file      = {:kim2011.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank3},
}

@InProceedings{Doriya2015,
  author    = {Rajesh Doriya and Siddharth Mishra and Swati Gupta},
  booktitle = {International Conference on Computing, Communication {\&} Automation},
  title     = {A brief survey and analysis of multi-robot communication and coordination},
  year      = {2015},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/ccaa.2015.7148524},
  file      = {:doriya2015.pdf:PDF},
  groups    = {Multi-Robot Communication},
}

@InBook{Parker2016,
  author    = {Parker, Lynne E. and Rus, Daniela and Sukhatme, Gaurav S.},
  editor    = {Siciliano, Bruno and Khatib, Oussama},
  pages     = {1335--1384},
  publisher = {Springer International Publishing},
  title     = {Multiple Mobile Robot Systems},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-32552-1},
  abstract  = {Within the context of multiple mobile, and networked robot systems, this chapter explores the current state of the art. After a brief introduction, we first examine architectures for multirobot cooperation, exploring the alternative approaches that have been developed. Next, we explore communications issues and their impact on multirobot teams in Sect. 53.3, followed by a discussion of networked mobile robots in Sect. 53.4. Following this we discuss swarm robot systems in Sect. 53.5 and modular robot systems in Sect. 53.6. While swarm and modular systems typically assume large numbers of homogeneous robots, other types of multirobot systems include heterogeneous robots. We therefore next discuss heterogeneity in cooperative robot teams in Sect. 53.7. Once robot teams allow for individual heterogeneity, issues of task allocation become important; Sect. 53.8 therefore discusses common approaches to task allocation. Section 53.9 discusses the challenges of multirobot learning, and some representative approaches. We outline some of the typical application domains which serve as test beds for multirobot systems research in Sect. 53.10. Finally, we conclude in Sect. 53.11 with some summary remarks and suggestions for further reading.},
  booktitle = {Springer Handbook of Robotics},
  doi       = {10.1007/978-3-319-32552-1_53},
  file      = {:Parker2016 - Multiple Mobile Robot Systems.pdf:PDF},
  groups    = {Multi-robot},
  ranking   = {rank3},
  url       = {https://doi.org/10.1007/978-3-319-32552-1_53},
}

@Article{Rosa2015,
  author    = {Lorenzo Rosa and Marco Cognetti and Andrea Nicastro and Pol Alvarez and Giuseppe Oriolo},
  journal   = {{IFAC}-{PapersOnLine}},
  title     = {Multi-task Cooperative Control in a Heterogeneous Ground-Air Robot Team},
  year      = {2015},
  number    = {5},
  pages     = {53--58},
  volume    = {48},
  doi       = {10.1016/j.ifacol.2015.06.463},
  file      = {:Rosa2015 - Multi Task Cooperative Control in a Heterogeneous Ground Air Robot Team.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Elsevier {BV}},
}

@InProceedings{Stegagno2013,
  author    = {Paolo Stegagno and Marco Cognetti and Lorenzo Rosa and Pietro Peliti and Giuseppe Oriolo},
  booktitle = {2013 {IEEE} International Conference on Robotics and Automation},
  title     = {Relative localization and identification in a heterogeneous multi-robot system},
  year      = {2013},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra.2013.6630822},
  file      = {:Stegagno2013 - Relative Localization and Identification in a Heterogeneous Multi Robot System.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Mathew2015,
  author    = {Neil Mathew and Stephen L. Smith and Steven L. Waslander},
  journal   = {{IEEE} Transactions on Automation Science and Engineering},
  title     = {Planning Paths for Package Delivery in Heterogeneous Multirobot Teams},
  year      = {2015},
  month     = {oct},
  number    = {4},
  pages     = {1298--1308},
  volume    = {12},
  doi       = {10.1109/tase.2015.2461213},
  file      = {:2015TASE_Package_Delivery.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank2},
}

@Article{Krizmancic2020,
  author    = {Marko Krizmancic and Barbara Arbanas and Tamara Petrovic and Frano Petric and Stjepan Bogdan},
  journal   = {{IEEE} Robotics and Automation Letters},
  title     = {Cooperative Aerial-Ground Multi-Robot System for Automated Construction Tasks},
  year      = {2020},
  month     = {apr},
  number    = {2},
  pages     = {798--805},
  volume    = {5},
  doi       = {10.1109/lra.2020.2965855},
  file      = {:Krizmancic2020 - Cooperative Aerial Ground Multi Robot System for Automated Construction Tasks.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Roldan2016,
  author    = {Juan Rold{\'{a}}n and Pablo Garcia-Aunon and Mario Garz{\'{o}}n and Jorge de Le{\'{o}}n and Jaime del Cerro and Antonio Barrientos},
  journal   = {Sensors},
  title     = {Heterogeneous Multi-Robot System for Mapping Environmental Variables of Greenhouses},
  year      = {2016},
  month     = {jul},
  number    = {7},
  pages     = {1018},
  volume    = {16},
  doi       = {10.3390/s16071018},
  file      = {:sensors-16-01018.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {{MDPI} {AG}},
}

@Article{Rahimi2014,
  author    = {Reihane Rahimi and Farzaneh Abdollahi and Karo Naqshi},
  journal   = {Robotics and Autonomous Systems},
  title     = {Time-varying formation control of a collaborative heterogeneous multi agent system},
  year      = {2014},
  month     = {dec},
  number    = {12},
  pages     = {1799--1805},
  volume    = {62},
  doi       = {10.1016/j.robot.2014.07.005},
  file      = {:Rahimi2014 - Time Varying Formation Control of a Collaborative Heterogeneous Multi Agent System.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Elsevier {BV}},
}

@Article{Nguyen2020,
  author     = {Thanh Thi Nguyen and Ngoc Duy Nguyen and Saeid Nahavandi},
  journal    = {{IEEE} Transactions on Cybernetics},
  title      = {Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications},
  year       = {2020},
  month      = {sep},
  number     = {9},
  pages      = {3826--3839},
  volume     = {50},
  doi        = {10.1109/tcyb.2020.2977374},
  file       = {:Nguyen2020 - Deep Reinforcement Learning for Multiagent Systems_ a Review of Challenges, Solutions, and Applications.pdf:PDF},
  groups     = {MAS Reviews},
  publisher  = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{8460759,
  author    = {Manjanna, Sandeep and Li, Alberto Quattrini and Smith, Ryan N. and Rekleitis, Ioannis and Dudek, Gregory},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Heterogeneous Multi-Robot System for Exploration and Strategic Water Sampling},
  year      = {2018},
  month     = {May},
  pages     = {4873-4880},
  abstract  = {Physical sampling of water for off-site analysis is necessary for many applications like monitoring the quality of drinking water in reservoirs, understanding marine ecosystems, and measuring contamination levels in fresh-water systems. In this paper, the focus is on algorithms for efficient measurement and sampling using a multi-robot, data-driven, water-sampling behavior, where autonomous surface vehicles plan and execute water sampling using the chlorophyll density as a cue for plankton-rich water samples. We use two Autonomous Surface Vehicles (ASVs), one equipped with a water quality sensor and the other equipped with a water-sampling apparatus. The ASV with the sensor acts as an explorer, measuring and building a spatial map of chlorophyll density in the given region of interest. The ASV equipped with the water sampling apparatus makes decisions in real time on where to sample the water based on the suggestions made by the explorer robot. We evaluate the system in the context of measuring chlorophyll distributions. We do this both in simulation based on real geophysical data from MODIS measurements, and on real robots in a water reservoir. We demonstrate the effectiveness of the proposed approach in several ways including in terms of mean error in the interpolated data as a function of distance traveled.},
  doi       = {10.1109/ICRA.2018.8460759},
  file      = {:8460759 - Heterogeneous Multi Robot System for Exploration and Strategic Water Sampling.pdf:PDF},
  groups    = {Heterogeneous},
  issn      = {2577-087X},
}

@Article{schillinger2018simultaneous,
  author    = {Schillinger, Philipp and B{\"u}rger, Mathias and Dimarogonas, Dimos V},
  journal   = {The international journal of robotics research},
  title     = {Simultaneous task allocation and planning for temporal logic goals in heterogeneous multi-robot systems},
  year      = {2018},
  number    = {7},
  pages     = {818--838},
  volume    = {37},
  file      = {:schillinger2018simultaneous - Simultaneous Task Allocation and Planning for Temporal Logic Goals in Heterogeneous Multi Robot Systems.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Sage Publications Sage UK: London, England},
}

@Article{Verma2021,
  author    = {Janardan Kumar Verma and Virender Ranga},
  journal   = {Journal of Intelligent {\&} Robotic Systems},
  title     = {Multi-Robot Coordination Analysis, Taxonomy, Challenges and Future Scope},
  year      = {2021},
  month     = {apr},
  number    = {1},
  volume    = {102},
  doi       = {10.1007/s10846-021-01378-2},
  file      = {:Verma2021 - Multi Robot Coordination Analysis, Taxonomy, Challenges and Future Scope.pdf:PDF},
  groups    = {Multi-robot},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Mayya2021,
  author    = {Siddharth Mayya and Diego S. D{\textquotesingle}antonio and David Saldana and Vijay Kumar},
  journal   = {{IEEE} Robotics and Automation Letters},
  title     = {Resilient Task Allocation in Heterogeneous Multi-Robot Systems},
  year      = {2021},
  month     = {apr},
  number    = {2},
  pages     = {1327--1334},
  volume    = {6},
  doi       = {10.1109/lra.2021.3057559},
  file      = {:Mayya2021 - Resilient Task Allocation in Heterogeneous Multi Robot Systems.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Amigoni2017,
  author    = {Francesco Amigoni and Jacopo Banfi and Nicola Basilico},
  journal   = {{IEEE} Intelligent Systems},
  title     = {Multirobot Exploration of Communication-Restricted Environments: A Survey},
  year      = {2017},
  month     = {nov},
  number    = {6},
  pages     = {48--57},
  volume    = {32},
  doi       = {10.1109/mis.2017.4531226},
  file      = {:Amigoni2017 - Multirobot Exploration of Communication Restricted Environments_ a Survey.pdf:PDF},
  groups    = {Multi-robot, Information gathering},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Yang2020,
  author    = {Qin Yang and Ramviyas Parasuraman},
  booktitle = {2020 {IEEE} International Symposium on Safety, Security, and Rescue Robotics ({SSRR})},
  title     = {Needs-driven Heterogeneous Multi-Robot Cooperation in Rescue Missions},
  year      = {2020},
  month     = {nov},
  publisher = {{IEEE}},
  doi       = {10.1109/ssrr50563.2020.9292570},
  file      = {:Yang2020 - Needs Driven Heterogeneous Multi Robot Cooperation in Rescue Missions.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Schuster2020,
  author    = {Martin J. Schuster and Marcus G. Muller and Sebastian G. Brunner and Hannah Lehner and Peter Lehner and Ryo Sakagami and Andreas Domel and Lukas Meyer and Bernhard Vodermayer and Riccardo Giubilato and Mallikarjuna Vayugundla and Josef Reill and Florian Steidle and Ingo von Bargen and Kristin Bussmann and Rico Belder and Philipp Lutz and Wolfgang Sturzl and Michal Smisek and Moritz Maier and Samantha Stoneman and Andre Fonseca Prince and Bernhard Rebele and Maximilian Durner and Emanuel Staudinger and Siwei Zhang and Robert Pohlmann and Esther Bischoff and Christian Braun and Susanne Schroder and Enrico Dietz and Sven Frohmann and Anko Borner and Heinz-Wilhelm Hubers and Bernard Foing and Rudolph Triebel and Alin O. Albu-Schaffer and Armin Wedler},
  journal   = {{IEEE} Robotics and Automation Letters},
  title     = {The {ARCHES} Space-Analogue Demonstration Mission: Towards Heterogeneous Teams of Autonomous Robots for Collaborative Scientific Sampling in Planetary Exploration},
  year      = {2020},
  month     = {oct},
  number    = {4},
  pages     = {5315--5322},
  volume    = {5},
  doi       = {10.1109/lra.2020.3007468},
  file      = {:Schuster2020 - The ARCHES Space Analogue Demonstration Mission_ Towards Heterogeneous Teams of Autonomous Robots for Collaborative Scientific Sampling in Planetary Exploration.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{giubilato2020gpgm,
  author       = {Giubilato, Riccardo and Le Gentil, Cedric and Vayugundla, Mallikarjuna and Vidal-Calleja, Teresa and Triebel, Rudolph},
  booktitle    = {IROS Workshop on Planetary Exploration Robots: Challenges and Opportunities (PLANROBO20)},
  title        = {GPGM-SLAM: Towards a Robust SLAM System for Unstructured Planetary Environments with Gaussian Process Gradient Maps},
  year         = {2020},
  organization = {ETH Zurich, Department of Mechanical and Process Engineering},
  file         = {:giubilato2020gpgm - GPGM SLAM_ Towards a Robust SLAM System for Unstructured Planetary Environments with Gaussian Process Gradient Maps (1).pdf:PDF},
  groups       = {Space Exploration, Information gathering},
}

@Article{wedler2021german,
  author    = {Wedler, Armin and Schuster, Martin J and M{\"u}ller, Marcus G and Vodermayer, Bernhard and Meyer, Lukas and Giubilato, Riccardo and Vayugundla, Mallikarjuna and Smisek, Michal and D{\"o}mel, Andreas and Steidle, Florian and others},
  journal   = {Philosophical Transactions of the Royal Society A},
  title     = {German Aerospace Center's advanced robotic technology for future lunar scientific missions},
  year      = {2021},
  number    = {2188},
  pages     = {20190574},
  volume    = {379},
  file      = {:wedler2021german - German Aerospace Center's Advanced Robotic Technology for Future Lunar Scientific Missions.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {The Royal Society Publishing},
}

@InProceedings{dlr132829,
  author    = {Martin J. Schuster and Marcus G. M{\"u}ller and Sebastian G. Brunner and Hannah Lehner and Peter Lehner and Andreas D{\"o}mel and Mallikarjuna Vayugundla and Florian Steidle and Philipp Lutz and Ryo Sakagami and Lukas Meyer and Rico Belder and Michal Smisek and Wolfgang St{\"u}rzl and Rudolph Triebel and Armin Wedler},
  booktitle = {Workshop on Informed Scientific Sampling in Large-scale Outdoor Environments at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Towards Heterogeneous Robotic Teams for Collaborative Scientific Sampling in Lunar and Planetary Environments},
  year      = {2019},
  abstract  = {Teams of mobile robots will play a crucial role in future scientific missions to explore the surfaces of extraterrestrial bodies such as Moon or Mars. Taking scientific samples is an expensive task when operating far away in challenging, previously unknown environments, especially in hard-to-reach areas, such as craters, pits, and subterranean caves. In contrast to current single-robot missions, future robotic teams will increase efficiency via increased autonomy and parallelization, improve robustness via functional redundancy, as well as benefit from complementary capabilities of the individual robots. In this work, we present our heterogeneous robotic team consisting of flying and driving robots that we plan to deploy on a scientific sampling demonstration mission in a Moon-analogue environment on Mt. Etna, Sicily, Italy in 2020 as part of the ARCHES project. We first describe the robots' individual capabilities and then highlight their tasks in the joint mission scenario. In addition, we present preliminary experiments on important subtasks: the analysis of volcanic rocks via spectral images, collaborative multi-robot 6D SLAM in a Moon-analogue environment as well as with a rover and a drone in a Mars-like scenario, and demonstrations of autonomous robotic sample-return missions therein.},
  file      = {:dlr132829 - Towards Heterogeneous Robotic Teams for Collaborative Scientific Sampling in Lunar and Planetary Environments.pdf:PDF},
  groups    = {Space Exploration},
  keywords  = {scientific sampling; multi-robot; planetary exploration;},
  url       = {https://elib.dlr.de/132829/},
}

@InProceedings{dlr122782,
  author    = {Armin Wedler and Martina Wilde and Andreas D{\"o}mel and Marcus Gerhard M{\"u}ller and Josef Reill and Martin Schuster and Wolfgang St{\"u}rzl and Rudolph Triebel and Heinrich Gmeiner and Bernhard Vodermayer and Kristin Bussmann, and Mallikarjuna Vayugundla and Sebastian Brunner and Hannah Lehner and Peter Lehner and Anko B{\"o}rner and Rainer Krenn and Armin Dammann and Uwe-Carsten Fiebig and Emanuel Staudinger and Frank Wenzh{\"o}fer and Sascha Fl{\"o}gel and Stefan Sommer and Tamim Asfour and Michael Flad and S{\"o}ren Hohmann and Martin Brandauer and Alin Olimpiu Albu-Sch{\"a}ffer},
  booktitle = {69th International Astronautical Congress (IAC)},
  title     = {From single autonomous robots toward cooperative robotic interactions for future planetary exploration missions},
  year      = {2018},
  month     = {October},
  publisher = {International Astronautical Federation (IAF)},
  series    = {Preceedings of the 69th International Astronautical Congress (IAC)},
  abstract  = {Mobile robotics will play a key role in future space, ocean and deep sea exploration activities. Besides the actual development of the robotic systems, the different ways of commanding those systems, using technologies varying from teleoperation with the human in the loop, through shared autonomy towards highly autonomous systems, will be the main challenges of these missions. This paper describes the robotic activities of the DLR institutions within the Helmholtz projects ROBEX and ARCHES, dealing with robots for autonomous space and ocean exploration applications. Furthermore, it describes the challenges, the overlap and the synergies of those domains, the different approaches of operating robots from far distances in extreme environments and gives an outlook on future mission possibilities.},
  file      = {:dlr122782 - From Single Autonomous Robots toward Cooperative Robotic Interactions for Future Planetary Exploration Missions.pdf:PDF},
  groups    = {Space Exploration},
  journal   = {Proceedings of the International Astronautical Congress, IAC},
  keywords  = {robotics, autonomy, exploration, shared autonomy, teleoperation},
  url       = {https://elib.dlr.de/122782/},
}

@PhdThesis{dlr132830,
  author   = {Martin J. Schuster},
  school   = {University of Bremen},
  title    = {Collaborative Localization and Mapping for Autonomous Planetary Exploration: Distributed Stereo Vision-Based 6D SLAM in GNSS-Denied Environments},
  year     = {2019},
  month    = {August},
  abstract = {Mobile robots are a crucial element of present and future scientific missions to explore the surfaces of foreign celestial bodies such as Moon and Mars. The deployment of teams of robots allows to improve efficiency and robustness in such challenging environments. As long communication round-trip times to Earth render the teleoperation of robotic systems inefficient to impossible, on-board autonomy is a key to success. The robots operate in Global Navigation Satellite System (GNSS)-denied environments and thus have to rely on space-suitable on-board sensors such as stereo camera systems. They need to be able to localize themselves online, to model their surroundings, as well as to share information about the environment and their position therein. These capabilities constitute the basis for the local autonomy of each system as well as for any coordinated joint action within the team, such as collaborative autonomous exploration. In this thesis, we present a novel approach for stereo vision-based on-board and online Simultaneous Localization and Mapping (SLAM) for multi-robot teams given the challenges imposed by planetary exploration missions. We combine distributed local and decentralized global estimation methods to get the best of both worlds: A local reference filter on each robot provides real-time local state estimates required for robot control and fast reactive behaviors. We designed a novel graph topology to incorporate these state estimates into an online incremental graph optimization to compute global pose and map estimates that serve as input to higher-level autonomy functions. In order to model the 3D geometry of the environment, we generate dense 3D point cloud and probabilistic voxel-grid maps from noisy stereo data. We distribute the computational load and reduce the required communication bandwidth between robots by locally aggregating high-bandwidth vision data into partial maps that are then exchanged between robots and composed into global models of the environment. We developed methods for intra- and inter-robot map matching to recognize previously visited locations in semi- and unstructured environments based on their estimated local geometry, which is mostly invariant to light conditions as well as different sensors and viewpoints in heterogeneous multi-robot teams. A decoupling of observable and unobservable states in the local filter allows us to introduce a novel optimization: Enforcing all submaps to be gravity-aligned, we can reduce the dimensionality of the map matching from 6D to 4D. In addition to map matches, the robots use visual fiducial markers to detect each other. In this context, we present a novel method for modeling the errors of the loop closure transformations that are estimated from these detections. We demonstrate the robustness of our methods by integrating them on a total of five different ground-based and aerial mobile robots that were deployed in a total of 31 real-world experiments for quantitative evaluations in semi- and unstructured indoor and outdoor settings. In addition, we validated our SLAM framework through several different demonstrations at four public events in Moon and Mars-like environments. These include, among others, autonomous multi-robot exploration tests at a Moon-analogue site on top of the volcano Mt. Etna, Italy, as well as the collaborative mapping of a Mars-like environment with a heterogeneous robotic team of flying and driving robots in more than 35 public demonstration runs.},
  file     = {:dlr132830 - Collaborative Localization and Mapping for Autonomous Planetary Exploration_ Distributed Stereo Vision Based 6D SLAM in GNSS Denied Environments.pdf:PDF},
  groups   = {Space Exploration},
  keywords = {SLAM, collaborative SLAM, distributed SLAM, localization, mapping, stereo-vision, multi-robot, planetary exploration, autonomous robots},
  url      = {https://elib.dlr.de/132830/},
}

@Article{Chang2020,
  author        = {Yuan Chang and Chao Yan and Xingyu Liu and Xiangke Wang and Han Zhou and Xiaojia Xiang and Dengqing Tang},
  title         = {Time-Efficient Mars Exploration of Simultaneous Coverage and Charging with Multiple Drones},
  year          = {2020},
  month         = nov,
  abstract      = {This paper presents a time-efficient scheme for Mars exploration by the cooperation of multiple drones and a rover. To maximize effective coverage of the Mars surface in the long run, a comprehensive framework has been developed with joint consideration for limited energy, sensor model, communication range and safety radius, which we call TIME-SC2 (TIme-efficient Mars Exploration of Simultaneous Coverage and Charging). First, we propose a multi-drone coverage control algorithm by leveraging emerging deep reinforcement learning and design a novel information map to represent dynamic system states. Second, we propose a near-optimal charging scheduling algorithm to navigate each drone to an individual charging slot, and we have proven that there always exists feasible solutions. The attractiveness of this framework not only resides on its ability to maximize exploration efficiency, but also on its high autonomy that has greatly reduced the non-exploring time. Extensive simulations have been conducted to demonstrate the remarkable performance of TIME-SC2 in terms of time-efficiency, adaptivity and flexibility.},
  archiveprefix = {arXiv},
  eprint        = {2011.07759},
  file          = {:Chang2020 - Time Efficient Mars Exploration of Simultaneous Coverage and Charging with Multiple Drones.pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.AI},
  primaryclass  = {cs.RO},
}

@InProceedings{Klodt2015,
  author    = {Lukas Klodt and Saman Khodaverdian and Volker Willert},
  booktitle = {2015 {IEEE} Conference on Control Applications ({CCA})},
  title     = {Motion control for {UAV}-{UGV} cooperation with visibility constraint},
  year      = {2015},
  month     = {sep},
  publisher = {{IEEE}},
  doi       = {10.1109/cca.2015.7320804},
  file      = {:Klodt2015 - Motion Control for UAV UGV Cooperation with Visibility Constraint.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Li2016,
  author    = {Jianqiang Li and Genqiang Deng and Chengwen Luo and Qiuzhen Lin and Qiao Yan and Zhong Ming},
  journal   = {{IEEE} Transactions on Vehicular Technology},
  title     = {A Hybrid Path Planning Method in Unmanned Air/Ground Vehicle ({UAV}/{UGV}) Cooperative Systems},
  year      = {2016},
  month     = {dec},
  number    = {12},
  pages     = {9585--9596},
  volume    = {65},
  doi       = {10.1109/tvt.2016.2623666},
  file      = {:li2016.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank2},
}

@Article{Liu2018,
  author    = {Chi Harold Liu and Zheyu Chen and Jian Tang and Jie Xu and Chengzhe Piao},
  journal   = {{IEEE} Journal on Selected Areas in Communications},
  title     = {Energy-Efficient {UAV} Control for Effective and Fair Communication Coverage: A Deep Reinforcement Learning Approach},
  year      = {2018},
  month     = {sep},
  number    = {9},
  pages     = {2059--2070},
  volume    = {36},
  doi       = {10.1109/jsac.2018.2864373},
  groups    = {Navigation},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank3},
}

@Article{ROPERO2019260,
  author   = {Fernando Ropero and Pablo Muñoz and María D. R-Moreno},
  journal  = {Engineering Applications of Artificial Intelligence},
  title    = {TERRA: A path planning algorithm for cooperative UGV–UAV exploration},
  year     = {2019},
  issn     = {0952-1976},
  pages    = {260-272},
  volume   = {78},
  abstract = {In this paper, we consider the scenario of exploring a planetary surface with a system formed by an Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV). The goal is to reach a set of target points minimizing the travelling distance. Some expected key problems in planetary explorations are the UGVs functionality constraints to reach some target points as a single robot system and the UAVs energy constraints to reach all the target points on its own. We present an approach based on the coordination of a hybrid UGV–UAV system, in which both robots work together for reaching all the target points. Our strategy proposes the UGV as a moving charging station to solve the UAV energy constraint problem, and the UAV as the robotic system in charge of reaching the target points to solve the UGV functionality constraints. To overcome this problem, we formulate a strategy merging combinatorial classic techniques and modern evolutionary approaches aiming to optimize the travelling distance. Our solution has been tested in several simulation runs with different target points distributions. The results demonstrate that our approach is able to generate a coordinated plan for optimizing the hybrid UGV–UAV system in the exploration scenario.},
  doi      = {https://doi.org/10.1016/j.engappai.2018.11.008},
  file     = {:ROPERO2019260 - TERRA_ a Path Planning Algorithm for Cooperative UGV–UAV Exploration.pdf:PDF},
  groups   = {Space Exploration, Heterogeneous},
  keywords = {Exploration, Cooperation, Routing, Heterogeneous robots},
  url      = {https://www.sciencedirect.com/science/article/pii/S095219761830246X},
}

@Article{Stamenkovic2019,
  author    = {V. Stamenkovi{\'{c}} and L. W. Beegle and K. Zacny and D. D. Arumugam and P. Baglioni and N. Barba and J. Baross and M. S. Bell and R. Bhartia and J. G. Blank and P. J. Boston and D. Breuer and W. Brinckerhoff and M. S. Burgin and I. Cooper and V. Cormarkovic and A. Davila and R. M. Davis and C. Edwards and G. Etiope and W. W. Fischer and D. P. Glavin and R. E. Grimm and F. Inagaki and J. L. Kirschvink and A. Kobayashi and T. Komarek and M. Malaska and J. Michalski and B. M{\'{e}}nez and M. Mischna and D. Moser and J. Mustard and T. C. Onstott and V. J. Orphan and M. R. Osburn and J. Plaut and A.-C. Plesa and N. Putzig and K. L. Rogers and L. Rothschild and M. Russell and H. Sapers and B. Sherwood Lollar and T. Spohn and J. D. Tarnas and M. Tuite and D. Viola and L. M. Ward and B. Wilcox and R. Woolley},
  journal   = {Nature Astronomy},
  title     = {The next frontier for planetary and human exploration},
  year      = {2019},
  month     = {jan},
  number    = {2},
  pages     = {116--120},
  volume    = {3},
  doi       = {10.1038/s41550-018-0676-9},
  file      = {:10.1038@s41550-018-0676-9.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Agha2021,
  author        = {Ali Agha and Kyohei Otsu and Benjamin Morrell and David D. Fan and Rohan Thakker and Angel Santamaria-Navarro and Sung-Kyun Kim and Amanda Bouman and Xianmei Lei and Jeffrey Edlund and Muhammad Fadhil Ginting and Kamak Ebadi and Matthew Anderson and Torkom Pailevanian and Edward Terry and Michael Wolf and Andrea Tagliabue and Tiago Stegun Vaquero and Matteo Palieri and Scott Tepsuporn and Yun Chang and Arash Kalantari and Fernando Chavez and Brett Lopez and Nobuhiro Funabiki and Gregory Miles and Thomas Touma and Alessandro Buscicchio and Jesus Tordesillas and Nikhilesh Alatur and Jeremy Nash and William Walsh and Sunggoo Jung and Hanseob Lee and Christoforos Kanellakis and John Mayo and Scott Harper and Marcel Kaufmann and Anushri Dixit and Gustavo Correa and Carlyn Lee and Jay Gao and Gene Merewether and Jairo Maldonado-Contreras and Gautam Salhotra and Maira Saboia Da Silva and Benjamin Ramtoula and Yuki Kubo and Seyed Fakoorian and Alexander Hatteland and Taeyeon Kim and Tara Bartlett and Alex Stephens and Leon Kim and Chuck Bergh and Eric Heiden and Thomas Lew and Abhishek Cauligi and Tristan Heywood and Andrew Kramer and Henry A. Leopold and Chris Choi and Shreyansh Daftry and Olivier Toupet and Inhwan Wee and Abhishek Thakur and Micah Feras and Giovanni Beltrame and George Nikolakopoulos and David Shim and Luca Carlone and Joel Burdick},
  title         = {NeBula: Quest for Robotic Autonomy in Challenging Environments; TEAM CoSTAR at the DARPA Subterranean Challenge},
  year          = {2021},
  month         = mar,
  abstract      = {This paper presents and discusses algorithms, hardware, and software architecture developed by the TEAM CoSTAR (Collaborative SubTerranean Autonomous Robots), competing in the DARPA Subterranean Challenge. Specifically, it presents the techniques utilized within the Tunnel (2019) and Urban (2020) competitions, where CoSTAR achieved 2nd and 1st place, respectively. We also discuss CoSTAR's demonstrations in Martian-analog surface and subsurface (lava tubes) exploration. The paper introduces our autonomy solution, referred to as NeBula (Networked Belief-aware Perceptual Autonomy). NeBula is an uncertainty-aware framework that aims at enabling resilient and modular autonomy solutions by performing reasoning and decision making in the belief space (space of probability distributions over the robot and world states). We discuss various components of the NeBula framework, including: (i) geometric and semantic environment mapping; (ii) a multi-modal positioning system; (iii) traversability analysis and local planning; (iv) global motion planning and exploration behavior; (i) risk-aware mission planning; (vi) networking and decentralized reasoning; and (vii) learning-enabled adaptation. We discuss the performance of NeBula on several robot types (e.g. wheeled, legged, flying), in various environments. We discuss the specific results and lessons learned from fielding this solution in the challenging courses of the DARPA Subterranean Challenge competition.},
  archiveprefix = {arXiv},
  eprint        = {2103.11470},
  file          = {:Agha2021 - NeBula_ Quest for Robotic Autonomy in Challenging Environments\; TEAM CoSTAR at the DARPA Subterranean Challenge.pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.AI},
  primaryclass  = {cs.RO},
}

@InProceedings{thakker2021autonomous,
  author       = {Thakker, Rohan and Alatur, Nikhilesh and Paton, Michael and Otsu, Kyohei and Toupet, Olivier and Agha-mohammadi, Ali-akbar},
  booktitle    = {Experimental Robotics: The 17th International Symposium},
  title        = {Autonomous Off-road Navigation over Extreme Terrains with Perceptually-challenging Conditions},
  year         = {2021},
  organization = {Springer Nature},
  pages        = {161},
  file         = {:thakker2021autonomous - Autonomous off Road Navigation Over Extreme Terrains with Perceptually Challenging Conditions.pdf:PDF},
  groups       = {Space Exploration},
}

@Article{Abcouwer2020,
  author        = {Neil Abcouwer and Shreyansh Daftry and Siddarth Venkatraman and Tyler del Sesto and Olivier Toupet and Ravi Lanka and Jialin Song and Yisong Yue and Masahiro Ono},
  title         = {Machine Learning Based Path Planning for Improved Rover Navigation (Pre-Print Version)},
  year          = {2020},
  month         = nov,
  abstract      = {Enhanced AutoNav (ENav), the baseline surface navigation software for NASA's Perseverance rover, sorts a list of candidate paths for the rover to traverse, then uses the Approximate Clearance Evaluation (ACE) algorithm to evaluate whether the most highly ranked paths are safe. ACE is crucial for maintaining the safety of the rover, but is computationally expensive. If the most promising candidates in the list of paths are all found to be infeasible, ENav must continue to search the list and run time-consuming ACE evaluations until a feasible path is found. In this paper, we present two heuristics that, given a terrain heightmap around the rover, produce cost estimates that more effectively rank the candidate paths before ACE evaluation. The first heuristic uses Sobel operators and convolution to incorporate the cost of traversing high-gradient terrain. The second heuristic uses a machine learning (ML) model to predict areas that will be deemed untraversable by ACE. We used physics simulations to collect training data for the ML model and to run Monte Carlo trials to quantify navigation performance across a variety of terrains with various slopes and rock distributions. Compared to ENav's baseline performance, integrating the heuristics can lead to a significant reduction in ACE evaluations and average computation time per planning cycle, increase path efficiency, and maintain or improve the rate of successful traverses. This strategy of targeting specific bottlenecks with ML while maintaining the original ACE safety checks provides an example of how ML can be infused into planetary science missions and other safety-critical software.},
  archiveprefix = {arXiv},
  eprint        = {2011.06022},
  file          = {:Abcouwer2020 - Machine Learning Based Path Planning for Improved Rover Navigation (Pre Print Version).pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.LG, I.2.6; I.2.9; I.2.8},
  primaryclass  = {cs.RO},
}

@InProceedings{Toupet2020,
  author    = {Olivier Toupet and Tyler Del Sesto and Masahiro Ono and Steven Myint and Joshua vander Hook and Michael McHenry},
  booktitle = {2020 {IEEE} Aerospace Conference},
  title     = {A {ROS}-based Simulator for Testing the Enhanced Autonomous Navigation of the Mars 2020 Rover},
  year      = {2020},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/aero47225.2020.9172345},
  groups    = {Space Exploration},
}

@PhdThesis{matheron2020,
  author = {Guillaume Matheron},
  title  = {Integrating Motion Planning into Reinforcement Learning to solve hard exploration problems},
  year   = {2020},
  file   = {:matheron2020 - Integrating Motion Planning into Reinforcement Learning to Solve Hard Exploration Problems.pdf:PDF},
  groups = {Information gathering},
}

@Article{Lamarre2020,
  author    = {Lamarre, Olivier and Asghar, Ahmad Bila and Kelly, Jonathan},
  title     = {Impact of Traversability Uncertainty on Global Navigation Planning in Planetary Environments},
  year      = {2020},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  doi       = {10.3929/ETHZ-B-000450119},
  file      = {:Lamarre2020 - Impact of Traversability Uncertainty on Global Navigation Planning in Planetary Environments.pdf:PDF},
  groups    = {Space Exploration},
  keywords  = {Space robotics},
  language  = {en},
  publisher = {ETH Zurich},
}

@Article{Seewald2020,
  author    = {Seewald, Adam},
  title     = {Beyond Traditional Energy Planning: the Weight of Computations in Planetary Exploration},
  year      = {2020},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  doi       = {10.3929/ETHZ-B-000450120},
  file      = {:Seewald2020 - Beyond Traditional Energy Planning_ the Weight of Computations in Planetary Exploration.pdf:PDF},
  groups    = {Space Exploration},
  keywords  = {Space robotics},
  language  = {en},
  publisher = {ETH Zurich},
}

@Article{Kodgule2019,
  author        = {Suhit Kodgule and Alberto Candela and David Wettergreen},
  title         = {Non-myopic Planetary Exploration Combining In Situ and Remote Measurements},
  year          = {2019},
  month         = apr,
  abstract      = {Remote sensing can provide crucial information for planetary rovers. However, they must validate these orbital observations with in situ measurements. Typically, this involves validating hyperspectral data using a spectrometer on-board the field robot. In order to achieve this, the robot must visit sampling locations that jointly improve a model of the environment while satisfying sampling constraints. However, current planners follow sub-optimal greedy strategies that are not scalable to larger regions. We demonstrate how the problem can be effectively defined in an MDP framework and propose a planning algorithm based on Monte Carlo Tree Search, which is devoid of the common drawbacks of existing planners and also provides superior performance. We evaluate our approach using hyperspectral imagery of a well-studied geologic site in Cuprite, Nevada.},
  archiveprefix = {arXiv},
  eprint        = {1904.12255},
  file          = {:Kodgule2019 - Non Myopic Planetary Exploration Combining in Situ and Remote Measurements.pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.AI, cs.LG},
  primaryclass  = {cs.RO},
}

@InProceedings{Candela2020,
  author    = {Alberto Candela and Suhit Kodgule and Kevin Edelson and Srinivasan Vijayarangan and David R. Thompson and Eldar Noe Dobrea and David Wettergreen},
  booktitle = {2020 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  title     = {Planetary Rover Exploration Combining Remote and In Situ Measurements for Active Spectroscopic Mapping},
  year      = {2020},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra40945.2020.9196973},
  file      = {:Candela2020 - Planetary Rover Exploration Combining Remote and in Situ Measurements for Active Spectroscopic Mapping.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{Ono2020,
  author    = {Masahiro Ono and Brandon Rothrock and Kyohei Otsu and Shoya Higa and Yumi Iwashita and Annie Didier and Tanvir Islam and Christopher Laporte and Vivian Sun and Kathryn Stack and Jacek Sawoniewicz and Shreyansh Daftry and Virisha Timmaraju and Sami Sahnoune and Chris A. Mattmann and Olivier Lamarre and Sourish Ghosh and Dicong Qiu and Shunichiro Nomura and Hiya Roy and Hemanth Sarabu and Gabrielle Hedrick and Larkin Folsom and Sean Suehr and Hyoshin Park},
  booktitle = {2020 {IEEE} Aerospace Conference},
  title     = {{MAARS}: Machine learning-based Analytics for Automated Rover Systems},
  year      = {2020},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/aero47225.2020.9172271},
  file      = {:09172271.pdf:PDF},
  groups    = {Space Exploration},
}

@Article{Chen2020,
  author        = {Fanfei Chen and John D. Martin and Yewei Huang and Jinkun Wang and Brendan Englot},
  title         = {Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs},
  year          = {2020},
  month         = jul,
  abstract      = {We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward-simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.},
  archiveprefix = {arXiv},
  eprint        = {2007.12640},
  file          = {:Chen2020 - Autonomous Exploration under Uncertainty Via Deep Reinforcement Learning on Graphs.pdf:PDF},
  groups        = {Information gathering},
  keywords      = {cs.RO, cs.LG},
  primaryclass  = {cs.RO},
}

@InProceedings{10.5555/3237383.3237907,
  author    = {Hong, Zhang-Wei and Su, Shih-Yang and Shann, Tzu-Yun and Chang, Yi-Hsiang and Lee, Chun-Yi},
  booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {A Deep Policy Inference Q-Network for Multi-Agent Systems},
  year      = {2018},
  address   = {Richland, SC},
  pages     = {1388–1396},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '18},
  abstract  = {We present DPIQN, a deep policy inference Q-network that targets multi-agent systems composed of controllable agents, collaborators, and opponents that interact with each other. We focus on one challenging issue in such systems---modeling agents with varying strategies---and propose to employ "policy features'' learned from raw observations (e.g., raw images) of collaborators and opponents by inferring their policies. DPIQN incorporates the learned policy features as a hidden vector into its own deep Q-network (DQN), such that it is able to predict better Q values for the controllable agents than the state-of-the-art deep reinforcement learning models. We further propose an enhanced version of DPIQN, called deep recurrent policy inference Q-network (DRPIQN), for handling partial observability. Both DPIQN and DRPIQN are trained by an adaptive training procedure, which adjusts the network's attention to learn the policy features and its own Q-values at different phases of the training process. We present a comprehensive analysis of DPIQN and DRPIQN, and highlight their effectiveness and generalizability in various multi-agent settings. Our models are evaluated in a classic soccer game involving both competitive and collaborative scenarios. Experimental results performed on 1 vs. 1 and 2 vs. 2 games show that DPIQN and DRPIQN demonstrate superior performance to the baseline DQN and deep recurrent Q-network (DRQN) models. We also explore scenarios in which collaborators or opponents dynamically change their policies, and show that DPIQN and DRPIQN do lead to better overall performance in terms of stability and mean scores.},
  comment   = {DPIQN and DRPIQN},
  file      = {:10.5555_3237383.3237907 - A Deep Policy Inference Q Network for Multi Agent Systems.pdf:PDF},
  groups    = {Multi-agent RL},
  keywords  = {deep reinforcement learning, multi-agent learning, opponent modeling},
  location  = {Stockholm, Sweden},
  numpages  = {9},
}

@Article{Foerster2016b,
  author        = {Jakob N. Foerster and Yannis M. Assael and Nando de Freitas and Shimon Whiteson},
  title         = {Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks},
  year          = {2016},
  month         = feb,
  abstract      = {We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.},
  archiveprefix = {arXiv},
  eprint        = {1602.02672},
  file          = {:Foerster2016 - Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q Networks.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
}

@InProceedings{Kurek2016,
  author    = {Mateusz Kurek and Wojciech Jaskowski},
  booktitle = {2016 {IEEE} Conference on Computational Intelligence and Games ({CIG})},
  title     = {Heterogeneous team deep q-learning in low-dimensional multi-agent environments},
  year      = {2016},
  month     = {sep},
  publisher = {{IEEE}},
  doi       = {10.1109/cig.2016.7860413},
  file      = {:Kurek2016 - Heterogeneous Team Deep Q Learning in Low Dimensional Multi Agent Environments.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Xiao2020,
  author    = {Jian Xiao and Gang Wang and Ying Zhang and Lei Cheng},
  journal   = {{IEEE} Access},
  title     = {A Distributed Multi-Agent Dynamic Area Coverage Algorithm Based on Reinforcement Learning},
  year      = {2020},
  pages     = {33511--33521},
  volume    = {8},
  doi       = {10.1109/access.2020.2967225},
  groups    = {Coverage Path Planning},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Li2020,
  author        = {Sheng Li and Jayesh K. Gupta and Peter Morales and Ross Allen and Mykel J. Kochenderfer},
  title         = {Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning},
  year          = {2020},
  month         = jun,
  abstract      = {Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph based formalization allows reasoning about the joint action based on the structure of interactions. However, they often require domain expertise in their design. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for inferring the dynamic coordination graph structure which is then used by a graph neural network based module to learn to implicitly reason about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via standard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes. We demonstrate that DICG solves the relative overgeneralization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challenging StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.},
  archiveprefix = {arXiv},
  eprint        = {2006.11438},
  file          = {:Li2020 - Deep Implicit Coordination Graphs for Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.MA},
  primaryclass  = {cs.LG},
}

@InProceedings{Jiang2020Graph,
  author    = {Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},
  booktitle = {International Conference on Learning Representations},
  title     = {Graph Convolutional Reinforcement Learning},
  year      = {2020},
  file      = {:Jiang2020Graph - Graph Convolutional Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Graphs},
  url       = {https://openreview.net/forum?id=HkxdQkSYDB},
}

@Article{Zheng2020,
  author        = {Han Zheng and Pengfei Wei and Jing Jiang and Guodong Long and Qinghua Lu and Chengqi Zhang},
  title         = {Cooperative Heterogeneous Deep Reinforcement Learning},
  year          = {2020},
  month         = nov,
  abstract      = {Numerous deep reinforcement learning agents have been proposed, and each of them has its strengths and flaws. In this work, we present a Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a policy by integrating the advantages of heterogeneous agents. Specifically, we propose a cooperative learning framework that classifies heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efficient, to guide the learning of local agents so that local agents can benefit from sample-efficient agents and simultaneously maintain their advantages, e.g., stability. Global agents also benefit from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines.},
  archiveprefix = {arXiv},
  eprint        = {2011.00791},
  file          = {:Zheng2020 - Cooperative Heterogeneous Deep Reinforcement Learning.pdf:PDF},
  groups        = {H-MADRL},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@Article{Meneghetti2020a,
  author        = {Douglas De Rizzo Meneghetti and Reinaldo Augusto da Costa Bianchi},
  title         = {Specializing Inter-Agent Communication in Heterogeneous Multi-Agent Reinforcement Learning using Agent Class Information},
  year          = {2020},
  month         = dec,
  abstract      = {Inspired by recent advances in agent communication with graph neural networks, this work proposes the representation of multi-agent communication capabilities as a directed labeled heterogeneous agent graph, in which node labels denote agent classes and edge labels, the communication type between two classes of agents. We also introduce a neural network architecture that specializes communication in fully cooperative heterogeneous multi-agent tasks by learning individual transformations to the exchanged messages between each pair of agent classes. By also employing encoding and action selection modules with parameter sharing for environments with heterogeneous agents, we demonstrate comparable or superior performance in environments where a larger number of agent classes operates.},
  archiveprefix = {arXiv},
  eprint        = {2012.07617},
  file          = {:Meneghetti2020a - Specializing Inter Agent Communication in Heterogeneous Multi Agent Reinforcement Learning Using Agent Class Information.pdf:PDF},
  groups        = {H-MADRL, Graphs, Communication},
  keywords      = {cs.AI, cs.LG, cs.MA},
  primaryclass  = {cs.AI},
}

@Article{Zhou2020,
  author        = {Ming Zhou and Jun Luo and Julian Villella and Yaodong Yang and David Rusu and Jiayu Miao and Weinan Zhang and Montgomery Alban and Iman Fadakar and Zheng Chen and Aurora Chongxi Huang and Ying Wen and Kimia Hassanzadeh and Daniel Graves and Dong Chen and Zhengbang Zhu and Nhat Nguyen and Mohamed Elsayed and Kun Shao and Sanjeevan Ahilan and Baokuan Zhang and Jiannan Wu and Zhengang Fu and Kasra Rezaee and Peyman Yadmellat and Mohsen Rohani and Nicolas Perez Nieves and Yihan Ni and Seyedershad Banijamali and Alexander Cowen Rivers and Zheng Tian and Daniel Palenicek and Haitham bou Ammar and Hongbo Zhang and Wulong Liu and Jianye Hao and Jun Wang},
  title         = {SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving},
  year          = {2020},
  month         = oct,
  abstract      = {Multi-agent interaction is a fundamental aspect of autonomous driving in the real world. Despite more than a decade of research and development, the problem of how to competently interact with diverse road users in diverse scenarios remains largely unsolved. Learning methods have much to offer towards solving this problem. But they require a realistic multi-agent simulator that generates diverse and competent driving interactions. To meet this need, we develop a dedicated simulation platform called SMARTS (Scalable Multi-Agent RL Training School). SMARTS supports the training, accumulation, and use of diverse behavior models of road users. These are in turn used to create increasingly more realistic and diverse interactions that enable deeper and broader research on multi-agent interaction. In this paper, we describe the design goals of SMARTS, explain its basic architecture and its key features, and illustrate its use through concrete multi-agent experiments on interactive scenarios. We open-source the SMARTS platform and the associated benchmark tasks and evaluation metrics to encourage and empower research on multi-agent learning for autonomous driving. Our code is available at https://github.com/huawei-noah/SMARTS.},
  archiveprefix = {arXiv},
  eprint        = {2010.09776},
  file          = {:Zhou2020 - SMARTS_ Scalable Multi Agent Reinforcement Learning Training School for Autonomous Driving.pdf:PDF},
  groups        = {Navigation, Multi-robot},
  keywords      = {cs.MA, cs.AI, cs.GT, cs.LG, cs.SY, eess.SY},
  primaryclass  = {cs.MA},
}

@Article{Mitchell2019,
  author        = {Rupert Mitchell and Jenny Fletcher and Jacopo Panerati and Amanda Prorok},
  title         = {Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous Multi-Lane Driving},
  year          = {2019},
  month         = nov,
  abstract      = {Autonomous driving promises to transform road transport. Multi-vehicle and multi-lane scenarios, however, present unique challenges due to constrained navigation and unpredictable vehicle interactions. Learning-based methods---such as deep reinforcement learning---are emerging as a promising approach to automatically design intelligent driving policies that can cope with these challenges. Yet, the process of safely learning multi-vehicle driving behaviours is hard: while collisions---and their near-avoidance---are essential to the learning process, directly executing immature policies on autonomous vehicles raises considerable safety concerns. In this article, we present a safe and efficient framework that enables the learning of driving policies for autonomous vehicles operating in a shared workspace, where the absence of collisions cannot be guaranteed. Key to our learning procedure is a sim2real approach that uses real-world online policy adaptation in a mixed-reality setup, where other vehicles and static obstacles exist in the virtual domain. This allows us to perform safe learning by simulating (and learning from) collisions between the learning agent(s) and other objects in virtual reality. Our results demonstrate that, after only a few runs in mixed-reality, collisions are significantly reduced.},
  archiveprefix = {arXiv},
  eprint        = {1911.11699},
  file          = {:Mitchell2019 - Multi Vehicle Mixed Reality Reinforcement Learning for Autonomous Multi Lane Driving.pdf:PDF},
  groups        = {Navigation},
  keywords      = {cs.RO, cs.AI, cs.LG, cs.MA, I.2.6; I.2.9},
  primaryclass  = {cs.RO},
}

@Article{Omidshafiei2019,
  author    = {Shayegan Omidshafiei and Dong-Ki Kim and Miao Liu and Gerald Tesauro and Matthew Riemer and Christopher Amato and Murray Campbell and Jonathan P. How},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Learning to Teach in Cooperative Multiagent Reinforcement Learning},
  year      = {2019},
  month     = {jul},
  pages     = {6128--6136},
  volume    = {33},
  doi       = {10.1609/aaai.v33i01.33016128},
  file      = {:4570-Article Text-7609-1-10-20190707.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  ranking   = {rank3},
}

@Article{Foerster_Farquhar_Afouras_Nardelli_Whiteson_2018,
  author       = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Counterfactual Multi-Agent Policy Gradients},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state. &lt;/p&gt;},
  comment      = {COMA},
  file         = {:Foerster_Farquhar_Afouras_Nardelli_Whiteson_2018 - Counterfactual Multi Agent Policy Gradients.pdf:PDF},
  groups       = {Multi-agent RL, Credit Assignment},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11794},
}

@InProceedings{10.5555/3306127.3332052,
  author    = {Samvelyan, Mikayel and Rashid, Tabish and Schroeder de Witt, Christian and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
  booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {The StarCraft Multi-Agent Challenge},
  year      = {2019},
  address   = {Richland, SC},
  pages     = {2186–2188},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '19},
  abstract  = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.},
  file      = {:10.5555_3306127.3332052 - The StarCraft Multi Agent Challenge (2).pdf:PDF},
  groups    = {Multi-agent RL},
  isbn      = {9781450363099},
  keywords  = {multi-agent learning, starcraft, reinforcement learning},
  location  = {Montreal QC, Canada},
  numpages  = {3},
  ranking   = {rank2},
}

@Article{ShalevShwartz2016a,
  author        = {Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
  title         = {Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},
  year          = {2016},
  month         = oct,
  abstract      = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.},
  archiveprefix = {arXiv},
  eprint        = {1610.03295},
  file          = {:- Safe, Multi Agent, Reinforcement Learning for Autonomous Driving.pdf:PDF},
  groups        = {Navigation},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
  ranking       = {rank2},
}

@Article{Wang2020d,
  author        = {Rose E. Wang and Michael Everett and Jonathan P. How},
  journal       = {Reinforcement Learning for Real Life (RL4RealLife) Workshop in the 36th International Conference on Machine Learning, Long Beach, California, USA, 2019},
  title         = {R-MADDPG for Partially Observable Environments and Limited Communication},
  year          = {2020},
  month         = feb,
  abstract      = {There are several real-world tasks that would benefit from applying multiagent reinforcement learning (MARL) algorithms, including the coordination among self-driving cars. The real world has challenging conditions for multiagent learning systems, such as its partial observable and nonstationary nature. Moreover, if agents must share a limited resource (e.g. network bandwidth) they must all learn how to coordinate resource use. This paper introduces a deep recurrent multiagent actor-critic framework (R-MADDPG) for handling multiagent coordination under partial observable set-tings and limited communication. We investigate recurrency effects on performance and communication use of a team of agents. We demonstrate that the resulting framework learns time dependencies for sharing missing observations, handling resource limitations, and developing different communication patterns among agents.},
  archiveprefix = {arXiv},
  eprint        = {2002.06684},
  file          = {:Wang2020a - R MADDPG for Partially Observable Environments and Limited Communication.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.MA, cs.AI},
  primaryclass  = {cs.MA},
}

@InProceedings{NEURIPS2018_6a8018b3,
  author     = {Jiang, Jiechuan and Lu, Zongqing},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Learning Attentional Communication for Multi-Agent Cooperation},
  year       = {2018},
  editor     = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {31},
  comment    = {ATOC},
  file       = {:NEURIPS2018_6a8018b3 - Learning Attentional Communication for Multi Agent Cooperation (1).pdf:PDF},
  groups     = {Multi-agent RL, Communication},
  ranking    = {rank1},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper/2018/file/6a8018b3a00b69c008601b8becae392b-Paper.pdf},
}

@Article{Stone2000,
  author    = {Peter Stone and Manuela Veloso},
  journal   = {Autonomous Robots},
  title     = {Multiagent Systems: A Survey from a Machine Learning Perspective},
  year      = {2000},
  number    = {3},
  pages     = {345--383},
  volume    = {8},
  comment   = {Survey MAS organizing them along two axes: degree of heterogeneity and degree of communication (homogeneous
non-communicating agents; heterogeneous non-communicating agents; and heterogeneous communicating
agents)},
  doi       = {10.1023/a:1008942012299},
  file      = {:Stone2000 - Multiagent Systems_ a Survey from a Machine Learning Perspective.pdf:PDF},
  groups    = {MAS Reviews},
  publisher = {Springer Science and Business Media {LLC}},
  ranking   = {rank4},
}

@Article{Wakilpoor2020,
  author        = {Ceyer Wakilpoor and Patrick J. Martin and Carrie Rebhuhn and Amanda Vu},
  title         = {Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment Mapping},
  year          = {2020},
  month         = oct,
  abstract      = {Reinforcement learning in heterogeneous multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in homogeneous settings and simple benchmarks. In this work, we present an actor-critic algorithm that allows a team of heterogeneous agents to learn decentralized control policies for covering an unknown environment. This task is of interest to national security and emergency response organizations that would like to enhance situational awareness in hazardous areas by deploying teams of unmanned aerial vehicles. To solve this multi-agent coverage path planning problem in unknown environments, we augment a multi-agent actor-critic architecture with a new state encoding structure and triplet learning loss to support heterogeneous agent learning. We developed a simulation environment that includes real-world environmental factors such as turbulence, delayed communication, and agent loss, to train teams of agents as well as probe their robustness and flexibility to such disturbances.},
  archiveprefix = {arXiv},
  eprint        = {2010.02663},
  file          = {:Wakilpoor2020 - Heterogeneous Multi Agent Reinforcement Learning for Unknown Environment Mapping.pdf:PDF},
  groups        = {H-MADRL},
  keywords      = {cs.MA, cs.AI},
  primaryclass  = {cs.MA},
  readstatus    = {skimmed},
}

@InProceedings{10.5555/3237383.3237451,
  author    = {Palmer, Gregory and Tuyls, Karl and Bloembergen, Daan and Savani, Rahul},
  booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Lenient Multi-Agent Deep Reinforcement Learning},
  year      = {2018},
  address   = {Richland, SC},
  pages     = {443–451},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '18},
  abstract  = {Much of the success of single agent deep reinforcement learning (DRL) in recent years can be attributed to the use of experience replay memories (ERM), which allow Deep Q-Networks (DQNs) to be trained efficiently through sampling stored state transitions. However, care is required when using ERMs for multi-agent deep reinforcement learning (MA-DRL), as stored transitions can become outdated when agents update their policies in parallel citefoerster2017stabilising. In this work we apply leniency citepanait2006lenient to MA-DRL. Lenient agents map state-action pairs to decaying temperature values that control the amount of leniency applied towards negative policy updates that are sampled from the ERM. This introduces optimism in the value-function update, and has been shown to facilitate cooperation in tabular fully-cooperative multi-agent reinforcement learning problems. We evaluate our Lenient-DQN (LDQN) empirically against the related Hysteretic-DQN (HDQN) algorithm citeomidshafiei2017deep as well as a modified version we call scheduled -HDQN, that uses average reward learning near terminal states. Evaluations take place in extended variations of the Coordinated Multi-Agent Object Transportation Problem (CMOTP) citebucsoniu2010multi. We find that LDQN agents are more likely to converge to the optimal policy in a stochastic reward CMOTP compared to standard and scheduled-HDQN agents.},
  file      = {:10.5555_3237383.3237451 - Lenient Multi Agent Deep Reinforcement Learning (1).pdf:PDF},
  groups    = {Multi-agent RL},
  keywords  = {leniency, multi-agent deep reinforcement learning},
  location  = {Stockholm, Sweden},
  numpages  = {9},
  ranking   = {rank1},
}

@InProceedings{Raileanu2018,
  author    = {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Modeling Others using Oneself in Multi-Agent Reinforcement Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {4257--4266},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.},
  file      = {:Raileanu2018 - Modeling Others Using Oneself in Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf},
  ranking   = {rank1},
  url       = {http://proceedings.mlr.press/v80/raileanu18a.html},
}

@Article{Heinrich2016,
  author        = {Johannes Heinrich and David Silver},
  title         = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},
  year          = {2016},
  month         = mar,
  abstract      = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.},
  archiveprefix = {arXiv},
  comment       = {NFSP},
  eprint        = {1603.01121},
  file          = {:Heinrich2016 - Deep Reinforcement Learning from Self Play in Imperfect Information Games.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.GT},
  primaryclass  = {cs.LG},
  ranking       = {rank1},
}

@InProceedings{Rabinowitz2018,
  author    = {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Machine Theory of Mind},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {4218--4227},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {–} a ToMnet {–} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.},
  file      = {:Rabinowitz2018 - Machine Theory of Mind.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v80/rabinowitz18a/rabinowitz18a.pdf},
  ranking   = {rank2},
  url       = {http://proceedings.mlr.press/v80/rabinowitz18a.html},
}

@Book{oliehoek2016concise,
  author    = {Oliehoek, Frans A and Amato, Christopher},
  publisher = {Springer},
  title     = {A concise introduction to decentralized POMDPs},
  year      = {2016},
  file      = {:oliehoek2016concise - A Concise Introduction to Decentralized POMDPs.pdf:PDF},
  groups    = {MAS Reviews},
}

@InProceedings{9196684,
  author     = {Xiao, Yuchen and Hoffman, Joshua and Xia, Tian and Amato, Christopher},
  booktitle  = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title      = {Learning Multi-Robot Decentralized Macro-Action-Based Policies via a Centralized Q-Net},
  year       = {2020},
  month      = {May},
  pages      = {10695-10701},
  abstract   = {In many real-world multi-robot tasks, high-quality solutions often require a team of robots to perform asynchronous actions under decentralized control. Decentralized multi-agent reinforcement learning methods have difficulty learning decentralized policies because of the environment appearing to be non-stationary due to other agents also learning at the same time. In this paper, we address this challenge by proposing a macro-action-based decentralized multi-agent double deep recurrent Q-net (MacDec-MADDRQN) which trains each decentralized Q-net using a centralized Q-net for action selection. A generalized version of MacDec-MADDRQN with two separate training environments, called Parallel-MacDec-MADDRQN, is also presented to leverage either centralized or decentralized exploration. The advantages and the practical nature of our methods are demonstrated by achieving near-centralized results in simulation and having real robots accomplish a warehouse tool delivery task in an efficient way.},
  doi        = {10.1109/ICRA40945.2020.9196684},
  file       = {:9196684 - Learning Multi Robot Decentralized Macro Action Based Policies Via a Centralized Q Net.pdf:PDF},
  groups     = {Multi-robot},
  issn       = {2577-087X},
  readstatus = {read},
}

@InProceedings{Xiao2020a,
  author    = {Xiao, Yuchen and Hoffman, Joshua and Amato, Christopher},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Macro-Action-Based Deep Multi-Agent Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {1146--1161},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {In real-world multi-robot systems, performing high-quality, collaborative behaviors requires robots to asynchronously reason about high-level action selection at varying time durations. Macro-Action Decentralized Partially Observable Markov Decision Processes (MacDec-POMDPs) provide a general framework for asynchronous decision making under uncertainty in fully cooperative multi-agent tasks. However, multi-agent deep reinforcement learning methods have only been developed for (synchronous) primitive-action problems. This paper proposes two Deep Q-Network (DQN) based methods for learning decentralized and centralized macro-action-value functions with novel macro-action trajectory replay buffers introduced for each case. Evaluations on benchmark problems and a larger domain demonstrate the advantage of learning with macro-actions over primitive-actions and the scalability of our approaches.},
  file      = {:Xiao2020a - Macro Action Based Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v100/xiao20a/xiao20a.pdf},
  priority  = {prio1},
  url       = {http://proceedings.mlr.press/v100/xiao20a.html},
}

@Article{Yu2021a,
  author        = {Chao Yu and Akash Velu and Eugene Vinitsky and Yu Wang and Alexandre Bayen and Yi Wu},
  title         = {The Surprising Effectiveness of MAPPO in Cooperative, Multi-Agent Games},
  year          = {2021},
  month         = mar,
  abstract      = {Proximal Policy Optimization (PPO) is a popular on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent problems. In this work, we investigate Multi-Agent PPO (MAPPO), a multi-agent PPO variant which adopts a centralized value function. Using a 1-GPU desktop, we show that MAPPO achieves performance comparable to the state-of-the-art in three popular multi-agent testbeds: the Particle World environments, Starcraft II Micromanagement Tasks, and the Hanabi Challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. In the majority of environments, we find that compared to off-policy baselines, MAPPO achieves better or comparable sample complexity as well as substantially faster running time. Finally, we present 5 factors most influential to MAPPO's practical performance with ablation studies.},
  archiveprefix = {arXiv},
  eprint        = {2103.01955},
  file          = {:Yu2021a - The Surprising Effectiveness of MAPPO in Cooperative, Multi Agent Games.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.MA},
  primaryclass  = {cs.LG},
}

@Article{Wang2020e,
  author        = {Tonghan Wang and Tarun Gupta and Anuj Mahajan and Bei Peng and Shimon Whiteson and Chongjie Zhang},
  title         = {RODE: Learning Roles to Decompose Multi-Agent Tasks},
  year          = {2020},
  month         = oct,
  abstract      = {Role-based learning holds the promise of achieving scalable multi-agent learning by decomposing complex tasks using roles. However, it is largely unclear how to efficiently discover such a set of roles. To solve this problem, we propose to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi-level learning hierarchy -- the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. We further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, our method (1) outperforms the current state-of-the-art MARL algorithms on 10 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and (2) achieves rapid transfer to new environments with three times the number of agents. Demonstrative videos are available at https://sites.google.com/view/rode-marl .},
  archiveprefix = {arXiv},
  eprint        = {2010.01523},
  file          = {:Wang2020b - RODE_ Learning Roles to Decompose Multi Agent Tasks.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank1},
}

@Article{Liu2021,
  author        = {Bo Liu and Qiang Liu and Peter Stone and Animesh Garg and Yuke Zhu and Animashree Anandkumar},
  title         = {Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team Composition},
  year          = {2021},
  month         = may,
  abstract      = {In real-world multiagent systems, agents with different capabilities may join or leave without altering the team's overarching goals. Coordinating teams with such dynamic composition is challenging: the optimal team strategy varies with the composition. We propose COPA, a coach-player framework to tackle this problem. We assume the coach has a global view of the environment and coordinates the players, who only have partial views, by distributing individual strategies. Specifically, we 1) adopt the attention mechanism for both the coach and the players; 2) propose a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with the players. We validate our methods on a resource collection task, a rescue game, and the StarCraft micromanagement tasks. We demonstrate zero-shot generalization to new team compositions. Our method achieves comparable or better performance than the setting where all players have a full view of the environment. Moreover, we see that the performance remains high even when the coach communicates as little as 13% of the time using the adaptive communication strategy.},
  archiveprefix = {arXiv},
  eprint        = {2105.08692},
  file          = {:Liu2021 - Coach Player Multi Agent Reinforcement Learning for Dynamic Team Composition.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Lazaridou2020,
  author        = {Angeliki Lazaridou and Marco Baroni},
  title         = {Emergent Multi-Agent Communication in the Deep Learning Era},
  year          = {2020},
  month         = jun,
  abstract      = {The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they also can develop a shared language to interact. From a scientific perspective, understanding the conditions under which language evolves in communities of deep agents and its emergent features can shed light on human language evolution. From an applied perspective, endowing deep networks with the ability to solve problems interactively by communicating with each other and with us should make them more flexible and useful in everyday life. This article surveys representative recent language emergence studies from both of these two angles.},
  archiveprefix = {arXiv},
  eprint        = {2006.02419},
  file          = {:Lazaridou2020 - Emergent Multi Agent Communication in the Deep Learning Era.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.CL, cs.AI},
  primaryclass  = {cs.CL},
  readstatus    = {read},
}

@InProceedings{pmlr-v100-chen20a,
  author    = {Chen, Dian and Zhou, Brady and Koltun, Vladlen and Kr\"ahenb\"uhl, Philipp},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Learning by Cheating},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {66--75},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art.},
  file      = {:pmlr-v100-chen20a - Learning by Cheating.pdf:PDF},
  groups    = {Navigation},
  pdf       = {http://proceedings.mlr.press/v100/chen20a/chen20a.pdf},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v100/chen20a.html},
}

@InProceedings{pmlr-v70-haarnoja17a,
  author    = {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Reinforcement Learning with Deep Energy-Based Policies},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {06--11 Aug},
  pages     = {1352--1361},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  file      = {:pmlr-v70-haarnoja17a - Reinforcement Learning with Deep Energy Based Policies.pdf:PDF},
  groups    = {Value based},
  pdf       = {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v70/haarnoja17a.html},
}

@Article{OpenAI2019,
  author        = {OpenAI and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique P. d. O. Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
  title         = {Dota 2 with Large Scale Deep Reinforcement Learning},
  year          = {2019},
  month         = dec,
  abstract      = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arXiv},
  eprint        = {1912.06680},
  file          = {:OpenAI2019 - Dota 2 with Large Scale Deep Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank3},
  readstatus    = {skimmed},
}

@Article{Singh2018,
  author        = {Amanpreet Singh and Tushar Jain and Sainbayar Sukhbaatar},
  title         = {Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks},
  year          = {2018},
  month         = dec,
  abstract      = {Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability.},
  archiveprefix = {arXiv},
  comment       = {IC3Net},
  eprint        = {1812.09755},
  file          = {:Singh2018 - Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  ranking       = {rank1},
}

@InProceedings{Kottur2017,
  author    = {Satwik Kottur and Jos{\'{e}} Moura and Stefan Lee and Dhruv Batra},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  title     = {Natural Language Does Not Emerge `Naturally' in Multi-Agent Dialog},
  year      = {2017},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/d17-1321},
  file      = {:Kottur2017 - Natural Language Does Not Emerge `Naturally' in Multi Agent Dialog.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank1},
}

@InProceedings{10.5555/3294996.3295030,
  author    = {Hoshen, Yedid},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {VAIN: Attentional Multi-Agent Predictive Modeling},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {2698–2708},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'17},
  abstract  = {Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.},
  file      = {:10.5555_3294996.3295030 - VAIN_ Attentional Multi Agent Predictive Modeling.pdf:PDF},
  groups    = {Communication},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {11},
  ranking   = {rank1},
}

@InProceedings{pmlr-v87-das18a,
  author    = {Das, Abhishek and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle = {Proceedings of The 2nd Conference on Robot Learning},
  title     = {Neural Modular Control for Embodied Question Answering},
  year      = {2018},
  editor    = {Aude Billard and Anca Dragan and Jan Peters and Jun Morimoto},
  month     = {29--31 Oct},
  pages     = {53--62},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {87},
  abstract  = {We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. ‘exit room’, ‘find kitchen’, ‘find refrigerator’, etc.). We use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies. On the challenging EQA [1] benchmark in House3D [2], requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.},
  file      = {:pmlr-v87-das18a - Neural Modular Control for Embodied Question Answering.pdf:PDF},
  groups    = {Hierarchical RL, Language-Augmented RL},
  pdf       = {http://proceedings.mlr.press/v87/das18a/das18a.pdf},
  priority  = {prio1},
  ranking   = {rank2},
  url       = {http://proceedings.mlr.press/v87/das18a.html},
}

@InProceedings{pmlr-v80-zhang18n,
  author    = {Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Basar, Tamer},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {5872--5881},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {We consider the fully decentralized multi-agent reinforcement learning (MARL) problem, where the agents are connected via a time-varying and possibly sparse communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. To maximize the globally averaged return over the network, we propose two fully decentralized actor-critic algorithms, which are applicable to large-scale MARL problems in an online fashion. Convergence guarantees are provided when the value functions are approximated within the class of linear functions. Our work appears to be the first theoretical study of fully decentralized MARL algorithms for networked agents that use function approximation.},
  file      = {:pmlr-v80-zhang18n - Fully Decentralized Multi Agent Reinforcement Learning with Networked Agents.pdf:PDF},
  groups    = {Multi-agent RL, Communication},
  pdf       = {http://proceedings.mlr.press/v80/zhang18n/zhang18n.pdf},
  priority  = {prio2},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v80/zhang18n.html},
}

@InProceedings{8619581,
  author    = {Zhang, Kaiqing and Yang, Zhuoran and Basar, Tamer},
  booktitle = {2018 IEEE Conference on Decision and Control (CDC)},
  title     = {Networked Multi-Agent Reinforcement Learning in Continuous Spaces},
  year      = {2018},
  month     = {Dec},
  pages     = {2771-2776},
  abstract  = {Many real-world tasks on practical control systems involve the learning and decision-making of multiple agents, under limited communications and observations. In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where multiple agents perform reinforcement learning in a common environment, and are able to exchange information via a possibly time-varying communication network. In particular, we focus on a collaborative MARL setting where each agent has individual reward functions, and the objective of all the agents is to maximize the network-wide averaged long-term return. To this end, we propose a fully decentralized actor-critic algorithm that only relies on neighbor-to-neighbor communications among agents. To promote the use of the algorithm on practical control systems, we focus on the setting with continuous state and action spaces, and adopt the newly proposed expected policy gradient to reduce the variance of the gradient estimate. We provide convergence guarantees for the algorithm when linear function approximation is employed, and corroborate our theoretical results via simulations.},
  doi       = {10.1109/CDC.2018.8619581},
  file      = {:zhang2018.pdf:PDF},
  groups    = {Multi-agent RL, Communication},
  issn      = {2576-2370},
}

@Article{Zhang2019,
  author        = {Kaiqing Zhang and Zhuoran Yang and Tamer Başar},
  title         = {Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},
  year          = {2019},
  month         = nov,
  abstract      = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
  archiveprefix = {arXiv},
  eprint        = {1911.10635},
  file          = {:Zhang2019 - Multi Agent Reinforcement Learning_ a Selective Overview of Theories and Algorithms.pdf:PDF},
  groups        = {MAS Reviews},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank3},
}

@InProceedings{pittir39099,
  author    = {Akshat Agarwal and Sumit Kumar and Katia Sycara and Michael Lewis},
  booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS'2020)},
  title     = {Learning Transferable Cooperative Behavior in Multi-Agent Team},
  year      = {2020},
  address   = {Aukland, NZ},
  publisher = {IFMAS},
  abstract  = {While multi-agent interactions can be naturally modeled as a graph, the environment has traditionally been considered as a black box. To better utilize the inherent structure of our environment, we propose to create a shared agent-entity graph, where agents and environmental entities form vertices, and edges exist between the vertices which can communicate with each other, allowing agents to selectively attend to different parts of the environment, while also introducing invariance to the number of agents or entities present in the system as well as permutation invariance. We present stateof- the-art results on coverage, formation and line control tasks for multi-agent teams in a fully decentralized execution framework.},
  file      = {:pittir39099 - Learning Transferable Cooperative Behavior in Multi Agent Team (1).pdf:PDF},
  groups    = {Communication, Graphs},
  journal   = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS'2020)},
  ranking   = {rank1},
  url       = {http://d-scholarship.pitt.edu/39099/},
}

@InProceedings{8929168,
  author    = {Luo, Tianze and Subagdja, Budhitama and Wang, Di and Tan, Ah-Hwee},
  booktitle = {2019 IEEE International Conference on Agents (ICA)},
  title     = {Multi-Agent Collaborative Exploration through Graph-based Deep Reinforcement Learning},
  year      = {2019},
  month     = {Oct},
  pages     = {2-7},
  abstract  = {Autonomous exploration by a single or multiple agents in an unknown environment leads to various applications in automation, such as cleaning, search and rescue, etc. Traditional methods normally take frontier locations and segmented regions of the environment into account to efficiently allocate target locations to different agents to visit. They may employ ad hoc solutions to allocate the task to the agents, but the allocation may not be efficient. In the literature, few studies focused on enhancing the traditional methods by applying machine learning models for agent performance improvement. In this paper, we propose a graph-based deep reinforcement learning approach to effectively perform multi-agent exploration. Specifically, we first design a hierarchical map segmentation method to transform the environment exploration problem to the graph domain, wherein each node of the graph corresponds to a segmented region in the environment and each edge indicates the distance between two nodes. Subsequently, based on the graph structure, we apply a Graph Convolutional Network (GCN) to allocate the exploration target to each agent. Our experiments show that our proposed model significantly improves the efficiency of map explorations across varying sizes of collaborative agents over the traditional methods.},
  doi       = {10.1109/AGENTS.2019.8929168},
  file      = {:8929168 - Multi Agent Collaborative Exploration through Graph Based Deep Reinforcement Learning.pdf:PDF},
  groups    = {Graphs},
}

@Article{Meneghetti2020,
  author        = {Douglas De Rizzo Meneghetti and Reinaldo Augusto da Costa Bianchi},
  title         = {Towards Heterogeneous Multi-Agent Reinforcement Learning with Graph Neural Networks},
  year          = {2020},
  month         = sep,
  abstract      = {This work proposes a neural network architecture that learns policies for multiple agent classes in a heterogeneous multi-agent reinforcement setting. The proposed network uses directed labeled graph representations for states, encodes feature vectors of different sizes for different entity classes, uses relational graph convolution layers to model different communication channels between entity types and learns distinct policies for different agent classes, sharing parameters wherever possible. Results have shown that specializing the communication channels between entity classes is a promising step to achieve higher performance in environments composed of heterogeneous entities.},
  archiveprefix = {arXiv},
  doi           = {10.5753/eniac.2020.12161},
  eprint        = {2009.13161},
  file          = {:Meneghetti2020 - Towards Heterogeneous Multi Agent Reinforcement Learning with Graph Neural Networks.pdf:PDF},
  groups        = {H-MADRL, Graphs, Communication},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
  readstatus    = {skimmed},
}

@Article{Ryu2020,
  author    = {Heechang Ryu and Hayong Shin and Jinkyoo Park},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Multi-Agent Actor-Critic with Hierarchical Graph Attention Network},
  year      = {2020},
  month     = {apr},
  number    = {05},
  pages     = {7236--7243},
  volume    = {34},
  doi       = {10.1609/aaai.v34i05.6214},
  file      = {:6214-Article Text-9439-1-10-20200516.pdf:PDF},
  groups    = {Graphs},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@Article{pesce2020improving,
  author    = {Pesce, Emanuele and Montana, Giovanni},
  journal   = {Machine Learning},
  title     = {Improving coordination in small-scale multi-agent deep reinforcement learning through memory-driven communication},
  year      = {2020},
  pages     = {1--21},
  file      = {:pesce2020improving - Improving Coordination in Small Scale Multi Agent Deep Reinforcement Learning through Memory Driven Communication.pdf:PDF},
  groups    = {Communication},
  publisher = {Springer},
}

@InProceedings{10.5555/3294771.3294976,
  author    = {Havrylov, Serhii and Titov, Ivan},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {Emergence of Language with Multi-Agent Games: Learning to Communicate with Sequences of Symbols},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {2146–2156},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'17},
  abstract  = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator (Jang et al., 2017)) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
  file      = {:10.5555_3294771.3294976 - Emergence of Language with Multi Agent Games_ Learning to Communicate with Sequences of Symbols.pdf:PDF},
  groups    = {Communication},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {11},
  ranking   = {rank1},
}

@InProceedings{10.5555/3398761.3398892,
  author    = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Capacity, Bandwidth, and Compositionality in Emergent Language Learning},
  year      = {2020},
  address   = {Richland, SC},
  pages     = {1125–1133},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '20},
  abstract  = {Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
  file      = {:10.5555_3398761.3398892 - Capacity, Bandwidth, and Compositionality in Emergent Language Learning.pdf:PDF},
  groups    = {Communication},
  isbn      = {9781450375184},
  keywords  = {multi-agent communication, compositionality, emergent languages},
  location  = {Auckland, New Zealand},
  numpages  = {9},
}

@InProceedings{harding-graesser-etal-2019-emergent,
  author    = {Harding Graesser, Laura and Cho, Kyunghyun and Kiela, Douwe},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Emergent Linguistic Phenomena in Multi-Agent Communication Games},
  year      = {2019},
  address   = {Hong Kong, China},
  month     = nov,
  pages     = {3700--3710},
  publisher = {Association for Computational Linguistics},
  abstract  = {We describe a multi-agent communication framework for examining high-level linguistic phenomena at the community-level. We demonstrate that complex linguistic behavior observed in natural language can be reproduced in this simple setting: i) the outcome of contact between communities is a function of inter- and intra-group connectivity; ii) linguistic contact either converges to the majority protocol, or in balanced cases leads to novel creole languages of lower complexity; and iii) a linguistic continuum emerges where neighboring languages are more mutually intelligible than farther removed languages. We conclude that at least some of the intricate properties of language evolution need not depend on complex evolved linguistic capabilities, but can emerge from simple social exchanges between perceptually-enabled agents playing communication games.},
  doi       = {10.18653/v1/D19-1384},
  file      = {:harding-graesser-etal-2019-emergent - Emergent Linguistic Phenomena in Multi Agent Communication Games.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank1},
  url       = {https://www.aclweb.org/anthology/D19-1384},
}

@InProceedings{cao2018emergent,
  author    = {Kris Cao and Angeliki Lazaridou and Marc Lanctot and Joel Z Leibo and Karl Tuyls and Stephen Clark},
  booktitle = {International Conference on Learning Representations},
  title     = {Emergent Communication through Negotiation},
  year      = {2018},
  file      = {:cao2018emergent - Emergent Communication through Negotiation (1).pdf:PDF},
  groups    = {Communication},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=Hk6WhagRW},
}

@InProceedings{evtimova2018emergent,
  author    = {Katrina Evtimova and Andrew Drozdov and Douwe Kiela and Kyunghyun Cho},
  booktitle = {International Conference on Learning Representations},
  title     = {Emergent Communication in a Multi-Modal, Multi-Step Referential Game},
  year      = {2018},
  file      = {:evtimova2018emergent - Emergent Communication in a Multi Modal, Multi Step Referential Game.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=rJGZq6g0-},
}

@InProceedings{lazaridou2018emergence,
  author    = {Angeliki Lazaridou and Karl Moritz Hermann and Karl Tuyls and Stephen Clark},
  booktitle = {International Conference on Learning Representations},
  title     = {Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input},
  year      = {2018},
  file      = {:lazaridou2018emergence - Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=HJGv1Z-AW},
}

@InProceedings{kharitonov2020entropy,
  author       = {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
  booktitle    = {International Conference on Machine Learning},
  title        = {Entropy minimization in emergent languages},
  year         = {2020},
  organization = {PMLR},
  pages        = {5220--5230},
  file         = {:kharitonov2020entropy - Entropy Minimization in Emergent Languages.pdf:PDF},
  groups       = {Communication},
}

@InProceedings{10.5555/3306127.3331757,
  author    = {Lowe, Ryan and Foerster, Jakob and Boureau, Y-Lan and Pineau, Joelle and Dauphin, Yann},
  booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {On the Pitfalls of Measuring Emergent Communication},
  year      = {2019},
  address   = {Richland, SC},
  pages     = {693–701},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '19},
  abstract  = {How do we know if communication is emerging in a multi-agent system? The vast majority of recent papers on emergent communication show that adding a communication channel leads to an increase in reward or task success. This is a useful indicator, but provides only a coarse measure of the agent's learned communication abilities. As we move towards more complex environments, it becomes imperative to have a set of finer tools that allow qualitative and quantitative insights into the emergence of communication. This may be especially useful to allow humans to monitor agents' behaviour, whether for fault detection, assessing performance, or even building trust. In this paper, we examine a few intuitive existing metrics for measuring communication, and show that they can be misleading. Specifically, by training deep reinforcement learning agents to play simple matrix games augmented with a communication channel, we find a scenario where agents appear to communicate (their messages provide information about their subsequent action), and yet the messages do not impact the environment or other agent in any way. We explain this phenomenon using ablation studies and by visualizing the representations of the learned policies. We also survey some commonly used metrics for measuring emergent communication, and provide recommendations as to when these metrics should be used.},
  file      = {:10.5555_3306127.3331757 - On the Pitfalls of Measuring Emergent Communication.pdf:PDF},
  groups    = {Communication},
  isbn      = {9781450363099},
  keywords  = {deep learning, learning agent capabilities, multi-agent learning},
  location  = {Montreal QC, Canada},
  numpages  = {9},
  ranking   = {rank1},
}

@InProceedings{chaabouni-etal-2020-compositionality,
  author    = {Chaabouni, Rahma and Kharitonov, Eugene and Bouchacourt, Diane and Dupoux, Emmanuel and Baroni, Marco},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title     = {Compositionality and Generalization In Emergent Languages},
  year      = {2020},
  address   = {Online},
  month     = jul,
  pages     = {4427--4442},
  publisher = {Association for Computational Linguistics},
  abstract  = {Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.},
  doi       = {10.18653/v1/2020.acl-main.407},
  file      = {:chaabouni-etal-2020-compositionality - Compositionality and Generalization in Emergent Languages.pdf:PDF},
  groups    = {Communication},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.407},
}

@Article{Crandall2018,
  author    = {Jacob W. Crandall and Mayada Oudah and Tennom and Fatimah Ishowo-Oloko and Sherief Abdallah and Jean-Fran{\c{c}}ois Bonnefon and Manuel Cebrian and Azim Shariff and Michael A. Goodrich and Iyad Rahwan},
  journal   = {Nature Communications},
  title     = {Cooperating with machines},
  year      = {2018},
  month     = {jan},
  number    = {1},
  volume    = {9},
  doi       = {10.1038/s41467-017-02597-8},
  file      = {:Crandall2018 - Cooperating with Machines.pdf:PDF},
  groups    = {Communication},
  publisher = {Springer Science and Business Media {LLC}},
  ranking   = {rank2},
}

@InProceedings{Lowe2020On,
  author    = {Ryan Lowe and Abhinav Gupta and Jakob Foerster and Douwe Kiela and Joelle Pineau},
  booktitle = {International Conference on Learning Representations},
  title     = {On the interaction between supervision and self-play in emergent communication},
  year      = {2020},
  file      = {:Lowe2020On - On the Interaction between Supervision and Self Play in Emergent Communication.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=rJxGLlBtwH},
}

@InProceedings{NEURIPS2019_14cfdb59,
  author    = {Zhang, Sai Qian and Zhang, Qi and Lin, Jieyu},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  comment   = {communication integrated in QMIX, with tricks to make it more efficient},
  file      = {:NEURIPS2019_14cfdb59 - Efficient Communication in Multi Agent Reinforcement Learning Via Variance Based Control.pdf:PDF},
  groups    = {Multi-agent RL, Communication},
  priority  = {prio1},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper/2019/file/14cfdb59b5bda1fc245aadae15b1984a-Paper.pdf},
}

@InProceedings{Christianos2020,
  author    = {Filippos Christianos and Lukas Sch{\"a}fer and Albrecht, {Stefano V}},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS 2020)},
  title     = {Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning},
  year      = {2020},
  month     = dec,
  note      = {Thirty-fourth Conference on Neural Information Processing Systems, NeurIPS 2020 ; Conference date: 06-12-2020 Through 12-12-2020},
  pages     = {10707--10717},
  publisher = {Curran Associates Inc},
  abstract  = {Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms two baselines and two state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.},
  day       = {6},
  file      = {:Christianos2020 - Shared Experience Actor Critic for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  language  = {English},
  url       = {https://nips.cc/Conferences/2020},
}

@Article{Zhou2020a,
  author        = {Meng Zhou and Ziyu Liu and Pengwei Sui and Yixuan Li and Yuk Ying Chung},
  title         = {Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning},
  year          = {2020},
  month         = jul,
  abstract      = {We present a multi-agent actor-critic method that aims to implicitly address the credit assignment problem under fully cooperative settings. Our key motivation is that credit assignment among agents may not require an explicit formulation as long as (1) the policy gradients derived from a centralized critic carry sufficient information for the decentralized agents to maximize their joint action value through optimal cooperation and (2) a sustained level of exploration is enforced throughout training. Under the centralized training with decentralized execution (CTDE) paradigm, we achieve the former by formulating the centralized critic as a hypernetwork such that a latent state representation is integrated into the policy gradients through its multiplicative association with the stochastic policies; to achieve the latter, we derive a simple technique called adaptive entropy regularization where magnitudes of the entropy gradients are dynamically rescaled based on the current policy stochasticity to encourage consistent levels of exploration. Our algorithm, referred to as LICA, is evaluated on several benchmarks including the multi-agent particle environments and a set of challenging StarCraft II micromanagement tasks, and we show that LICA significantly outperforms previous methods.},
  archiveprefix = {arXiv},
  comment       = {LICA},
  eprint        = {2007.02529},
  file          = {:Zhou2020a - Learning Implicit Credit Assignment for Cooperative Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
}

@InProceedings{James2019,
  author    = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks},
  year      = {2019},
  month     = {June},
  file      = {:James_2019_CVPR - Sim to Real Via Sim to Sim_ Data Efficient Robotic Grasping Via Randomized to Canonical Adaptation Networks.pdf:PDF},
  groups    = {Sim to Real},
  ranking   = {rank3},
}

@InProceedings{Chebotar2019,
  author    = {Yevgen Chebotar and Ankur Handa and Viktor Makoviychuk and Miles Macklin and Jan Issac and Nathan Ratliff and Dieter Fox},
  booktitle = {2019 International Conference on Robotics and Automation ({ICRA})},
  title     = {Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra.2019.8793789},
  file      = {:Chebotar2019 - Closing the Sim to Real Loop_ Adapting Simulation Randomization with Real World Experience.pdf:PDF},
  groups    = {Sim to Real},
  ranking   = {rank3},
}

@InProceedings{Peng2018,
  author    = {Xue Bin Peng and Marcin Andrychowicz and Wojciech Zaremba and Pieter Abbeel},
  booktitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  title     = {Sim-to-Real Transfer of Robotic Control with Dynamics Randomization},
  year      = {2018},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra.2018.8460528},
  file      = {:peng2018.pdf:PDF},
  groups    = {Sim to Real},
  ranking   = {rank4},
}

@Article{Andrychowicz2020,
  author   = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
  journal  = {The International Journal of Robotics Research},
  title    = {Learning dexterous in-hand manipulation},
  year     = {2020},
  number   = {1},
  pages    = {3-20},
  volume   = {39},
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.},
  doi      = {10.1177/0278364919887447},
  eprint   = {https://doi.org/10.1177/0278364919887447},
  file     = {:doi_10.1177_0278364919887447 - Learning Dexterous in Hand Manipulation.pdf:PDF},
  groups   = {Sim to Real},
  ranking  = {rank4},
  url      = {https://doi.org/10.1177/0278364919887447},
}

@Article{Sutton1999,
  author   = {Richard S. Sutton and Doina Precup and Satinder Singh},
  journal  = {Artificial Intelligence},
  title    = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  year     = {1999},
  issn     = {0004-3702},
  number   = {1},
  pages    = {181-211},
  volume   = {112},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
  doi      = {https://doi.org/10.1016/S0004-3702(99)00052-1},
  file     = {:SUTTON1999181 - Between MDPs and Semi MDPs_ a Framework for Temporal Abstraction in Reinforcement Learning.pdf:PDF},
  groups   = {RL},
  keywords = {Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes},
  ranking  = {rank4},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370299000521},
}

@InProceedings{Colas2020,
  author    = {Colas, C\'{e}dric and Karch, Tristan and Lair, Nicolas and Dussoux, Jean-Michel and Moulin-Frier, Cl\'{e}ment and Dominey, Peter and Oudeyer, Pierre-Yves},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Language as a Cognitive Tool to Imagine Goals in Curiosity Driven Exploration},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {3761--3774},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  comment   = {IMAGINE},
  file      = {:NEURIPS2020_274e6fcf - Language As a Cognitive Tool to Imagine Goals in Curiosity Driven Exploration.pdf:PDF},
  groups    = {Curiosity, Language-Augmented RL},
  priority  = {prio1},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf},
}

@Article{Chan2019,
  author        = {Harris Chan and Yuhuai Wu and Jamie Kiros and Sanja Fidler and Jimmy Ba},
  title         = {ACTRCE: Augmenting Experience via Teacher's Advice For Multi-Goal Reinforcement Learning},
  year          = {2019},
  month         = feb,
  abstract      = {Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failed experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, and even small amount of advice is sufficient for the agent to achieve good performance.},
  archiveprefix = {arXiv},
  eprint        = {1902.04546},
  file          = {:Chan2019 - ACTRCE_ Augmenting Experience Via Teacher's Advice for Multi Goal Reinforcement Learning.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.LG, cs.AI, cs.NE, stat.ML},
  primaryclass  = {cs.LG},
}

@InProceedings{NEURIPS2019_0af78794,
  author    = {Jiang, YiDing and Gu, Shixiang (Shane) and Murphy, Kevin P and Finn, Chelsea},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Language as an Abstraction for Hierarchical Deep Reinforcement Learning},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  file      = {:NEURIPS2019_0af78794 - Language As an Abstraction for Hierarchical Deep Reinforcement Learning.pdf:PDF},
  groups    = {Hierarchical RL, Language-Augmented RL},
  url       = {https://proceedings.neurips.cc/paper/2019/file/0af787945872196b42c9f73ead2565c8-Paper.pdf},
}

@InProceedings{Cideron2020,
  author    = {Geoffrey Cideron and Mathieu Seurin and Florian Strub and Olivier Pietquin},
  booktitle = {2020 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
  title     = {{HIGhER}: Improving instruction following with Hindsight Generation for Experience Replay},
  year      = {2020},
  month     = {dec},
  publisher = {{IEEE}},
  doi       = {10.1109/ssci47803.2020.9308603},
  file      = {:Cideron2020 - HIGhER_ Improving Instruction Following with Hindsight Generation for Experience Replay.pdf:PDF},
  groups    = {Language-Augmented RL},
}

@Article{Zhou_Small_2021,
  author       = {Zhou, Li and Small, Kevin},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Inverse Reinforcement Learning with Natural Language Goals},
  year         = {2021},
  month        = {May},
  number       = {12},
  pages        = {11116-11124},
  volume       = {35},
  abstractnote = {Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.},
  file         = {:Zhou_Small_2021 - Inverse Reinforcement Learning with Natural Language Goals.pdf:PDF},
  groups       = {Language-Augmented RL},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17326},
}

@Article{Nguyen2021,
  author        = {Khanh Nguyen and Dipendra Misra and Robert Schapire and Miro Dudík and Patrick Shafto},
  title         = {Interactive Learning from Activity Description},
  year          = {2021},
  month         = feb,
  abstract      = {We present a novel interactive learning protocol that enables training request-fulfilling agents by verbally describing their activities. Unlike imitation learning (IL), our protocol allows the teaching agent to provide feedback in a language that is most appropriate for them. Compared with reward in reinforcement learning (RL), the description feedback is richer and allows for improved sample complexity. We develop a probabilistic framework and an algorithm that practically implements our protocol. Empirical results in two challenging request-fulfilling problems demonstrate the strengths of our approach: compared with RL baselines, it is more sample-efficient; compared with IL baselines, it achieves competitive success rates without requiring the teaching agent to be able to demonstrate the desired behavior using the learning agent's actions. Apart from empirical evaluation, we also provide theoretical guarantees for our algorithm under certain assumptions about the teacher and the environment.},
  archiveprefix = {arXiv},
  eprint        = {2102.07024},
  file          = {:Nguyen2021 - Interactive Learning from Activity Description.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.CL, cs.AI, cs.HC, cs.LG},
  primaryclass  = {cs.CL},
}

@Article{Hill2019,
  author        = {Felix Hill and Andrew Lampinen and Rosalia Schneider and Stephen Clark and Matthew Botvinick and James L. McClelland and Adam Santoro},
  title         = {Environmental drivers of systematicity and generalization in a situated agent},
  year          = {2019},
  month         = oct,
  abstract      = {The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we consider tests of out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room. We first describe a comparatively generic agent architecture that exhibits strong performance on these tests. We then identify three aspects of the training regime and environment that make a significant difference to its performance: (a) the number of object/word experiences in the training set; (b) the visual invariances afforded by the agent's perspective, or frame of reference; and (c) the variety of visual input inherent in the perceptual aspect of the agent's perception. Our findings indicate that the degree of generalisation that networks exhibit can depend critically on particulars of the environment in which a given task is instantiated. They further suggest that the propensity for neural networks to generalise in systematic ways may increase if, like human children, those networks have access to many frames of richly varying, multi-modal observations as they learn.},
  archiveprefix = {arXiv},
  eprint        = {1910.00571},
  file          = {:Hill2019 - Environmental Drivers of Systematicity and Generalization in a Situated Agent.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@InProceedings{pmlr-v70-andreas17a,
  author    = {Jacob Andreas and Dan Klein and Sergey Levine},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Modular Multitask Reinforcement Learning with Policy Sketches},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {06--11 Aug},
  pages     = {166--175},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.},
  file      = {:pmlr-v70-andreas17a - Modular Multitask Reinforcement Learning with Policy Sketches.pdf:PDF},
  groups    = {Language-Augmented RL},
  pdf       = {http://proceedings.mlr.press/v70/andreas17a/andreas17a.pdf},
  ranking   = {rank4},
  url       = {http://proceedings.mlr.press/v70/andreas17a.html},
}

@InProceedings{Shridhar2021,
  author    = {Mohit Shridhar and Xingdi Yuan and Marc-Alexandre Cote and Yonatan Bisk and Adam Trischler and Matthew Hausknecht},
  booktitle = {International Conference on Learning Representations},
  title     = {ALFWorld: Aligning Text and Embodied Environments for Interactive Learning},
  year      = {2021},
  file      = {:Shridhar2021 - ALFWorld_ Aligning Text and Embodied Environments for Interactive Learning.pdf:PDF},
  groups    = {Language-Augmented RL},
  url       = {https://openreview.net/forum?id=0IOX0YcCdTn},
}

@InProceedings{akakzia:hal-03121146,
  author      = {Akakzia, Ahmed and Colas, C{\'e}dric and Oudeyer, Pierre-Yves and Chetouani, Mohamed and Sigaud, Olivier},
  booktitle   = {{ICLR 2021 - Ninth International Conference on Learning Representation}},
  title       = {{Grounding Language to Autonomously-Acquired Skills via Goal Generation}},
  year        = {2021},
  address     = {Vienna / Virtual, Austria},
  month       = May,
  comment     = {LGB},
  file        = {:akakzia_hal-03121146 - Grounding Language to Autonomously Acquired Skills Via Goal Generation.pdf:PDF},
  groups      = {Language-Augmented RL},
  hal_id      = {hal-03121146},
  hal_version = {v1},
  keywords    = {Reinforcement learning ; Artificial intelligence and robotics},
  pdf         = {https://hal.inria.fr/hal-03121146/file/2006.07185.pdf},
  priority    = {prio1},
  url         = {https://hal.inria.fr/hal-03121146},
}

@InProceedings{Luketina2019,
  author    = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rocktäschel, Tim},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
  title     = {A Survey of Reinforcement Learning Informed by Natural Language},
  year      = {2019},
  month     = {7},
  pages     = {6309--6317},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  doi       = {10.24963/ijcai.2019/880},
  file      = {:Luketina2019 - A Survey of Reinforcement Learning Informed by Natural Language.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio1},
  ranking   = {rank2},
  url       = {https://doi.org/10.24963/ijcai.2019/880},
}

@Article{Karch2021,
  author        = {Tristan Karch and Laetitia Teodorescu and Katja Hofmann and Clément Moulin-Frier and Pierre-Yves Oudeyer},
  title         = {Grounding Spatio-Temporal Language with Transformers},
  year          = {2021},
  month         = jun,
  abstract      = {Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents. We also release our code under open-source license as well as pretrained models and datasets to encourage the wider community to build upon and extend our work in the future.},
  archiveprefix = {arXiv},
  eprint        = {2106.08858},
  file          = {:Karch2021 - Grounding Spatio Temporal Language with Transformers.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.AI, cs.CL, cs.LG},
  primaryclass  = {cs.AI},
  priority      = {prio2},
}

@InProceedings{Wang2020b,
  author    = {Wang, Tonghan and Dong, Heng and Lesser, Victor and Zhang, Chongjie},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {{ROMA}: Multi-Agent Reinforcement Learning with Emergent Roles},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {9876--9886},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at https://sites.google.com/view/romarl/.},
  file      = {:Wang2020 - ROMA_ Multi Agent Reinforcement Learning with Emergent Roles.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v119/wang20f/wang20f.pdf},
  ranking   = {rank2},
  url       = {http://proceedings.mlr.press/v119/wang20f.html},
}

@InProceedings{Wang2021,
  author    = {Jianhao Wang and Zhizhou Ren and Terry Liu and Yang Yu and Chongjie Zhang},
  booktitle = {International Conference on Learning Representations},
  title     = {QPLEX: Duplex Dueling Multi-Agent Q-Learning},
  year      = {2021},
  file      = {:Wang2021 - QPLEX_ Duplex Dueling Multi Agent Q Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=Rcmk0xxIQV},
}

@InProceedings{Wang2020c,
  author    = {Tonghan Wang and Jianhao Wang and Chongyi Zheng and Chongjie Zhang},
  booktitle = {International Conference on Learning Representations},
  title     = {Learning Nearly Decomposable Value Functions Via Communication Minimization},
  year      = {2020},
  file      = {:Wang2020 - Learning Nearly Decomposable Value Functions Via Communication Minimization.pdf:PDF},
  groups    = {Communication},
  priority  = {prio1},
  url       = {https://openreview.net/forum?id=HJx-3grYDB},
}

@Book{Vygotsky1934,
  author = {Lev S. Vygotsky},
  title  = {Thought and Language},
  year   = {1934},
  groups = {Language-Augmented RL},
}

@InProceedings{Andreas2018,
  author    = {Jacob Andreas and Dan Klein and Sergey Levine},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  title     = {Learning with Latent Language},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/n18-1197},
  file      = {:Andreas2018 - Learning with Latent Language.pdf:PDF},
  groups    = {Language-Augmented RL},
  ranking   = {rank1},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:RL\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Policy based\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Actor Critic\;2\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:TRPO\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:DDPG\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Soft Actor Critic\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Model based\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Model-based exploration\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Latent Dynamics Model\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Intrinsic goals\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Curiosity\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Disagreement\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Diversity\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Visitation count\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Random Exploration\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Adversary Guidance\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Value based\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Robotics\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-robot\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Heterogeneous\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Multi-Robot Communication\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Navigation\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-task\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Information gathering\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Coverage Path Planning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Platforms\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Sim to Real\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Space Exploration\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:POMDP\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:MARL\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-agent Systems\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-agent learning\;2\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Credit Assignment\;2\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:Shapley value\;2\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:Wonderful Life Utility\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-agent RL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Graphs\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Communication\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:MAS Reviews\;2\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Heterogeneous Systems\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:H-MADRL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Data Augmentation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Offline RL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Meta-Learning\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Imitation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Hierarchical RL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Language-Augmented RL\;0\;1\;0x8a8a8aff\;\;\;;
}
