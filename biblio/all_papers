@InProceedings{Nguyen2018,
  author    = {Duc Thien Nguyen and Akshat Kumar and Hoong Chuin Lau},
  booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS 2018)},
  title     = {Credit Assignment For Collective Multiagent RL With Global Rewards},
  year      = {2018},
  pages     = {8113-8124},
  cdate     = {1514764800000},
  crossref  = {conf/nips/2018},
  file      = {:NguyenKL18 - Credit Assignment for Collective Multiagent RL with Global Rewards.pdf:PDF},
  groups    = {Multi-agent RL, Credit Assignment},
  ranking   = {rank1},
  url       = {http://papers.nips.cc/paper/8033-credit-assignment-for-collective-multiagent-rl-with-global-rewards},
}

@InProceedings{AndrychowiczCRS17,
  author     = {Marcin Andrychowicz and Dwight Crow and Alex Ray and Jonas Schneider and Rachel Fong and Peter Welinder and Bob McGrew and Josh Tobin and Pieter Abbeel and Wojciech Zaremba},
  booktitle  = {NIPS},
  title      = {Hindsight Experience Replay},
  year       = {2017},
  pages      = {5055-5065},
  cdate      = {1483228800000},
  crossref   = {conf/nips/2017},
  file       = {:AndrychowiczCRS17 - Hindsight Experience Replay.pdf:PDF},
  groups     = {RL in Robotics, Goal-conditioned RL},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {http://papers.nips.cc/paper/7090-hindsight-experience-replay},
}

@Article{Haarnoja2018_SAC,
  author     = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  title      = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  year       = {2018},
  month      = {10--15 Jul},
  pages      = {1861--1870},
  volume     = {80},
  abstract   = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  booktitle  = {Proceedings of the 35th International Conference on Machine Learning},
  comment    = {Soft Actor Critic
- PROBLEM:
	Model-free DRL algos typically suffer from two major challenges: very high sample complexity and brittle convergence properties (=> meticulous hyperparameter tuning)
- SOLUTION:
	Soft Actor-Critic: based on Maximum Entropy RL framework: maximize expected reward while also maximizing entropy = succeed at the task while acting as randomly as possible
- MODEL:
	Double DQN to mitigate overestimation of Q values
	Target network for policy and Q-functions for more stable training
- RESULTS:
	Outperforms sota model-free DRL methods (DDPG and PPO)
	Very stable accross different random seeds},
  editor     = {Dy, Jennifer and Krause, Andreas},
  file       = {:Haarnoja2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF},
  groups     = {Policy based, Soft Actor Critic, Actor Critic},
  pdf        = {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  publisher  = {PMLR},
  ranking    = {rank5},
  readstatus = {read},
  series     = {Proceedings of Machine Learning Research},
  url        = {https://proceedings.mlr.press/v80/haarnoja18b.html},
}

@InProceedings{Fujimoto2018_TD3,
  author     = {Scott Fujimoto and Herke van Hoof and David Meger},
  booktitle  = {Proceedings of the 35th International Conference on Machine Learning},
  title      = {Addressing Function Approximation Error in Actor-Critic Methods},
  year       = {2018},
  editor     = {Dy, Jennifer and Krause, Andreas},
  month      = {10--15 Jul},
  pages      = {1587--1596},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {80},
  abstract   = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  comment    = {- PROBLEM:
	Overestimation of Q-values estimates leading to suboptimal policies, even in DDPG
- SOLUTION:
	TD3: Double DQN and Clipped Double-Q trick
	Target networks => more stable training
	Delayed Policy Updates => more stable
	Target Policy smoothing: add noise to choses action  => mitigates overestimation of Q-values
- RESULTS:
	Reduces overestimation of Q-values
	Outperforms all previous policy-based algorithms},
  file       = {:Fujimoto2018 - Addressing Function Approximation Error in Actor Critic Methods.pdf:PDF},
  groups     = {Policy based, DDPG, Actor Critic},
  pdf        = {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v80/fujimoto18a.html},
}

@Article{Schulman2017_PPO,
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  comment       = {PPO},
  eprint        = {1707.06347},
  file          = {:Schulman2017 - Proximal Policy Optimization Algorithms.pdf:PDF},
  groups        = {Policy based, Actor Critic, TRPO-PPO},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  ranking       = {rank5},
  readstatus    = {read},
}

@InProceedings{Schulman2015_TRPO,
  author     = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle  = {Proceedings of the 32nd International Conference on Machine Learning},
  title      = {Trust Region Policy Optimization},
  year       = {2015},
  address    = {Lille, France},
  editor     = {Bach, Francis and Blei, David},
  month      = {07--09 Jul},
  pages      = {1889--1897},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {37},
  abstract   = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  comment    = {TRPO},
  file       = {:Schulman2015 - Trust Region Policy Optimization.pdf:PDF},
  groups     = {Policy based, Actor Critic, TRPO-PPO},
  pdf        = {http://proceedings.mlr.press/v37/schulman15.pdf},
  ranking    = {rank5},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v37/schulman15.html},
}

@Article{Schrittwieser2019_Muzero,
  author        = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
  title         = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  year          = {2019},
  month         = nov,
  abstract      = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  comment       = {Muzero},
  doi           = {10.1038/s41586-020-03051-4},
  eprint        = {1911.08265},
  file          = {:Schrittwieser2019 - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hafner2019,
  author        = {Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
  title         = {Dream to Control: Learning Behaviors by Latent Imagination},
  year          = {2019},
  month         = dec,
  abstract      = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  archiveprefix = {arXiv},
  comment       = {Dreamer},
  eprint        = {1912.01603},
  file          = {:Hafner2019 - Dream to Control_ Learning Behaviors by Latent Imagination.pdf:PDF},
  groups        = {Model based, Latent Dynamics Model},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@InProceedings{Pathak2017_ICM,
  author        = {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  title         = {Curiosity-driven Exploration by Self-supervised Prediction},
  year          = {2017},
  month         = may,
  pages         = {2778-2787},
  volume        = {PMLR 70},
  abstract      = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  archiveprefix = {arXiv},
  comment       = {Intrinsinc Curiosity Module},
  eprint        = {1705.05363},
  file          = {:Pathak2017 - Curiosity Driven Exploration by Self Supervised Prediction.pdf:PDF},
  groups        = {Curiosity, Intrinsic goals},
  journal       = {Proceedings of the 34th International Conference on Machine Learning},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
  readstatus    = {read},
}

@InProceedings{Burda2019_RND,
  author        = {Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
  booktitle     = {7th International Conference on Learning Representations},
  title         = {Exploration by Random Network Distillation},
  year          = {2019},
  month         = oct,
  abstract      = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  archiveprefix = {arXiv},
  eprint        = {1810.12894},
  file          = {:Burda2018 - Exploration by Random Network Distillation.pdf:PDF},
  groups        = {Curiosity, Intrinsic goals},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
  readstatus    = {read},
}

@Article{Lillicrap2015_DDPG,
  author        = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  title         = {Continuous control with deep reinforcement learning},
  year          = {2015},
  month         = sep,
  abstract      = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Adapt DQN to continuous action domain
- SOLUTION:
	Combine Actor-Critic with the recent successes of DQN
	Policy network to compute action that maximizes Q
	DQN to learn Q
- RESULTS:
	Achieves learning policy for continuous actions
	Struggles to learn from raw pixels somtimes
	Overestimation of Q values sometimes},
  eprint        = {1509.02971},
  file          = {:Lillicrap2015 - Continuous Control with Deep Reinforcement Learning.pdf:PDF},
  groups        = {Policy based, DDPG, Actor Critic},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Williams1992_Reinforce,
  author     = {Williams, Ronald J.},
  journal    = {Machine Learning},
  title      = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  year       = {1992},
  issn       = {1573-0565},
  month      = may,
  number     = {3–4},
  pages      = {229--256},
  volume     = {8},
  comment    = {REINFORCE},
  doi        = {10.1007/bf00992696},
  groups     = {Policy based},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{Zhang2018,
  author        = {Marvin Zhang and Sharad Vikram and Laura Smith and Pieter Abbeel and Matthew J. Johnson and Sergey Levine},
  title         = {SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning},
  year          = {2018},
  month         = aug,
  abstract      = {Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.},
  archiveprefix = {arXiv},
  eprint        = {1808.09105},
  file          = {:Zhang2018 - SOLAR_ Deep Structured Representations for Model Based Reinforcement Learning.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@InProceedings{Sekar2020_Plan2Explore,
  author        = {Ramanan Sekar and Oleh Rybkin and Kostas Daniilidis and Pieter Abbeel and Danijar Hafner and Deepak Pathak},
  title         = {Planning to Explore via Self-Supervised World Models},
  year          = {2020},
  month         = may,
  pages         = {8583-8592},
  volume        = {PMLR 119},
  abstract      = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Find learning algorithms that are sample efficient and not task specific.
- SOLUTION:
	Plan2Explore: self-supervised exploration and fast adaptation to new tasks. Instead of maximizing an instinsic reward in retrospect, it learns a world model to plan ahead and seek novelty in future situations.
- MODEL:
	Encode images with CNN
	Learn a world model with PlaNet
	Learn policy and value with Dreamer
	Induce exploration by generating an intrinsic reward using ensemble disagreement, like Pathak2019
	No reward from the environment during learning the model
	After learning the world model through exploration, adapts on tasks in zero-shot (only imagination) or few-shot (imagination and few iteraction)},
  eprint        = {2005.05960},
  file          = {:Sekar2020 - Planning to Explore Via Self Supervised World Models.pdf:PDF},
  groups        = {Model based, Disagreement, Model-based exploration, Intrinsic goals},
  journal       = {Proceedings of the 37th International Conference on Machine Learning},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.NE, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
  readstatus    = {read},
}

@InProceedings{Pathak2019_Disagreement,
  author        = {Deepak Pathak and Dhiraj Gandhi and Abhinav Gupta},
  booktitle     = {Proceedings of the 36th International Conference on Machine Learning},
  title         = {Self-Supervised Exploration via Disagreement},
  year          = {2019},
  month         = jun,
  pages         = {5062--5071},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  volume        = {97},
  abstract      = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/},
  archiveprefix = {arXiv},
  eprint        = {1906.04161},
  file          = {:Pathak2019 - Self Supervised Exploration Via Disagreement.pdf:PDF},
  groups        = {Disagreement, Intrinsic goals},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
  readstatus    = {read},
  url           = {https://proceedings.mlr.press/v97/pathak19a.html},
}

@InProceedings{Hessel2018_Rainbow,
  author       = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  title        = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance. &lt;/p&gt;},
  doi          = {10.1609/aaai.v32i1.11796},
  file         = {:Hessel2017 - Rainbow_ Combining Improvements in Deep Reinforcement Learning.pdf:PDF},
  groups       = {Value based},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11796},
}

@Article{Vecerik2017_Demonstrations,
  author        = {Mel Vecerik and Todd Hester and Jonathan Scholz and Fumin Wang and Olivier Pietquin and Bilal Piot and Nicolas Heess and Thomas Rothörl and Thomas Lampe and Martin Riedmiller},
  title         = {Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sampling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Normalized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kinesthetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.},
  archiveprefix = {arXiv},
  eprint        = {1707.08817},
  file          = {:Vecerik2017 - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards.pdf:PDF},
  groups        = {DDPG, Policy based, Actor Critic, RL in Robotics},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank3},
}

@Article{Eysenbach2018,
  author        = {Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
  title         = {Diversity is All You Need: Learning Skills without a Reward Function},
  year          = {2018},
  month         = feb,
  abstract      = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  archiveprefix = {arXiv},
  eprint        = {1802.06070},
  file          = {:Eysenbach2018 - Diversity Is All You Need_ Learning Skills without a Reward Function.pdf:PDF},
  groups        = {Diversity, Intrinsic goals},
  keywords      = {cs.AI, cs.RO},
  primaryclass  = {cs.AI},
  readstatus    = {skimmed},
}

@Article{Burda2018a,
  author     = {Yuri Burda and Harri Edwards and Deepak Pathak and Amos Storkey and Trevor Darrell and Alexei A. Efros},
  title      = {Large-Scale Study of Curiosity-Driven Learning},
  year       = {2019},
  abstract   = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  booktitle  = {International Conference on Learning Representations},
  file       = {:Burda2018a - Large Scale Study of Curiosity Driven Learning.pdf:PDF},
  groups     = {Curiosity, Intrinsic goals},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=rJNwDjAqYX},
}

@Article{Achiam2017,
  author        = {Joshua Achiam and Shankar Sastry},
  title         = {Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},
  year          = {2017},
  month         = mar,
  abstract      = {Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.},
  archiveprefix = {arXiv},
  eprint        = {1703.01732},
  file          = {:Achiam2017 - Surprise Based Intrinsic Motivation for Deep Reinforcement Learning.pdf:PDF},
  groups        = {Curiosity, Intrinsic goals},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Article{Ha2018,
  author        = {David Ha and Jürgen Schmidhuber},
  title         = {World Models},
  year          = {2018},
  month         = mar,
  abstract      = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Human base their decisions and actions on a mental model of the world, itself based on their senses and predictions of the future.
	RL algos often use small NNs because they iterate faster to learn a good policy, but they would benefit from large RNNs that learn rich spatial and temporal representations of data
- SOLUTION:
	Large RNN to learn a world model in an unsupervised manner
	Small controller to learn to perform task in this world model
- MODEL: 
	VAE to learn abstract, compressed latent representation of obseverd image frame
	Mixture Density Network-RNN to predict future latent representation: model probability density of next latent state (as a mixture of Gaussian distribution) based on current action, latent state and hidden state of the RNN
	Very simple controller trained separately in latent imagination: single layer linear model to map latent state and hidden state to action
- RESULTS:
	Demonstrate the possibility of training agents entirely inside of its simulated latent space dream world},
  doi           = {10.5281/zenodo.1207631},
  eprint        = {1803.10122},
  file          = {:Ha2018 - World Models.pdf:PDF},
  groups        = {Model based, Latent Dynamics Model},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hafner2018,
  author        = {Danijar Hafner and Timothy Lillicrap and Ian Fischer and Ruben Villegas and David Ha and Honglak Lee and James Davidson},
  title         = {Learning Latent Dynamics for Planning from Pixels},
  year          = {2018},
  month         = nov,
  abstract      = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
  archiveprefix = {arXiv},
  comment       = {- MODEL:
	PlaNet: Deep Planning Network, a model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space
- RESULTS:
	Beat A3C and sometimes D4PG
	Way more sample efficient (200x)},
  eprint        = {1811.04551},
  file          = {:Hafner2018 - Learning Latent Dynamics for Planning from Pixels.pdf:PDF},
  groups        = {Model based, Latent Dynamics Model},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@InProceedings{Shyam2019_MAX,
  author        = {Pranav Shyam and Wojciech Jaśkowski and Faustino Gomez},
  title         = {Model-Based Active Exploration},
  year          = {2019},
  month         = oct,
  pages         = {5779-5788},
  volume        = {PMLR 97},
  abstract      = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Over-commitment: intrinsic exploration bonus have to be unlearned once the novelty of a state's vicinity has worn off, making exploration inefficient
- SOLUTION:
	Model-based Active eXploration (MAX): actively seek out novelty in future states by measuring the amount of conflict between predictions of an ensemble of forward models
- RESULTS:
	Active exploration prevents from getting stuck in a local optimum
	Model-based methods suffer from model-bias: bad model in certain regions of the state space leading to bad policy. MAX would explore more difficult aspects of the environment, thereby improving the quality of the models
	Less computationally efficient than baselines, BUT trading off for data efficiency},
  eprint        = {1810.12162},
  file          = {:Shyam2018 - Model Based Active Exploration.pdf:PDF},
  groups        = {Model-based exploration, Model based, Disagreement},
  journal       = {Proceedings of the 36th International Conference on Machine Learning},
  keywords      = {cs.LG, cs.AI, cs.IT, cs.NE, math.IT, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
  readstatus    = {skimmed},
}

@Article{Lee2019,
  author        = {Alex X. Lee and Anusha Nagabandi and Pieter Abbeel and Sergey Levine},
  title         = {Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model},
  year          = {2019},
  month         = jul,
  abstract      = {Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	It is difficult to learn directly from high-dimensional image inputs (task learning)
	It is difficult to extract compact representations of the underlying task-relevant information from which to learn (representation learning)
- SOLUTION:
	Treat task learning and representation learning separately: learn a latent representation space with a predictive model, and train a RL agent in that learning latent space
- MODEL:
	Maximize ELBO (model objective + maximum entropy RL objective)
	Sequential latent variable model objective: predict next latent state based on next state, current latent state and current action
	RL objective: Soft Actor-Critic
- RESULTS: 
	Better performance than several sota model-free and model-based approaches
	More sample-efficient},
  eprint        = {1907.00953},
  file          = {:Lee2019 - Stochastic Latent Actor Critic_ Deep Reinforcement Learning with a Latent Variable Model (1).pdf:PDF},
  groups        = {Model based, POMDP, Soft Actor Critic, Latent Dynamics Model},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Zhu2018,
  author        = {Pengfei Zhu and Xin Li and Pascal Poupart and Guanghui Miao},
  title         = {On Improving Deep Reinforcement Learning for POMDPs},
  year          = {2018},
  month         = apr,
  abstract      = {Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.},
  archiveprefix = {arXiv},
  eprint        = {1804.06309},
  file          = {:- On Improving Deep Reinforcement Learning for POMDPs.pdf:PDF},
  groups        = {POMDP},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Tampuu2015,
  author        = {Ardi Tampuu and Tambet Matiisen and Dorian Kodelja and Ilya Kuzovkin and Kristjan Korjus and Juhan Aru and Jaan Aru and Raul Vicente},
  title         = {Multiagent Cooperation and Competition with Deep Reinforcement Learning},
  year          = {2015},
  month         = nov,
  abstract      = {Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.},
  archiveprefix = {arXiv},
  eprint        = {1511.08779},
  file          = {:Tampuu2015 - Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL, Multi-agent Systems},
  keywords      = {cs.AI, cs.LG, q-bio.NC},
  primaryclass  = {cs.AI},
}

@Article{Wang2019,
  author        = {Yixiang Wang and Feng Wu},
  title         = {Multi-Agent Deep Reinforcement Learning with Adaptive Policies},
  year          = {2019},
  month         = nov,
  abstract      = {We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution. We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments.},
  archiveprefix = {arXiv},
  eprint        = {1912.00949},
  file          = {:Wang2019 - Multi Agent Deep Reinforcement Learning with Adaptive Policies.pdf:PDF},
  groups        = {Multi-agent RL, Multi-agent Systems},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Bard2020,
  author    = {Nolan Bard and Jakob N. Foerster and Sarath Chandar and Neil Burch and Marc Lanctot and H. Francis Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and Hugo Larochelle and Marc G. Bellemare and Michael Bowling},
  journal   = {Artif. Intell.},
  title     = {The Hanabi challenge: {A} new frontier for {AI} research},
  year      = {2020},
  pages     = {103216},
  volume    = {280},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ai/BardFCBLSPDMHDM20.bib},
  doi       = {10.1016/j.artint.2019.103216},
  file      = {:Bard2020 - The Hanabi Challenge_ a New Frontier for AI Research.pdf:PDF},
  groups    = {Multi-agent Systems, Multi-agent learning},
}

@Article{Long2020,
  author        = {Qian Long and Zihan Zhou and Abhibav Gupta and Fei Fang and Yi Wu and Xiaolong Wang},
  title         = {Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning},
  year          = {2020},
  month         = mar,
  abstract      = {In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely, EPC maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets of agents with the best adaptability to the next stage. We implement EPC on a popular MARL algorithm, MADDPG, and empirically show that our approach consistently outperforms baselines by a large margin as the number of agents grows exponentially.},
  archiveprefix = {arXiv},
  eprint        = {2003.10423},
  file          = {:Long2020 - Evolutionary Population Curriculum for Scaling Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL, Multi-agent Systems},
  keywords      = {cs.LG, cs.AI, cs.NE, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Li,
  author  = {Shihui Li and Yi Wu and Xinyue Cui and Honghua Dong and Fei Fang and Stuart Russell},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title   = {Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient},
  year    = {2019},
  issn    = {2374-3468},
  pages   = {4213-4220},
  volume  = {33},
  comment = {M3DDPG},
  doi     = {10.1609/aaai.v33i01.33014213},
  file    = {:Li - Robust Multi Agent Reinforcement Learning Via Minimax Deep Deterministic Policy Gradient.pdf:PDF},
  groups  = {Multi-agent RL, Multi-agent Systems},
  ranking = {rank2},
}

@Article{Hu2021,
  author        = {Siyi Hu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang},
  title         = {UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers},
  year          = {2021},
  month         = jan,
  abstract      = {Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).},
  archiveprefix = {arXiv},
  eprint        = {2101.08001},
  file          = {:Hu2021 - UPDeT_ Universal Multi Agent Reinforcement Learning Via Policy Decoupling with Transformers.pdf:PDF},
  groups        = {Multi-agent RL, Transformers in RL, Multi-agent Systems},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@Article{Rafailov2020,
  author        = {Rafael Rafailov and Tianhe Yu and Aravind Rajeswaran and Chelsea Finn},
  title         = {Offline Reinforcement Learning from Images with Latent Space Models},
  year          = {2020},
  month         = dec,
  abstract      = {Offline reinforcement learning (RL) refers to the problem of learning policies from a static dataset of environment interactions. Offline RL enables extensive use and re-use of historical datasets, while also alleviating safety concerns associated with online exploration, thereby expanding the real-world applicability of RL. Most prior work in offline RL has focused on tasks with compact state representations. However, the ability to learn directly from rich observation spaces like images is critical for real-world applications such as robotics. In this work, we build on recent advances in model-based algorithms for offline RL, and extend them to high-dimensional visual observation spaces. Model-based offline RL algorithms have achieved state of the art results in state based tasks and have strong theoretical guarantees. However, they rely crucially on the ability to quantify uncertainty in the model predictions, which is particularly challenging with image observations. To overcome this challenge, we propose to learn a latent-state dynamics model, and represent the uncertainty in the latent space. Our approach is both tractable in practice and corresponds to maximizing a lower bound of the ELBO in the unknown POMDP. In experiments on a range of challenging image-based locomotion and manipulation tasks, we find that our algorithm significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods. Moreover, we also find that our approach excels on an image-based drawer closing task on a real robot using a pre-existing dataset. All results including videos can be found online at https://sites.google.com/view/lompo/ .},
  archiveprefix = {arXiv},
  eprint        = {2012.11547},
  file          = {:Rafailov2020 - Offline Reinforcement Learning from Images with Latent Space Models.pdf:PDF},
  groups        = {Latent Dynamics Model, Offline RL},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
}

@Article{Laskin2020,
  author        = {Michael Laskin and Kimin Lee and Adam Stooke and Lerrel Pinto and Pieter Abbeel and Aravind Srinivas},
  title         = {Reinforcement Learning with Augmented Data},
  year          = {2020},
  month         = apr,
  abstract      = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
  archiveprefix = {arXiv},
  eprint        = {2004.14990},
  file          = {:Laskin2020 - Reinforcement Learning with Augmented Data.pdf:PDF},
  groups        = {Data Augmentation},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Kostrikov2020,
  author        = {Ilya Kostrikov and Denis Yarats and Rob Fergus},
  title         = {Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels},
  year          = {2020},
  month         = apr,
  abstract      = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  eprint        = {2004.13649},
  file          = {:Kostrikov2020 - Image Augmentation Is All You Need_ Regularizing Deep Reinforcement Learning from Pixels.pdf:PDF},
  groups        = {Data Augmentation},
  keywords      = {cs.LG, cs.CV, eess.IV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Kidambi2020,
  author        = {Rahul Kidambi and Aravind Rajeswaran and Praneeth Netrapalli and Thorsten Joachims},
  title         = {MOReL : Model-Based Offline Reinforcement Learning},
  year          = {2020},
  month         = may,
  abstract      = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.},
  archiveprefix = {arXiv},
  eprint        = {2005.05951},
  file          = {:Kidambi2020 - MOReL _ Model Based Offline Reinforcement Learning.pdf:PDF},
  groups        = {Model based, Offline RL},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Yu2020,
  author        = {Tianhe Yu and Garrett Thomas and Lantao Yu and Stefano Ermon and James Zou and Sergey Levine and Chelsea Finn and Tengyu Ma},
  title         = {MOPO: Model-based Offline Policy Optimization},
  year          = {2020},
  month         = may,
  abstract      = {Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. The code is available at https://github.com/tianheyu927/mopo.},
  archiveprefix = {arXiv},
  eprint        = {2005.13239},
  file          = {:Yu2020 - MOPO_ Model Based Offline Policy Optimization.pdf:PDF},
  groups        = {Model based, Offline RL},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Janner2019,
  author        = {Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
  title         = {When to Trust Your Model: Model-Based Policy Optimization},
  year          = {2019},
  month         = jun,
  abstract      = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  archiveprefix = {arXiv},
  eprint        = {1906.08253},
  file          = {:Janner2019 - When to Trust Your Model_ Model Based Policy Optimization.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Rajeswaran2020,
  author        = {Aravind Rajeswaran and Igor Mordatch and Vikash Kumar},
  title         = {A Game Theoretic Framework for Model Based Reinforcement Learning},
  year          = {2020},
  month         = apr,
  abstract      = {Model-based reinforcement learning (MBRL) has recently gained immense interest due to its potential for sample efficiency and ability to incorporate off-policy data. However, designing stable and efficient MBRL algorithms using rich function approximators have remained challenging. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. For algorithm development, we construct a Stackelberg game between the two players, and show that it can be solved with approximate bi-level optimization. This gives rise to two natural families of algorithms for MBRL based on which player is chosen as the leader in the Stackelberg game. Together, they encapsulate, unify, and generalize many previous MBRL algorithms. Furthermore, our framework is consistent with and provides a clear basis for heuristics known to be important in practice from prior works. Finally, through experiments we validate that our proposed algorithms are highly sample efficient, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation.},
  archiveprefix = {arXiv},
  eprint        = {2004.07804},
  file          = {:Rajeswaran2020 - A Game Theoretic Framework for Model Based Reinforcement Learning.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.AI, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Duan2016,
  author        = {Yan Duan and John Schulman and Xi Chen and Peter L. Bartlett and Ilya Sutskever and Pieter Abbeel},
  title         = {RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning},
  year          = {2016},
  month         = nov,
  abstract      = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  archiveprefix = {arXiv},
  eprint        = {1611.02779},
  file          = {:Duan2016 - RL$^2$_ Fast Reinforcement Learning Via Slow Reinforcement Learning.pdf:PDF},
  groups        = {Meta-Learning},
  keywords      = {cs.AI, cs.LG, cs.NE, stat.ML},
  primaryclass  = {cs.AI},
}

@Article{Mnih2013_DQN,
  author        = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  title         = {Playing Atari with Deep Reinforcement Learning},
  year          = {2013},
  month         = dec,
  abstract      = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  eprint        = {1312.5602},
  file          = {:Mnih2013 - Playing Atari with Deep Reinforcement Learning.pdf:PDF:https\://www.davidsilver.uk/wp-content/uploads/2020/03/dqn-1.pdf},
  groups        = {Deep Q-Network, Value based},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Mnih2015_DQN,
  author     = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and Andreas Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  journal    = {Nature 518},
  title      = {Human-level control through deep reinforcement learning},
  year       = {2015},
  number     = {7540},
  pages      = {529--533},
  volume     = {518},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/nature/MnihKSRVBGRFOPB15.bib},
  comment    = {DQN},
  doi        = {10.1038/nature14236},
  file       = {:Mnih2015_DQN - Human Level Control through Deep Reinforcement Learning.pdf:PDF:https\://training.incf.org/sites/default/files/2023-05/Human-level%20control%20through%20deep%20reinforcement%20learning.pdf},
  groups     = {Deep Q-Network, Value based},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@InProceedings{Hasselt2016_DoubleDQN,
  author     = {Hado van Hasselt and Arthur Guez and David Silver},
  booktitle  = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {USA}},
  title      = {Deep Reinforcement Learning with Double Q-Learning},
  year       = {2016},
  editor     = {Dale Schuurmans and Michael P. Wellman},
  pages      = {2094--2100},
  publisher  = {{AAAI} Press},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/aaai/HasseltGS16.bib},
  comment    = {Double DQN},
  file       = {:Hasselt2016.pdf:PDF},
  groups     = {Value based},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389},
}

@InProceedings{Schaul2016_PER,
  author     = {Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  booktitle  = {4th International Conference on Learning Representations},
  title      = {Prioritized Experience Replay},
  year       = {2016},
  editor     = {Yoshua Bengio and Yann LeCun},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/SchaulQAS15.bib},
  groups     = {PER, Value based},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {http://arxiv.org/abs/1511.05952},
}

@InProceedings{Wang2016_DuelingDQN,
  author     = {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado van Hasselt and Marc Lanctot and Nando de Freitas},
  booktitle  = {Proceedings of the 33rd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  title      = {Dueling Network Architectures for Deep Reinforcement Learning},
  year       = {2016},
  editor     = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
  pages      = {1995--2003},
  publisher  = {JMLR.org},
  series     = {{JMLR} Workshop and Conference Proceedings},
  volume     = {48},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/icml/WangSHHLF16.bib},
  file       = {:Wang2016 - Dueling Network Architectures for Deep Reinforcement Learning.pdf:PDF:http\://proceedings.mlr.press/v48/wangf16.pdf},
  groups     = {Dueling DQN, Value based},
  ranking    = {rank5},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v48/wangf16.html},
}

@Article{Ostrovski2017_PseudoCounts,
  author    = {Georg Ostrovski and Marc G. Bellemare and A{\"a}ron van den Oord and R{\'e}mi Munos},
  title     = {Count-Based Exploration with Neural Density Models},
  year      = {2017},
  month     = {06--11 Aug},
  pages     = {2721--2730},
  volume    = {70},
  abstract  = {Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma’s Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.’s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma’s Revenge.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor    = {Precup, Doina and Teh, Yee Whye},
  groups    = {Visitation count},
  pdf       = {http://proceedings.mlr.press/v70/ostrovski17a/ostrovski17a.pdf},
  publisher = {PMLR},
  ranking   = {rank3},
  series    = {Proceedings of Machine Learning Research},
  url       = {https://proceedings.mlr.press/v70/ostrovski17a.html},
}

@Article{Ecoffet2019,
  author        = {Adrien Ecoffet and Joost Huizinga and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  title         = {Go-Explore: a New Approach for Hard-Exploration Problems},
  year          = {2019},
  volume        = {abs/1901.10995},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1901-10995.bib},
  eprint        = {1901.10995},
  file          = {:Ecoffet2019 - Go Explore_ a New Approach for Hard Exploration Problems.pdf:PDF},
  groups        = {Random Exploration, Imitation, Exploration},
  ranking       = {rank3},
  url           = {http://arxiv.org/abs/1901.10995},
}

@InProceedings{Bellemare2017_DistributionalRL,
  author     = {Marc G. Bellemare and Will Dabney and R{\'e}mi Munos},
  booktitle  = {Proceedings of the 34th International Conference on Machine Learning},
  title      = {A Distributional Perspective on Reinforcement Learning},
  year       = {2017},
  address    = {International Convention Centre, Sydney, Australia},
  editor     = {Doina Precup and Yee Whye Teh},
  month      = {06--11 Aug},
  pages      = {449--458},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {70},
  abstract   = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  file       = {:Bellemare2017 - A Distributional Perspective on Reinforcement Learning.pdf:PDF:http\://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
  groups     = {Value based},
  pdf        = {http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
  ranking    = {rank4},
  readstatus = {skimmed},
  url        = {http://proceedings.mlr.press/v70/bellemare17a.html},
}

@InProceedings{Fortunato2018_NoisyDQN,
  author     = {Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
  booktitle  = {International Conference on Learning Representations},
  title      = {Noisy Networks for Exploration},
  year       = {2018},
  abstract   = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
  file       = {:- Noisy Networks for Exploration.pdf:PDF},
  groups     = {Value based},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=rywHCPkAW},
}

@InProceedings{FletBerliac2021_AGAC,
  author     = {Flet-Berliac, Yannis and Ferret, Johan and Pietquin, Olivier and Preux, Philippe and Geist, Matthieu},
  booktitle  = {9th International Conference on Learning Representations},
  title      = {Adversarially Guided Actor-Critic},
  year       = {2021},
  address    = {Vienna / Virtual, Austria},
  month      = May,
  abstract   = {Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks.},
  comment    = {AGAC},
  file       = {:FletBerliac2021 - Adversarially Guided Actor Critic.pdf:PDF},
  groups     = {Adversary Guidance},
  pdf        = {https://hal.inria.fr/hal-03167169/file/AGAC.pdf},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://hal.inria.fr/hal-03167169},
}

@Article{Yu2021,
  author        = {Tianhe Yu and Aviral Kumar and Rafael Rafailov and Aravind Rajeswaran and Sergey Levine and Chelsea Finn},
  title         = {COMBO: Conservative Offline Model-Based Policy Optimization},
  year          = {2021},
  month         = feb,
  abstract      = {Model-based algorithms, which learn a dynamics model from logged experience and perform some sort of pessimistic planning under the learned model, have emerged as a promising paradigm for offline reinforcement learning (offline RL). However, practical variants of such model-based algorithms rely on explicit uncertainty quantification for incorporating pessimism. Uncertainty estimation with complex models, such as deep neural networks, can be difficult and unreliable. We overcome this limitation by developing a new model-based offline RL algorithm, COMBO, that regularizes the value function on out-of-support state-action tuples generated via rollouts under the learned model. This results in a conservative estimate of the value function for out-of-support state-action tuples, without requiring explicit uncertainty estimation. We theoretically show that our method optimizes a lower bound on the true policy value, that this bound is tighter than that of prior methods, and our approach satisfies a policy improvement guarantee in the offline setting. Through experiments, we find that COMBO consistently performs as well or better as compared to prior offline model-free and model-based methods on widely studied offline RL benchmarks, including image-based tasks.},
  archiveprefix = {arXiv},
  eprint        = {2102.08363},
  file          = {:Yu2021 - COMBO_ Conservative Offline Model Based Policy Optimization.pdf:PDF},
  groups        = {Offline RL},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
}

@Article{Kumar2020,
  author        = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
  title         = {Conservative Q-Learning for Offline Reinforcement Learning},
  year          = {2020},
  month         = jun,
  abstract      = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  archiveprefix = {arXiv},
  eprint        = {2006.04779},
  file          = {:Kumar2020 - Conservative Q Learning for Offline Reinforcement Learning.pdf:PDF},
  groups        = {Offline RL},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Silver2016,
  author    = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  journal   = {Nature 529},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  year      = {2016},
  number    = {7587},
  pages     = {484--489},
  volume    = {529},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  comment   = {AlphaGo},
  doi       = {10.1038/nature16961},
  file      = {:Silver2016 - Mastering the Game of Go with Deep Neural Networks and Tree Search.pdf:PDF},
  groups    = {Model based},
  ranking   = {rank5},
}

@Article{Silver2017,
  author    = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy P. Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  journal   = {Nature 550},
  title     = {Mastering the game of Go without human knowledge},
  year      = {2017},
  number    = {7676},
  pages     = {354--359},
  volume    = {550},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/SilverSSAHGHBLB17.bib},
  comment   = {AlphaGo Zero},
  doi       = {10.1038/nature24270},
  file      = {:Silver2017 - Mastering the Game of Go without Human Knowledge.pdf:PDF},
  groups    = {Model based},
  ranking   = {rank5},
}

@Article{Silver2017a,
  author        = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy P. Lillicrap and Karen Simonyan and Demis Hassabis},
  title         = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  year          = {2017},
  volume        = {abs/1712.01815},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  comment       = {AlphaZero},
  eprint        = {1712.01815},
  file          = {:Silver2017a - Mastering Chess and Shogi by Self Play with a General Reinforcement Learning Algorithm.pdf:PDF},
  groups        = {Model based},
  ranking       = {rank5},
  url           = {http://arxiv.org/abs/1712.01815},
}

@Article{Schulman2016_GAE,
  author    = {John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
  title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  year      = {2016},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanMLJA15.bib},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  comment   = {General Advantage Estimation},
  editor    = {Yoshua Bengio and Yann LeCun},
  file      = {:Schulman2015a - High Dimensional Continuous Control Using Generalized Advantage Estimation.pdf:PDF},
  groups    = {RL, TRPO-PPO},
  ranking   = {rank5},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  url       = {http://arxiv.org/abs/1506.02438},
}

@Article{Heess2017,
  author        = {Nicolas Heess and Dhruva TB and Srinivasan Sriram and Jay Lemmon and Josh Merel and Greg Wayne and Yuval Tassa and Tom Erez and Ziyu Wang and S. M. Ali Eslami and Martin Riedmiller and David Silver},
  title         = {Emergence of Locomotion Behaviours in Rich Environments},
  year          = {2017},
  month         = jul,
  abstract      = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx_bgoTF7bs .},
  archiveprefix = {arXiv},
  eprint        = {1707.02286},
  file          = {:Heess2017 - Emergence of Locomotion Behaviours in Rich Environments.pdf:PDF},
  groups        = {TRPO-PPO},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Leibo2019,
  author        = {Joel Z. Leibo and Edward Hughes and Marc Lanctot and Thore Graepel},
  title         = {Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research},
  year          = {2019},
  month         = mar,
  abstract      = {Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.},
  archiveprefix = {arXiv},
  eprint        = {1903.00742},
  file          = {:Leibo2019 - Autocurricula and the Emergence of Innovation from Social Interaction_ a Manifesto for Multi Agent Intelligence Research.pdf:PDF},
  groups        = {Multi-agent Systems},
  keywords      = {cs.AI, cs.GT, cs.MA, cs.NE, q-bio.NC},
  primaryclass  = {cs.AI},
}

@Article{Baker2019,
  author        = {Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
  title         = {Emergent Tool Use From Multi-Agent Autocurricula},
  year          = {2019},
  month         = sep,
  abstract      = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
  archiveprefix = {arXiv},
  eprint        = {1909.07528},
  file          = {:Baker2019 - Emergent Tool Use from Multi Agent Autocurricula.pdf:PDF},
  groups        = {Multi-agent RL, Multi-agent Systems, Multi-agent learning},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
}

@Article{Bansal2017,
  author        = {Trapit Bansal and Jakub Pachocki and Szymon Sidor and Ilya Sutskever and Igor Mordatch},
  journal       = {ICLR 2018},
  title         = {Emergent Complexity via Multi-Agent Competition},
  year          = {2017},
  month         = oct,
  abstract      = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
  archiveprefix = {arXiv},
  eprint        = {1710.03748},
  file          = {:Bansal2017 - Emergent Complexity Via Multi Agent Competition.pdf:PDF},
  groups        = {Multi-agent Systems, Multi-agent learning},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Jaderberg2018_FTW,
  author        = {Max Jaderberg and Wojciech M. Czarnecki and Iain Dunning and Luke Marris and Guy Lever and Antonio Garcia Castaneda and Charles Beattie and Neil C. Rabinowitz and Ari S. Morcos and Avraham Ruderman and Nicolas Sonnerat and Tim Green and Louise Deason and Joel Z. Leibo and David Silver and Demis Hassabis and Koray Kavukcuoglu and Thore Graepel},
  journal       = {Science},
  title         = {Human-level performance in first-person multiplayer games with population-based deep reinforcement learning},
  year          = {2019},
  month         = may,
  number        = {6443},
  pages         = {859-865},
  volume        = {364},
  abstract      = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
  archiveprefix = {arXiv},
  doi           = {10.1126/science.aau6249},
  file          = {:Jaderberg2018 - Human Level Performance in First Person Multiplayer Games with Population Based Deep Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL, Population-based training, Independent Learning},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  ranking       = {rank4},
}

@Article{Liu2019,
  author        = {Siqi Liu and Guy Lever and Josh Merel and Saran Tunyasuvunakool and Nicolas Heess and Thore Graepel},
  title         = {Emergent Coordination Through Competition},
  year          = {2019},
  month         = feb,
  abstract      = {We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics. We demonstrate that decentralized, population-based training with co-play can lead to a progression in agents' behaviors: from random, to simple ball chasing, and finally showing evidence of cooperation. Our study highlights several of the challenges encountered in large scale multi-agent training in continuous control. In particular, we demonstrate that the automatic optimization of simple shaping rewards, not themselves conducive to co-operative behavior, can lead to long-horizon team behavior. We further apply an evaluation scheme, grounded by game theoretic principals, that can assess agent performance in the absence of pre-defined evaluation tasks or human baselines.},
  archiveprefix = {arXiv},
  eprint        = {1902.07151},
  file          = {:Liu2019 - Emergent Coordination through Competition.pdf:PDF},
  groups        = {Multi-agent RL, Multi-agent Systems},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Miikkulainen2004,
  author        = {R. Miikkulainen and K. O. Stanley},
  journal       = {Journal Of Artificial Intelligence Research, Volume 21, pages 63-100, 2004},
  title         = {Competitive Coevolution through Evolutionary Complexification},
  year          = {2004},
  abstract      = {Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure, complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.},
  archiveprefix = {arXiv},
  comment       = {following NEAT},
  doi           = {10.1613/jair.1338},
  eprint        = {1107.0037},
  file          = {:Miikkulainen2011 - Competitive Coevolution through Evolutionary Complexification.pdf:PDF},
  groups        = {Multi-agent learning},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Rosin1997,
  author = {Christopher D. Rosin and Richard K. Belew},
  title  = {New Methods for Competitive Coevolution},
  year   = {1997},
  issn   = {1063-6560},
  pages  = {1-29},
  volume = {5},
  doi    = {10.1162/evco.1997.5.1.1},
  groups = {Multi-agent learning},
}

@InProceedings{Sukhbaatar2016_CommNet,
  author     = {Sainbayar Sukhbaatar and Arthur Szlam and Rob Fergus},
  booktitle  = {Advances in Neural Information Processing Systems, pp. 2244–2252},
  title      = {Learning Multiagent Communication with Backpropagation},
  year       = {2016},
  abstract   = {Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.},
  comment    = {CommNet},
  file       = {:Sukhbaatar2016 - Learning Multiagent Communication with Backpropagation.pdf:PDF},
  groups     = {Communication},
  ranking    = {rank4},
  readstatus = {read},
}

@InProceedings{Mordatch2018_GroundedCompo,
  author     = {Igor Mordatch and Pieter Abbeel},
  booktitle  = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence},
  title      = {Emergence of Grounded Compositional Language in Multi-Agent Populations},
  year       = {2018},
  editor     = {Sheila A. McIlraith and Kilian Q. Weinberger},
  pages      = {1495--1502},
  publisher  = {{AAAI} Press},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/aaai/MordatchA18.bib},
  comment    = {Multi-agent particle environment},
  file       = {:Mordatch2017 - Emergence of Grounded Compositional Language in Multi Agent Populations.pdf:PDF},
  groups     = {Discrete language, Communication, Multi-agent Systems},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17007},
}

@InProceedings{Jaques2019_SocialInfluence,
  author        = {Natasha Jaques and Angeliki Lazaridou and Edward Hughes and Caglar Gulcehre and Pedro A. Ortega and DJ Strouse and Joel Z. Leibo and Nando de Freitas},
  booktitle     = {Proceedings of the 36th International Conference on Machine Learning},
  title         = {Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning},
  year          = {2019},
  pages         = {3040--3049},
  abstract      = {We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.},
  archiveprefix = {arXiv},
  eprint        = {1810.08647},
  file          = {:Jaques2018 - Social Influence As Intrinsic Motivation for Multi Agent Deep Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL, Social Learning, Intrinsic rewards in MARL, Communication},
  primaryclass  = {cs.LG},
  ranking       = {rank3},
  readstatus    = {read},
}

@Article{Leibo2019,
  author        = {Joel Z. Leibo and Julien Perolat and Edward Hughes and Steven Wheelwright and Adam H. Marblestone and Edgar Duéñez-Guzmán and Peter Sunehag and Iain Dunning and Thore Graepel},
  journal       = {Proceedings of the 18th International Conference on Autonomous Agents and Multi-Agent Systems, pp. 1099–1107.},
  title         = {Malthusian Reinforcement Learning},
  year          = {2019},
  month         = dec,
  abstract      = {Here we explore a new algorithmic framework for multi-agent reinforcement learning, called Malthusian reinforcement learning, which extends self-play to include fitness-linked population size dynamics that drive ongoing innovation. In Malthusian RL, increases in a subpopulation's average return drive subsequent increases in its size, just as Thomas Malthus argued in 1798 was the relationship between preindustrial income levels and population growth. Malthusian reinforcement learning harnesses the competitive pressures arising from growing and shrinking population size to drive agents to explore regions of state and policy spaces that they could not otherwise reach. Furthermore, in environments where there are potential gains from specialization and division of labor, we show that Malthusian reinforcement learning is better positioned to take advantage of such synergies than algorithms based on self-play.},
  archiveprefix = {arXiv},
  eprint        = {1812.07019},
  file          = {:Leibo2018 - Malthusian Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.NE, cs.MA, q-bio.PE},
  primaryclass  = {cs.NE},
}

@InProceedings{Mnih2016_A3C,
  author     = {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle  = {Proceedings of The 33rd International Conference on Machine Learning},
  title      = {Asynchronous Methods for Deep Reinforcement Learning},
  year       = {2016},
  address    = {New York, New York, USA},
  editor     = {Maria Florina Balcan and Kilian Q. Weinberger},
  month      = {20--22 Jun},
  pages      = {1928--1937},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {48},
  abstract   = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  comment    = {A3C},
  file       = {:pmlr-v48-mniha16 - Asynchronous Methods for Deep Reinforcement Learning.pdf:PDF},
  groups     = {Actor Critic, Policy based},
  pdf        = {http://proceedings.mlr.press/v48/mniha16.pdf},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {http://proceedings.mlr.press/v48/mniha16.html},
}

@InProceedings{10.5555/3091125.3091194,
  author    = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Multi-Agent Reinforcement Learning in Sequential Social Dilemmas},
  year      = {2017},
  address   = {Richland, SC},
  pages     = {464–473},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '17},
  abstract  = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
  file      = {:10.5555_3091125.3091194 - Multi Agent Reinforcement Learning in Sequential Social Dilemmas.pdf:PDF},
  groups    = {Multi-agent RL},
  keywords  = {agent-based social simulation, cooperation, markov games, non-cooperative games, social dilemmas},
  location  = {S\~{a}o Paulo, Brazil},
  numpages  = {10},
  ranking   = {rank2},
  url       = {https://arxiv.org/pdf/1702.03037.pdf},
}

@Article{Shoham2007_MAL,
  author     = {Yoav Shoham and Rob Powers and Trond Grenager},
  journal    = {Artificial Intelligence},
  title      = {If multi-agent learning is the answer, what is the question?},
  year       = {2007},
  issn       = {0004-3702},
  note       = {Foundations of Multi-Agent Learning},
  number     = {7},
  pages      = {365-377},
  volume     = {171},
  abstract   = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.11This article has a long history and owes many debts. A first version was presented at the NIPS workshop, Multi-Agent Learning: Theory and Practice, in 2002. A later version was presented at the AAAI Fall Symposium in 2004 [Y. Shoham, R. Powers, T. Grenager, On the agenda(s) of research on multi-agent learning, in: AAAI 2004 Symposium on Artificial Multi-Agent Learning (FS-04-02), AAAI Press, 2004]. Over time it has gradually evolved into the current form, as a result of our own work in the area as well as the feedback of many colleagues. We thank them all collectively, with special thanks to members of the multi-agent group at Stanford in the past three years. Rakesh Vohra and Michael Wellman provided detailed comments on the latest draft which resulted in substantive improvements, although we alone are responsible for the views put forward. This work was supported by NSF ITR grant IIS-0205633 and DARPA grant HR0011-05-1.},
  doi        = {https://doi.org/10.1016/j.artint.2006.02.006},
  file       = {:SHOHAM2007365 - If Multi Agent Learning Is the Answer, What Is the Question_ (2).pdf:PDF},
  groups     = {MAS Reviews, Multi-agent Systems},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://www.sciencedirect.com/science/article/pii/S0004370207000495},
}

@Article{Ecoffet2021,
  author        = {Adrien Ecoffet and Joost Huizinga and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  journal       = {Nature},
  title         = {First return then explore},
  year          = {2021},
  month         = feb,
  number        = {590},
  pages         = {580–586},
  abstract      = {The promise of reinforcement learning is to solve complex sequential decision problems by specifying a high-level reward function only. However, RL algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but despite substantial investments by the community, creating algorithms that can do so remains one of the central challenges of the field. We hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states ("detachment") and from failing to first return to a state before exploring from it ("derailment"). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before exploring. Go-Explore solves all heretofore unsolved Atari games (those for which algorithms could not previously outperform humans when evaluated following current community standards) and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a challenging and extremely sparse-reward robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The striking contrast between the substantial performance gains from Go-Explore and the simplicity of its mechanisms suggests that remembering promising states, returning to them, and exploring from them is a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.},
  archiveprefix = {arXiv},
  doi           = {https://doi.org/10.1038/s41586-020-03157-9},
  eprint        = {2004.12919},
  file          = {:Ecoffet2020 - First Return Then Explore.pdf:PDF},
  groups        = {Imitation},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank3},
}

@Article{Hester2018_DQNfDemo,
  author       = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and Dulac-Arnold, Gabriel and Agapiou, John and Leibo, Joel and Gruslys, Audrunas},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Deep Q-learning From Demonstrations},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator’s actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD’s performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN. &lt;/p&gt;},
  file         = {:Hester_Vecerik_Pietquin_Lanctot_Schaul_Piot_Horgan_Quan_Sendonaris_Osband_Dulac-Arnold_Agapiou_Leibo_Gruslys_2018 - Deep Q Learning from Demonstrations.pdf:PDF},
  groups       = {Imitation},
  ranking      = {rank4},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11757},
}

@Article{Panait2005,
  author     = {Liviu Panait and Sean Luke},
  journal    = {Autonomous Agents and Multi-Agent Systems},
  title      = {Cooperative Multi-Agent Learning: The State of the Art},
  year       = {2005},
  month      = {nov},
  number     = {3},
  pages      = {387--434},
  volume     = {11},
  doi        = {10.1007/s10458-005-2631-2},
  file       = {:Panait2005 - Cooperative Multi Agent Learning_ the State of the Art.pdf:PDF},
  groups     = {MAS Reviews},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{Lowe2017_MADDPG,
  author     = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  year       = {2017},
  volume     = {30},
  abstract   = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  comment    = {MADDPG},
  file       = {:10.5555_3295222.3295385 - Multi Agent Actor Critic for Mixed Cooperative Competitive Environments.pdf:PDF},
  groups     = {Multi-agent RL},
  location   = {Long Beach, California, USA},
  numpages   = {12},
  ranking    = {rank5},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.5555/3295222.3295385},
}

@InBook{Wolpert2002,
  author    = {David H. Wolpert and Kagan Tumer},
  pages     = {355-369},
  title     = {Optimal Payoff Functions for Members of Collectives},
  year      = {2002},
  abstract  = {Abstract We consider the problem of designing (perhaps massively distributed) collectives of computational processes to maximize a provided “world utility” function. We consider this problem when the behavior of each process in the collective can be cast as striving to maximize its own payoff utility function. For such cases the central design issue is how to initialize/update those payoff utility functions of the individual processes so as to induce behavior of the entire collective having good values of the world utility. Traditional “team game” approaches to this problem simply assign to each process the world utility as its payoff utility function. In previous work we used the “Collective Intelligence” (COIN) framework to derive a better choice of payoff utility functions, one that results in world utility performance up to orders of magnitude superior to that ensuing from the use of the team game utility. In this paper, we extend these results using a novel mathematical framework. Under that new framework we review the derivation of the general class of payoff utility functions that both (i) are easy for the individual processes to try to maximize, and (ii) have the property that if good values of them are achieved, then we are assured a high value of world utility. These are the “Aristocrat Utility” and a new variant of the “Wonderful Life Utility” that was introduced in the previous COIN work. We demonstrate experimentally that using these new utility functions can result in significantly improved performance over that of previously investigated COIN payoff utilities, over and above those previous utilities’ superiority to the conventional team game utility. These results also illustrate the substantial superiority of these payoff functions to perhaps the most natural version of the economics technique of “endogenizing externalities.”},
  booktitle = {Modeling Complexity in Economic and Social Systems},
  doi       = {10.1142/9789812777263_0020},
  eprint    = {https://www.worldscientific.com/doi/pdf/10.1142/9789812777263_0020},
  file      = {:doi_10.1142_9789812777263_0020 - Optimal Payoff Functions for Members of Collectives.pdf:PDF},
  groups    = {Wonderful Life Utility, Multi-agent learning},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/9789812777263_0020},
}

@Article{Michalak2014,
  author        = {Tomasz Pawel Michalak and Karthik V Aadithya and Piotr L. Szczepanski and Balaraman Ravindran and Nicholas R. Jennings},
  journal       = {Journal Of Artificial Intelligence Research, Volume 46, pages 607-650, 2013},
  title         = {Efficient Computation of the Shapley Value for Game-Theoretic Network Centrality},
  year          = {2014},
  month         = feb,
  abstract      = {The Shapley value---probably the most important normative payoff division scheme in coalitional games---has recently been advocated as a useful measure of centrality in networks. However, although this approach has a variety of real-world applications (including social and organisational networks, biological networks and communication networks), its computational properties have not been widely studied. To date, the only practicable approach to compute Shapley value-based centrality has been via Monte Carlo simulations which are computationally expensive and not guaranteed to give an exact answer. Against this background, this paper presents the first study of the computational aspects of the Shapley value for network centralities. Specifically, we develop exact analytical formulae for Shapley value-based centrality in both weighted and unweighted networks and develop efficient (polynomial time) and exact algorithms based on them. We empirically evaluate these algorithms on two real-life examples (an infrastructure network representing the topology of the Western States Power Grid and a collaboration network from the field of astrophysics) and demonstrate that they deliver significant speedups over the Monte Carlo approach. For instance, in the case of unweighted networks our algorithms are able to return the exact solution about 1600 times faster than the Monte Carlo approximation, even if we allow for a generous 10% error margin for the latter method.},
  archiveprefix = {arXiv},
  doi           = {10.1613/jair.3806},
  eprint        = {1402.0567},
  file          = {:Michalak2014 - Efficient Computation of the Shapley Value for Game Theoretic Network Centrality (2).pdf:PDF},
  groups        = {Shapley value},
  keywords      = {cs.GT},
  primaryclass  = {cs.GT},
}

@Article{Anshelevich2008,
  author     = {Elliot Anshelevich and Anirban Dasgupta and Jon Kleinberg and {\'{E}}va Tardos and Tom Wexler and Tim Roughgarden},
  journal    = {{SIAM} Journal on Computing},
  title      = {The Price of Stability for Network Design with Fair Cost Allocation},
  year       = {2008},
  month      = {jan},
  number     = {4},
  pages      = {1602--1623},
  volume     = {38},
  comment    = {Use Shapley value in potential games for allocating network cost to users.},
  doi        = {10.1137/070680096},
  file       = {:Anshelevich2008 - The Price of Stability for Network Design with Fair Cost Allocation.pdf:PDF},
  groups     = {Shapley value},
  publisher  = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  readstatus = {skimmed},
}

@Article{Shapley1953,
  author    = {Shapley, L. S.},
  journal   = {Contributions to the Theory of Games},
  title     = {A value for n-person games},
  year      = {1953},
  number    = {28},
  pages     = {307-317},
  volume    = {II},
  file      = {:10013542751 - A Value for N Person Games.pdf:PDF},
  groups    = {Shapley value},
  publisher = {Princeton University Press},
  url       = {https://ci.nii.ac.jp/naid/10013542751/en/},
}

@Article{Wang2020a,
  author     = {Jianhong Wang and Yuan Zhang and Tae-Kyun Kim and Yunjie Gu},
  journal    = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title      = {Shapley Q-Value: A Local Reward Approach to Solve Global Reward Games},
  year       = {2020},
  month      = {apr},
  number     = {05},
  pages      = {7285--7292},
  volume     = {34},
  doi        = {10.1609/aaai.v34i05.6220},
  file       = {:Wang2020 - Shapley Q Value_ a Local Reward Approach to Solve Global Reward Games.pdf:PDF},
  groups     = {Shapley value},
  publisher  = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  readstatus = {skimmed},
}

@InProceedings{Kim2018_SchedNet,
  author     = {Daewoo Kim and Sangwoo Moon and David Hostallero and Wan Ju Kang and Taeyoung Lee and Kyunghwan Son and Yung Yi},
  booktitle  = {International Conference on Learning Representations},
  title      = {Learning to Schedule Communication in Multi-agent Reinforcement Learning},
  year       = {2019},
  comment    = {SchedNet},
  file       = {:kim2018learning - Learning to Schedule Communication in Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Communication},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=SJxu5iR9KQ},
}

@InProceedings{Das2019_TarMAC,
  author     = {Das, Abhishek and Gervet, Th{\'e}ophile and Romoff, Joshua and Batra, Dhruv and Parikh, Devi and Rabbat, Mike and Pineau, Joelle},
  booktitle  = {Proceedings of the 36th International Conference on Machine Learning},
  title      = {{T}ar{MAC}: Targeted Multi-Agent Communication},
  year       = {2019},
  editor     = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  month      = {09--15 Jun},
  pages      = {1538--1546},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {97},
  abstract   = {We propose a targeted communication architecture for multi-agent reinforcement learning, where agents learn both <em>what</em> messages to send and <em>whom</em> to address them to while performing cooperative tasks in partially-observable environments. This targeting behavior is learnt solely from downstream task-specific reward without any communication supervision. We additionally augment this with a multi-round communication approach where agents coordinate via multiple rounds of communication before taking actions in the environment. We evaluate our approach on a diverse set of cooperative multi-agent tasks, of varying difficulties, with varying number of agents, in a variety of environments ranging from 2D grid layouts of shapes and simulated traffic junctions to 3D indoor environments, and demonstrate the benefits of targeted and multi-round communication. Moreover, we show that the targeted communication strategies learned by agents are interpretable and intuitive. Finally, we show that our architecture can be easily extended to mixed and competitive environments, leading to improved performance and sample complexity over recent state-of-the-art approaches.},
  file       = {:Das2019 - TarMAC_ Targeted Multi Agent Communication.pdf:PDF},
  groups     = {Communication},
  pdf        = {http://proceedings.mlr.press/v97/das19a/das19a.pdf},
  ranking    = {rank3},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v97/das19a.html},
}

@InProceedings{Iqbal2019,
  author    = {Iqbal, Shariq and Sha, Fei},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  month     = {09--15 Jun},
  pages     = {2961--2970},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.},
  file      = {:Iqbal2019 - Actor Attention Critic for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  priority  = {prio2},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v97/iqbal19a.html},
}

@InProceedings{Lanctot2017,
  author    = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audr\={u}nas and Lazaridou, Angeliki and Tuyls, Karl and P\'{e}rolat, Julien and Silver, David and Graepel, Thore},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {4193–4206},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'17},
  abstract  = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
  comment   = {PSRO},
  file      = {:Lanctot2017 - A Unified Game Theoretic Approach to Multiagent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {14},
  ranking   = {rank3},
  url       = {https://dl.acm.org/doi/pdf/10.5555/3294996.3295174},
}

@Article{Eccles2019,
  author     = {Tom Eccles and Yoram Bachrach and Guy Lever and Angeliki Lazaridou and Thore Graepel},
  journal    = {Advances in Neural Information Processing Systems},
  title      = {Biases for Emergent Communication in Multi-agent Reinforcement Learning},
  year       = {2019},
  abstract   = {We study the problem of emergent communication, in which language arises because speakers and listeners must communicate information in order to solve tasks. In temporally extended reinforcement learning domains, it has proved hard to learn such communication without centralized training of agents, due in part to a difficult joint exploration problem. We introduce inductive biases for positive signalling and positive listening, which ease this problem. In a simple one-step environment, we demonstrate how these biases ease the learning problem. We also apply our methods to a more extended environment, showing that agents with these inductive biases achieve better performance, and analyse the resulting communication protocols.},
  file       = {:Eccles2019 - Biases for Emergent Communication in Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Communication},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2019/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html},
}

@Article{BACHRACH2020103356,
  author   = {Yoram Bachrach and Richard Everett and Edward Hughes and Angeliki Lazaridou and Joel Z. Leibo and Marc Lanctot and Michael Johanson and Wojciech M. Czarnecki and Thore Graepel},
  journal  = {Artificial Intelligence},
  title    = {Negotiating team formation using deep reinforcement learning},
  year     = {2020},
  issn     = {0004-3702},
  pages    = {103356},
  volume   = {288},
  abstract = {When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory. Additionally, we investigate how the physical location of agents influences negotiation outcomes.},
  doi      = {https://doi.org/10.1016/j.artint.2020.103356},
  file     = {:BACHRACH2020103356 - Negotiating Team Formation Using Deep Reinforcement Learning.pdf:PDF},
  groups   = {Shapley value},
  keywords = {Multi-agent systems, Team formation, Coalition formation, Reinforcement learning, Deep learning, Cooperative games, Shapley value},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370220301077},
}

@Book{Wooldridge2009_IntroMAS,
  author     = {Wooldridge, Michael},
  publisher  = {John wiley \& sons},
  title      = {An introduction to multiagent systems},
  year       = {2009},
  file       = {:An Introduction to MultiAgent Systems ( PDFDrive ).pdf:PDF},
  groups     = {Multi-agent Systems, MAS Reviews},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://books.google.fr/books?hl=en&lr=&id=X3ZQ7yeDn2IC&oi=fnd&pg=PR13&ots=WHohqr4qa_&sig=0NufQwkvOFQvIdmAjHTz0F27huo&redir_esc=y#v=onepage&q&f=false},
}

@TechReport{wolpert99a,
  author      = {David H. Wolpert and Kagan Tumer},
  institution = {NASA},
  title       = {An Introduction to Collective Intelligence},
  year        = {1999},
  note        = {NASA-ARC-IC-99-63},
  abstract    = {This paper surveys the emerging science of how to
                  design a ``COllective INtelligence'' (COIN). A COIN
                  is a large multi-agent system where: (i) There is
                  little to no centralized communication or control;
                  and (ii) There is a provided world utility function
                  that rates the possible histories of the full
                  system. In particular, we are interested in COINs in
                  which each agent runs a reinforcement learning (RL)
                  algorithm. Rather than use a conventional modeling
                  approach (e.g., model the system dynamics, and
                  hand-tune agents to cooperate), we aim to solve the
                  COIN design problem implicitly, via the ``adaptive''
                  character of the RL algorithms of each of the
                  agents. This approach introduces an entirely new,
                  profound design problem: Assuming the RL algorithms
                  are able to achieve high rewards, what reward
                  functions for the individual agents will, when
                  pursued by those agents, result in high world
                  utility? In other words, what reward functions will
                  best ensure that we do not have phenomena like the
                  tragedy of the commons, Braess's paradox, or the
                  liquidity trap? Although still very young, research
                  specifically concentrating on the COIN design
                  problem has already resulted in successes in
                  artificial domains, in particular in packet-routing,
                  the leader-follower problem, and in variants of
                  Arthur's El Farol bar problem. It is expected that
                  as it matures and draws upon other disciplines
                  related to COINs, this research will greatly expand
                  the range of tasks addressable by human
                  engineers. Moreover, in addition to drawing on them,
                  such a fully developed science of COIN design may
                  provide much insight into other already established
                  scientific fields, such as economics, game theory,
                  and population biology},
  arxiv       = {cs.LG/9908014},
  comment     = {Shows how to set the agents' reward functions (using the "wonderful life" reward) so that the global utility is maximized. Extensive related work section.},
  file        = {:wolpert99a - An Introduction to Collective Intelligence.pdf:PDF},
  googleid    = {89pRsJDcBLUJ:scholar.google.com/},
  groups      = {Wonderful Life Utility},
  keywords    = {multiagent learning},
  url         = {http://jmvidal.cse.sc.edu/library/wolpert99a.pdf},
}

@Article{HernandezLeal2019,
  author    = {Pablo Hernandez-Leal and Bilal Kartal and Matthew E. Taylor},
  journal   = {Autonomous Agents and Multi-Agent Systems},
  title     = {A survey and critique of multiagent deep reinforcement learning},
  year      = {2019},
  month     = {oct},
  number    = {6},
  pages     = {750--797},
  volume    = {33},
  doi       = {10.1007/s10458-019-09421-1},
  file      = {:HernandezLeal2019 - A Survey and Critique of Multiagent Deep Reinforcement Learning.pdf:PDF},
  groups    = {MAS Reviews},
  publisher = {Springer Science and Business Media {LLC}},
  ranking   = {rank3},
}

@Misc{Vidal07,
  author = {José M Vidal},
  title  = {Fundamentals of Multiagent Systems},
  year   = {2007},
  file   = {:Vidal06 - Fundamentals of Multiagent Systems.pdf:PDF},
  groups = {MAS Reviews},
}

@InBook{Nowé2012_GameTheory,
  author     = {Now{\'e}, Ann and Vrancx, Peter and De Hauwere, Yann-Micha{\"e}l},
  editor     = {Wiering, Marco and van Otterlo, Martijn},
  pages      = {441--470},
  publisher  = {Springer Berlin Heidelberg},
  title      = {Game Theory and Multi-agent Reinforcement Learning},
  year       = {2012},
  address    = {Berlin, Heidelberg},
  isbn       = {978-3-642-27645-3},
  abstract   = {Reinforcement Learning was originally developed for Markov Decision Processes (MDPs). It allows a single agent to learn a policy that maximizes a possibly delayed reward signal in a stochastic stationary environment. It guarantees convergence to the optimal policy, provided that the agent can sufficiently experiment and the environment in which it is operating is Markovian. However, when multiple agents apply reinforcement learning in a shared environment, this might be beyond the MDP model. In such systems, the optimal policy of an agent depends not only on the environment, but on the policies of the other agents as well. These situations arise naturally in a variety of domains, such as: robotics, telecommunications, economics, distributed control, auctions, traffic light control, etc. In these domains multi-agent learning is used, either because of the complexity of the domain or because control is inherently decentralized. In such systems it is important that agents are capable of discovering good solutions to the problem at hand either by coordinating with other learners or by competing with them. This chapter focuses on the application reinforcement learning techniques in multi-agent systems. We describe a basic learning framework based on the economic research into game theory, and illustrate the additional complexity that arises in such systems. We also described a representative selection of algorithms for the different areas of multi-agent reinforcement learning research.},
  booktitle  = {Reinforcement Learning: State-of-the-Art},
  doi        = {10.1007/978-3-642-27645-3_14},
  file       = {:Nowé2012 - Game Theory and Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {MAS Reviews, Game theory},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://doi.org/10.1007/978-3-642-27645-3_14},
}

@Article{6303906,
  author   = {Y. {Cao} and W. {Yu} and W. {Ren} and G. {Chen}},
  journal  = {IEEE Transactions on Industrial Informatics},
  title    = {An Overview of Recent Progress in the Study of Distributed Multi-Agent Coordination},
  year     = {2013},
  issn     = {1941-0050},
  month    = {Feb},
  number   = {1},
  pages    = {427-438},
  volume   = {9},
  abstract = {This paper reviews some main results and progress in distributed multi-agent coordination, focusing on papers published in major control systems and robotics journals since 2006. Distributed coordination of multiple vehicles, including unmanned aerial vehicles, unmanned ground vehicles, and unmanned underwater vehicles, has been a very active research subject studied extensively by the systems and control community. The recent results in this area are categorized into several directions, such as consensus, formation control, optimization, and estimation. After the review, a short discussion section is included to summarize the existing research and to propose several promising research directions along with some open problems that are deemed important for further investigations.},
  doi      = {10.1109/TII.2012.2219061},
  file     = {:6303906 - An Overview of Recent Progress in the Study of Distributed Multi Agent Coordination.pdf:PDF},
  groups   = {MAS Reviews},
  keywords = {distributed control;multi-robot systems;distributed multiagent coordination;control systems;robotics journals;distributed coordination;multiple vehicles;unmanned aerial vehicles;unmanned ground vehicles;unmanned underwater vehicles;systems and control community;Delay effects;Network topology;Heuristic algorithms;Vehicle dynamics;Vehicles;Algorithm design and analysis;Delay;Distributed coordination;formation control;multi-agent system;sensor network},
  ranking  = {rank5},
}

@Article{Oliehoek2008,
  author    = {F. A. Oliehoek and M. T. J. Spaan and N. Vlassis},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {Optimal and Approximate Q-value Functions for Decentralized {POMDPs}},
  year      = {2008},
  month     = {may},
  pages     = {289--353},
  volume    = {32},
  doi       = {10.1613/jair.2447},
  file      = {:Oliehoek2008 - Optimal and Approximate Q Value Functions for Decentralized POMDPs.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {{AI} Access Foundation},
  ranking   = {rank2},
}

@Article{Kraemer2016,
  author   = {Landon Kraemer and Bikramjit Banerjee},
  journal  = {Neurocomputing},
  title    = {Multi-agent reinforcement learning as a rehearsal for decentralized planning},
  year     = {2016},
  issn     = {0925-2312},
  pages    = {82-94},
  volume   = {190},
  abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are a powerful tool for modeling multi-agent planning and decision-making under uncertainty. Prevalent Dec-POMDP solution techniques require centralized computation given full knowledge of the underlying model. Multi-agent reinforcement learning (MARL) based approaches have been recently proposed for distributed solution of Dec-POMDPs without full prior knowledge of the model, but these methods assume that conditions during learning and policy execution are identical. In some practical scenarios this may not be the case. We propose a novel MARL approach in which agents are allowed to rehearse with information that will not be available during policy execution. The key is for the agents to learn policies that do not explicitly rely on these rehearsal features. We also establish a weak convergence result for our algorithm, RLaR, demonstrating that RLaR converges in probability when certain conditions are met. We show experimentally that incorporating rehearsal features can enhance the learning rate compared to non-rehearsal-based learners, and demonstrate fast, (near) optimal performance on many existing benchmark Dec-POMDP problems. We also compare RLaR against an existing approximate Dec-POMDP solver which, like RLaR, does not assume a priori knowledge of the model. While RLaR׳s policy representation is not as scalable, we show that RLaR produces higher quality policies for most problems and horizons studied.},
  comment  = {RLaR},
  doi      = {https://doi.org/10.1016/j.neucom.2016.01.031},
  file     = {:KRAEMER201682 - Multi Agent Reinforcement Learning As a Rehearsal for Decentralized Planning.pdf:PDF},
  groups   = {Multi-agent RL},
  keywords = {Multi-agent reinforcement learning, Decentralized planning},
  priority = {prio2},
  ranking  = {rank3},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231216000783},
}

@Article{Jorge2016,
  author        = {Emilio Jorge and Mikael Kågebäck and Fredrik D. Johansson and Emil Gustavsson},
  title         = {Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence},
  year          = {2016},
  month         = nov,
  abstract      = {Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most natural language processing today. As an alternative, we propose the use of situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks for evolving a shared language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that the agents learn not only to encode physical concepts in their words, i.e. grounding, but also that the agents learn to hold a multi-step dialogue remembering the state of the dialogue from step to step.},
  archiveprefix = {arXiv},
  comment       = {DIAL with extra steps and one-hot messages},
  eprint        = {1611.03218},
  file          = {:Jorge2016 - Learning to Play Guess Who_ and Inventing a Grounded Language As a Consequence.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.AI, cs.CL, cs.LG, cs.MA},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
  readstatus    = {skimmed},
}

@Article{Tampuu2017_IDQN,
  author    = {Tampuu, Ardi AND Matiisen, Tambet AND Kodelja, Dorian AND Kuzovkin, Ilya AND Korjus, Kristjan AND Aru, Juhan AND Aru, Jaan AND Vicente, Raul},
  journal   = {PLOS ONE},
  title     = {Multiagent cooperation and competition with deep reinforcement learning},
  year      = {2017},
  month     = {04},
  number    = {4},
  pages     = {1-15},
  volume    = {12},
  abstract  = {Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.},
  comment   = {Independent Deep-Q-learning},
  doi       = {10.1371/journal.pone.0172395},
  file      = {:10.1371_journal.pone.0172395 - Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, IDQN, Independent Learning},
  publisher = {Public Library of Science},
  ranking   = {rank3},
  url       = {https://doi.org/10.1371/journal.pone.0172395},
}

@InProceedings{Tan1993,
  author     = {Tan, Ming},
  booktitle  = {Proceedings of the Tenth International Conference on Machine Learning},
  title      = {Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents},
  year       = {1993},
  address    = {San Francisco, CA, USA},
  pages      = {330–337},
  publisher  = {Morgan Kaufmann Publishers Inc.},
  series     = {ICML'93},
  file       = {:tan1993.pdf:PDF},
  groups     = {Multi-agent RL, Independent Learning},
  location   = {Amherst, MA, USA},
  numpages   = {8},
  ranking    = {rank5},
  readstatus = {read},
  url        = {https://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf},
}

@InProceedings{Das2017_CoopVisDial,
  author     = {Das, Abhishek and Kottur, Satwik and Moura, Jose M. F. and Lee, Stefan and Batra, Dhruv},
  booktitle  = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title      = {Learning Cooperative Visual Dialog Agents With Deep Reinforcement Learning},
  year       = {2017},
  month      = {Oct},
  file       = {:Das_2017_ICCV - Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning.pdf:PDF},
  groups     = {Discrete language, Communication, Visual-Language Learning, Language-Grounded Communication, Speaker-Listener},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://openaccess.thecvf.com/content_iccv_2017/html/Das_Learning_Cooperative_Visual_ICCV_2017_paper.html},
}

@InProceedings{Lazaridou2017,
  author     = {Angeliki Lazaridou and Alexander Peysakhovich and Marco Baroni},
  booktitle  = {International Conference on Learning Representations},
  title      = {Multi-Agent Cooperation and the Emergence of (Natural) Language},
  year       = {2017},
  month      = dec,
  abstract   = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.},
  file       = {:Lazaridou2016 - Multi Agent Cooperation and the Emergence of (Natural) Language.pdf:PDF},
  groups     = {Discrete language, Communication, Language-Grounded Communication, Speaker-Listener},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=Hk8N3Sclg},
}

@InCollection{Gupta2017_PSTRPO,
  author     = {Jayesh K. Gupta and Maxim Egorov and Mykel Kochenderfer},
  booktitle  = {Autonomous Agents and Multiagent Systems},
  publisher  = {Springer International Publishing},
  title      = {Cooperative Multi-agent Control Using Deep Reinforcement Learning},
  year       = {2017},
  pages      = {66--83},
  comment    = {centralised actor-critic algorithm with per-agent critics
sharing of policy parameters},
  doi        = {10.1007/978-3-319-71682-4_5},
  file       = {:Gupta2017 - Cooperative Multi Agent Control Using Deep Reinforcement Learning.pdf:PDF},
  groups     = {Multi-agent RL, Centralised Training and Execution, Independent Learning},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@InProceedings{Foerster2017_StabilisingExpReplay,
  author     = {Jakob Foerster and Nantas Nardelli and Gregory Farquhar and Triantafyllos Afouras and Philip H. S. Torr and Pushmeet Kohli and Shimon Whiteson},
  booktitle  = {Proceedings of the 34th International Conference on Machine Learning},
  title      = {Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning},
  year       = {2017},
  address    = {International Convention Centre, Sydney, Australia},
  editor     = {Doina Precup and Yee Whye Teh},
  month      = {06--11 Aug},
  pages      = {1146--1155},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {70},
  abstract   = {Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent’s value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.},
  comment    = {Replay stabilisation for Independent Q-learning},
  file       = {:foerster17b - Stabilising Experience Replay for Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Multi-agent RL},
  pdf        = {http://proceedings.mlr.press/v70/foerster17b/foerster17b.pdf},
  ranking    = {rank2},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v70/foerster17b.html},
}

@InProceedings{Rashid2018_QMIX,
  author     = {Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle  = {Proceedings of the 35th International Conference on Machine Learning},
  title      = {{QMIX}: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  year       = {2018},
  address    = {Stockholmsmässan, Stockholm Sweden},
  editor     = {Jennifer Dy and Andreas Krause},
  month      = {10--15 Jul},
  pages      = {4295--4304},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {80},
  abstract   = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
  file       = {:rashid18a - QMIX_ Monotonic Value Function Factorisation for Deep Multi Agent Reinforcement Learning.pdf:PDF;:rashid18a - QMIX_ Monotonic Value Function Factorisation for Deep Multi Agent Reinforcement Learning (1).pdf:PDF},
  groups     = {Multi-agent RL, Value factorisation},
  pdf        = {http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf},
  ranking    = {rank4},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v80/rashid18a.html},
}

@InProceedings{Omidshafiei2017_DecentrMultiTask,
  author     = {Shayegan Omidshafiei and Jason Pazis and Christopher Amato and Jonathan P. How and John Vian},
  booktitle  = {Proceedings of the 34th International Conference on Machine Learning},
  title      = {Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability},
  year       = {2017},
  address    = {International Convention Centre, Sydney, Australia},
  editor     = {Doina Precup and Yee Whye Teh},
  month      = {06--11 Aug},
  pages      = {2681--2690},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {70},
  abstract   = {Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.},
  comment    = {Multi-task MARL
Dec-Histeretic Deep Recurrent Q-Network
Concurrent Experience Replay Trajectories (CERT)},
  file       = {:omidshafiei17a - Deep Decentralized Multi Task Multi Agent Reinforcement Learning under Partial Observability.pdf:PDF},
  groups     = {Multi-agent RL, Multi-task, IDQN, Independent Learning},
  pdf        = {http://proceedings.mlr.press/v70/omidshafiei17a/omidshafiei17a.pdf},
  ranking    = {rank3},
  readstatus = {skimmed},
  url        = {http://proceedings.mlr.press/v70/omidshafiei17a.html},
}

@Article{Usunier2016,
  author        = {Nicolas Usunier and Gabriel Synnaeve and Zeming Lin and Soumith Chintala},
  title         = {Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks},
  year          = {2016},
  month         = sep,
  abstract      = {We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the state-action space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, {\epsilon}-greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.},
  archiveprefix = {arXiv},
  eprint        = {1609.02993},
  file          = {:Usunier2016 - Episodic Exploration for Deep Deterministic Policies_ an Application to StarCraft Micromanagement Tasks.pdf:PDF},
  groups        = {DDPG},
  keywords      = {cs.AI, cs.LG, I.2.1; I.2.6},
  primaryclass  = {cs.AI},
  ranking       = {rank2},
}

@InCollection{Littman1994_MarkovGames,
  author    = {Michael L. Littman},
  booktitle = {Machine Learning Proceedings 1994},
  publisher = {Morgan Kaufmann},
  title     = {Markov games as a framework for multi-agent reinforcement learning},
  year      = {1994},
  address   = {San Francisco (CA)},
  editor    = {William W. Cohen and Haym Hirsh},
  isbn      = {978-1-55860-335-6},
  pages     = {157-163},
  abstract  = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
  doi       = {https://doi.org/10.1016/B978-1-55860-335-6.50027-1},
  file      = {:LITTMAN1994157 - Markov Games As a Framework for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Joint Action Learning},
  ranking   = {rank3},
  url       = {https://www.sciencedirect.com/science/article/pii/B9781558603356500271},
}

@Article{Hu2021a,
  author        = {Jian Hu and Haibin Wu and Seth Austin Harding and Siyang Jiang and Shih-wei Liao},
  title         = {RIIT: Rethinking the Importance of Implementation Tricks in Multi-Agent Reinforcement Learning},
  year          = {2021},
  month         = feb,
  abstract      = {In recent years, Multi-Agent Deep Reinforcement Learning (MADRL) has been successfully applied to various complex scenarios such as computer games and robot swarms. We investigate the impact of "implementation tricks" of state-of-the-art (SOTA) QMIX-based algorithms. Firstly, we find that such tricks, described as auxiliary details to the core algorithm, seemingly of secondary importance, have a major impact. Our finding demonstrates that, after minimal tuning, QMIX attains extraordinarily high win rates and achieves SOTA in the StarCraft Multi-Agent Challenge (SMAC). Furthermore, we find QMIX's monotonicity condition improves sample efficiency in some cooperative tasks. We propose a new policy-based algorithm, called RIIT, to verify the importance of the monotonicity condition. RIIT also achieves SOTA in policy-based algorithms. At last, we prove theoretically that the Purely Cooperative Tasks can be represented by the monotonic mixing networks. We open-sourced the code at \url{https://github.com/hijkzzz/pymarl2}.},
  archiveprefix = {arXiv},
  eprint        = {2102.03479},
  file          = {:Hu2021a - RIIT_ Rethinking the Importance of Implementation Tricks in Multi Agent Reinforcement Learning (1).pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.MA},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Article{Brown2020,
  author        = {Noam Brown and Anton Bakhtin and Adam Lerer and Qucheng Gong},
  title         = {Combining Deep Reinforcement Learning and Search for Imperfect-Information Games},
  year          = {2020},
  month         = jul,
  abstract      = {The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.},
  archiveprefix = {arXiv},
  comment       = {ReBeL},
  eprint        = {2007.13544},
  file          = {:Brown2020 - Combining Deep Reinforcement Learning and Search for Imperfect Information Games.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.GT, cs.AI, cs.LG},
  primaryclass  = {cs.GT},
}

@InProceedings{Rashid2020_WQMIX,
  author     = {Rashid, Tabish and Farquhar, Gregory and Peng, Bei and Whiteson, Shimon},
  booktitle  = {Advances in Neural Information Processing Systems 33},
  title      = {Weighted QMIX: Expanding monotonic value function factorisation for deep multi−agent reinforcement learning},
  year       = {2020},
  pages      = {10199--10210},
  volume     = {33},
  file       = {:Rashid2020 - Weighted QMIX_ Expanding Monotonic Value Function Factorisation for Deep Multi−agent Reinforcement Learning.pdf:PDF},
  groups     = {Multi-agent RL, Value factorisation},
  organizer  = {34th Annual Conference on Neural Information Processing Systems (NeurIPS 2020)},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2020/hash/73a427badebe0e32caa2e1fc7530b7f3-Abstract.html},
}

@Article{Witt2020_IPPO,
  author        = {Schroeder de Witt, Christian and Tarun Gupta and Denys Makoviichuk and Viktor Makoviychuk and Philip H. S. Torr and Mingfei Sun and Shimon Whiteson},
  title         = {Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  year          = {2020},
  month         = nov,
  abstract      = {Most recently developed approaches to cooperative multi-agent reinforcement learning in the \emph{centralized training with decentralized execution} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent PPO (IPPO), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite SMAC with little hyperparameter tuning. We also compare IPPO to several variants; the results suggest that IPPO's strong performance may be due to its robustness to some forms of environment non-stationarity.},
  archiveprefix = {arXiv},
  comment       = {IPPO},
  eprint        = {2011.09533},
  file          = {:Witt2020 - Is Independent Learning All You Need in the StarCraft Multi Agent Challenge_.pdf:PDF},
  groups        = {Multi-agent RL, Independent Learning},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
  readstatus    = {read},
}

@InProceedings{Li2022_MAMT,
  author        = {Wenhao Li and Xiangfeng Wang and Bo Jin and Junjie Sheng and Hongyuan Zha},
  booktitle     = {The Tenth International Conference on Learning Representations, ICLR 2022},
  title         = {Dealing with Non-Stationarity in Multi-Agent Reinforcement Learning via Trust Region Decomposition},
  year          = {2022},
  abstract      = {Non-stationarity is one thorny issue in multi-agent reinforcement learning, which is caused by the policy changes of agents during the learning procedure. Current works to solve this problem have their own limitations in effectiveness and scalability, such as centralized critic and decentralized actor (CCDA), population-based self-play, modeling of others and etc. In this paper, we novelly introduce a $\delta$-stationarity measurement to explicitly model the stationarity of a policy sequence, which is theoretically proved to be proportional to the joint policy divergence. However, simple policy factorization like mean-field approximation will mislead to larger policy divergence, which can be considered as trust region decomposition dilemma. We model the joint policy as a general Markov random field and propose a trust region decomposition network based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established with the purpose to satisfy $\delta$-stationarity. MAMT can adjust the trust region of the local policies adaptively in an end-to-end manner, thereby approximately constraining the divergence of joint policy to alleviate the non-stationary problem. Our method can bring noticeable and stable performance improvement compared with baselines in coordination tasks of different complexity.},
  archiveprefix = {arXiv},
  eprint        = {2102.10616},
  file          = {:Li2022_MAMT - Dealing with Non Stationarity in Multi Agent Reinforcement Learning Via Trust Region Decomposition.pdf:PDF:https\://openreview.net/pdf?id=XHUxf5aRB3s},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.GT, cs.MA},
  primaryclass  = {cs.LG},
  ranking       = {rank1},
}

@InProceedings{Liu2020,
  author    = {Liu, Iou-Jen and Yeh, Raymond A. and Schwing, Alexander G.},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {PIC: Permutation Invariant Critic for Multi-Agent Deep Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {590--602},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Sample efficiency and scalability to a large number of agents are two important goals for multi-agent reinforcement learning systems. Recent works got us closer to those goals, addressing non-stationarity of the environment from a single agent’s perspective by utilizing a deep net critic which depends on all observations and actions. The critic input concatenates agent observations and actions in a user-specified order. However, since deep nets aren’t permutation invariant, a permuted input changes the critic output despite the environment remaining identical. To avoid this inefficiency, we propose a ‘permutation invariant critic’ (PIC), which yields identical output irrespective of the agent permutation. This consistent representation enables our model to scale to 30 times more agents and to achieve improvements of test episode reward between 15% to 50% on the challenging multi-agent particle environment (MPE).},
  file      = {:Liu2020 - PIC_ Permutation Invariant Critic for Multi Agent Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v100/liu20a/liu20a.pdf},
  url       = {http://proceedings.mlr.press/v100/liu20a.html},
}

@Article{Hu2003_NashQLearning,
  author  = {Hu, Junling and Wellman, Michael P},
  journal = {Journal of machine learning research},
  title   = {Nash Q-learning for general-sum stochastic games},
  year    = {2003},
  number  = {Nov},
  pages   = {1039--1069},
  volume  = {4},
  file    = {:hu2003nash - Nash Q Learning for General Sum Stochastic Games.pdf:PDF},
  groups  = {Multi-agent RL, Joint Action Learning},
  ranking = {rank4},
}

@Misc{yu2021benchmarking,
  author     = {Chao Yu and Akash Velu and Eugene Vinitsky and Yu Wang and Alexandre Bayen and Yi Wu},
  title      = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms},
  year       = {2021},
  comment    = {MAPPO},
  file       = {:NeurIPS_DRL_Workshop_BenchmarkingMARL.pdf:PDF},
  groups     = {MAS Reviews},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=t5lNr0Lw84H},
}

@Article{Papoudakis2020_BenchmarkMADRL,
  author     = {Georgios Papoudakis and Filippos Christianos and Lukas Sch{\"a}fer and Stefano V Albrecht},
  title      = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
  year       = {2021},
  booktitle  = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  file       = {:Papoudakis2020 - Comparative Evaluation of Multi Agent Deep Reinforcement Learning Algorithms (1).pdf:PDF},
  groups     = {MAS Reviews},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=cIrPX-Sn5n},
}

@InProceedings{Claus1998,
  author  = {Claus, Caroline and Boutilier, Craig},
  title   = {The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems},
  year    = {1998},
  month   = {07},
  file    = {:Claus1998 - The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf:PDF},
  groups  = {Multi-agent RL},
  journal = {The National Conference on Artificial Intelligence (AAAI 1998)},
  ranking = {rank2},
}

@InProceedings{littman2001friend,
  author    = {Littman, Michael L},
  booktitle = {ICML},
  title     = {Friend-or-foe Q-learning in general-sum games},
  year      = {2001},
  pages     = {322--328},
  volume    = {1},
  file      = {:littman2001friend - Friend or Foe Q Learning in General Sum Games.pdf:PDF},
  groups    = {Multi-agent RL, Joint Action Learning},
  ranking   = {rank2},
}

@InProceedings{Greenwald2003_CorrelatedQLearning,
  author    = {Greenwald, Amy and Hall, Keith and Serrano, Roberto},
  booktitle = {ICML},
  title     = {Correlated Q-learning},
  year      = {2003},
  pages     = {242--249},
  volume    = {3},
  file      = {:greenwald2003correlated - Correlated Q Learning.pdf:PDF},
  groups    = {Multi-agent RL, Joint Action Learning},
  ranking   = {rank3},
  url       = {https://dl.acm.org/doi/abs/10.5555/3041838.3041869},
}

@InBook{Buşoniu2010,
  author     = {Bu{\c{s}}oniu, Lucian and Babu{\v{s}}ka, Robert and De Schutter, Bart},
  editor     = {Srinivasan, Dipti and Jain, Lakhmi C.},
  pages      = {183--221},
  publisher  = {Springer Berlin Heidelberg},
  title      = {Multi-agent Reinforcement Learning: An Overview},
  year       = {2010},
  address    = {Berlin, Heidelberg},
  isbn       = {978-3-642-14435-6},
  abstract   = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of multi-agent reinforcement learning algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of multi-agent reinforcement learning are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where multi-agent reinforcement learning techniques have been applied are briefly discussed. Several multi-agent reinforcement learning algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the multi-agent reinforcement learning field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
  booktitle  = {Innovations in Multi-Agent Systems and Applications - 1},
  doi        = {10.1007/978-3-642-14435-6_7},
  file       = {:Buşoniu2010 - Multi Agent Reinforcement Learning_ an Overview.pdf:PDF},
  groups     = {MAS Reviews},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://doi.org/10.1007/978-3-642-14435-6_7},
}

@Article{bowling2005convergence,
  author  = {Bowling, Michael},
  journal = {Advances in neural information processing systems},
  title   = {Convergence and no-regret in multiagent learning},
  year    = {2005},
  pages   = {209--216},
  volume  = {17},
  comment = {GIGA-WoLF},
  file    = {:bowling2005convergence - Convergence and No Regret in Multiagent Learning.pdf:PDF},
  groups  = {Multi-agent RL},
  ranking = {rank2},
}

@InProceedings{Kok2004,
  author    = {Jelle R. Kok and Nikos Vlassis},
  booktitle = {21st International Conference on Machine learning - {ICML} {\textquotesingle}04},
  title     = {Sparse cooperative Q-learning},
  year      = {2004},
  publisher = {{ACM} Press},
  doi       = {10.1145/1015330.1015410},
  file      = {:Kok2004 - Sparse Cooperative Q Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank2},
}

@InProceedings{guestrin2002coordinated,
  author       = {Guestrin, Carlos and Lagoudakis, Michail and Parr, Ronald},
  booktitle    = {ICML},
  title        = {Coordinated reinforcement learning},
  year         = {2002},
  organization = {Citeseer},
  pages        = {227--234},
  volume       = {2},
  file         = {:guestrin2002coordinated - Coordinated Reinforcement Learning.pdf:PDF},
  groups       = {Multi-agent RL},
}

@Article{KokV06,
  author    = {Jelle R. Kok and Nikos A. Vlassis},
  journal   = {J. Mach. Learn. Res.},
  title     = {Collaborative Multiagent Reinforcement Learning by Payoff Propagation},
  year      = {2006},
  pages     = {1789--1828},
  volume    = {7},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/jmlr/KokV06.bib},
  file      = {:KokV06 - Collaborative Multiagent Reinforcement Learning by Payoff Propagation.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank3},
  timestamp = {Wed, 10 Jul 2019 15:28:11 +0200},
  url       = {http://jmlr.org/papers/v7/kok06a.html},
}

@InProceedings{Bowling2001_Rational,
  author       = {Bowling, Michael and Veloso, Manuela},
  booktitle    = {International joint conference on artificial intelligence},
  title        = {Rational and convergent learning in stochastic games},
  year         = {2001},
  number       = {1},
  organization = {Citeseer},
  pages        = {1021--1026},
  volume       = {17},
  comment      = {WoLF},
  groups       = {Multi-agent RL, Bayesian learning},
  ranking      = {rank3},
  url          = {https://dl.acm.org/doi/abs/10.5555/1642194.1642231},
}

@Article{BOWLING2002215,
  author   = {Michael Bowling and Manuela Veloso},
  journal  = {Artificial Intelligence},
  title    = {Multiagent learning using a variable learning rate},
  year     = {2002},
  issn     = {0004-3702},
  number   = {2},
  pages    = {215-250},
  volume   = {136},
  abstract = {Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents' policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, “Win or Learn Fast”, for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method.},
  comment  = {WoLF},
  doi      = {https://doi.org/10.1016/S0004-3702(02)00121-2},
  file     = {:BOWLING2002215 - Multiagent Learning Using a Variable Learning Rate.pdf:PDF},
  groups   = {Multi-agent RL},
  keywords = {Multiagent learning, Reinforcement learning, Game theory},
  ranking  = {rank3},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370202001212},
}

@Article{LITTMAN200155,
  author   = {Michael L. Littman},
  journal  = {Cognitive Systems Research},
  title    = {Value-function reinforcement learning in Markov games},
  year     = {2001},
  issn     = {1389-0417},
  number   = {1},
  pages    = {55-66},
  volume   = {2},
  abstract = {Markov games are a model of multiagent environments that are convenient for studying multiagent reinforcement learning. This paper describes a set of reinforcement-learning algorithms based on estimating value functions and presents convergence theorems for these algorithms. The main contribution of this paper is that it presents the convergence theorems in a way that makes it easy to reason about the behavior of simultaneous learners in a shared environment.},
  comment  = {Team Q-learning},
  doi      = {https://doi.org/10.1016/S1389-0417(01)00015-8},
  file     = {:LITTMAN200155 - Value Function Reinforcement Learning in Markov Games.pdf:PDF},
  groups   = {Multi-agent RL},
  keywords = {Reinforcement learning, Temporal difference learning, Value functions, Game theory, Markov games, -learning, Nash equilibria},
  url      = {https://www.sciencedirect.com/science/article/pii/S1389041701000158},
}

@InProceedings{Lauer00analgorithm,
  author    = {Martin Lauer and Martin Riedmiller},
  booktitle = {In Proceedings of the Seventeenth International Conference on Machine Learning},
  title     = {An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems},
  year      = {2000},
  pages     = {535--542},
  publisher = {Morgan Kaufmann},
  comment   = {Distributed Q-learning},
  file      = {:Lauer-Riedmiller2000.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank2},
}

@InProceedings{hu1998multiagent,
  author       = {Hu, Junling and Wellman, Michael P},
  booktitle    = {ICML},
  title        = {Multiagent reinforcement learning: theoretical framework and an algorithm},
  year         = {1998},
  organization = {Citeseer},
  pages        = {242--250},
  volume       = {98},
  comment      = {Nash Q learning},
  file         = {:hu1998multiagent - Multiagent Reinforcement Learning_ Theoretical Framework and an Algorithm (2).pdf:PDF},
  groups       = {Multi-agent RL},
  ranking      = {rank3},
}

@Article{Gmytrasiewicz2005,
  author    = {P. J. Gmytrasiewicz and P. Doshi},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {A Framework for Sequential Planning in Multi-Agent Settings},
  year      = {2005},
  month     = {jul},
  pages     = {49--79},
  volume    = {24},
  doi       = {10.1613/jair.1579},
  file      = {:Gmytrasiewicz2005 - A Framework for Sequential Planning in Multi Agent Settings.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {{AI} Access Foundation},
  ranking   = {rank2},
}

@InProceedings{zinkevich2003online,
  author    = {Zinkevich, Martin},
  booktitle = {Proceedings of the 20th international conference on machine learning (icml-03)},
  title     = {Online convex programming and generalized infinitesimal gradient ascent},
  year      = {2003},
  pages     = {928--936},
  comment   = {GIGA},
  file      = {:zinkevich2003online - Online Convex Programming and Generalized Infinitesimal Gradient Ascent.pdf:PDF},
  groups    = {Multi-agent RL},
}

@Article{conitzer2007awesome,
  author    = {Conitzer, Vincent and Sandholm, Tuomas},
  journal   = {Machine Learning},
  title     = {AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents},
  year      = {2007},
  number    = {1-2},
  pages     = {23--43},
  volume    = {67},
  file      = {:conitzer2007awesome - AWESOME_ a General Multiagent Learning Algorithm That Converges in Self Play and Learns a Best Response against Stationary Opponents.pdf:PDF},
  groups    = {Multi-agent RL, Equilibrium selection},
  publisher = {Springer},
  ranking   = {rank3},
}

@Article{foerster2017learning,
  author     = {Foerster, Jakob N and Chen, Richard Y and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
  journal    = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Learning with opponent-learning awareness},
  year       = {2018},
  comment    = {LOLA},
  file       = {:foerster2017learning - Learning with Opponent Learning Awareness.pdf:PDF},
  groups     = {Multi-agent RL},
  ranking    = {rank2},
  readstatus = {read},
}

@InProceedings{Mahajan2019_MAVEN,
  author     = {Mahajan, Anuj and Rashid, Tabish and Samvelyan, Mikayel and Whiteson, Shimon},
  booktitle  = {Advances in Neural Information Processing Systems 32},
  title      = {MAVEN: Multi-Agent Variational Exploration},
  year       = {2019},
  volume     = {32},
  abstract   = {Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43].},
  file       = {:Mahajan2019 - MAVEN_ Multi Agent Variational Exploration.pdf:PDF},
  groups     = {Multi-agent RL, Exploration in MARL},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2019/file/f816dc0acface7498e10496222e9db10-Paper.pdf},
}

@Article{doi_10.1177/0278364913495721,
  author   = {Jens Kober and J. Andrew Bagnell and Jan Peters},
  journal  = {The International Journal of Robotics Research},
  title    = {Reinforcement learning in robotics: A survey},
  year     = {2013},
  number   = {11},
  pages    = {1238-1274},
  volume   = {32},
  abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
  doi      = {10.1177/0278364913495721},
  eprint   = {https://doi.org/10.1177/0278364913495721},
  file     = {:doi_10.1177_0278364913495721 - Reinforcement Learning in Robotics_ a Survey.pdf:PDF},
  groups   = {RL in Robotics},
  ranking  = {rank5},
  url      = {https://doi.org/10.1177/0278364913495721},
}

@TechReport{yang2004multiagent,
  author      = {Yang, Erfu and Gu, Dongbing},
  institution = {tech. rep},
  title       = {Multiagent reinforcement learning for multi-robot systems: A survey},
  year        = {2004},
  file        = {:yang2004multiagent - Multiagent Reinforcement Learning for Multi Robot Systems_ a Survey.pdf:PDF},
  groups      = {Multi-robot},
  ranking     = {rank2},
}

@Article{Peng2017_BiCNet,
  author        = {Peng Peng and Ying Wen and Yaodong Yang and Quan Yuan and Zhenkun Tang and Haitao Long and Jun Wang},
  title         = {Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games},
  year          = {2017},
  month         = mar,
  abstract      = {Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.},
  archiveprefix = {arXiv},
  comment       = {BiCNet},
  eprint        = {1703.10069},
  file          = {:Peng2017 - Multiagent Bidirectionally Coordinated Nets_ Emergence of Human Level Coordination in Learning to Play StarCraft Combat Games.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
  readstatus    = {read},
}

@InProceedings{FSS1511673,
  author     = {Matthew Hausknecht and Peter Stone},
  booktitle  = {Sequential Decision Making for Intelligent Agents Papers from the AAAI 2015 Fall Symposium},
  title      = {Deep Recurrent Q-Learning for Partially Observable MDPs},
  year       = {2015},
  abstract   = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  conference = {AAAI Fall Symposium Series},
  file       = {:FSS1511673 - Deep Recurrent Q Learning for Partially Observable MDPs (1).pdf:PDF},
  groups     = {POMDP},
  keywords   = {Deep Reinforcement Learning; LSTM; Deep Networks; Reinforcement Learning; Atari; Deep Q-Learning},
  ranking    = {rank4},
  readstatus = {skimmed},
  url        = {https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/11673},
}

@InProceedings{45823,
  author    = {David Ha and Andrew Dai and Quoc V. Le},
  booktitle = {ICLR 2017},
  title     = {HyperNetworks},
  year      = {2017},
  file      = {:45823 - HyperNetworks.pdf:PDF},
  ranking   = {rank3},
  url       = {https://openreview.net/pdf?id=rkpACe1lx},
}

@Article{Rizk2019_CoopHeteroMRS,
  author     = {Rizk, Yara and Awad, Mariette and Tunstel, Edward W},
  journal    = {ACM Computing Surveys (CSUR)},
  title      = {Cooperative heterogeneous multi-robot systems: A survey},
  year       = {2019},
  number     = {2},
  pages      = {1--31},
  volume     = {52},
  doi        = {https://doi.org/10.1145/3303848},
  file       = {:rizk2019cooperative - Cooperative Heterogeneous Multi Robot Systems_ a Survey.pdf:PDF},
  groups     = {Multi-robot, Heterogeneous},
  publisher  = {ACM New York, NY, USA},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{Kapturowski2018,
  author     = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle  = {International conference on learning representations},
  title      = {Recurrent experience replay in distributed reinforcement learning},
  year       = {2018},
  comment    = {R2D2},
  file       = {:kapturowski2018recurrent - Recurrent Experience Replay in Distributed Reinforcement Learning.pdf:PDF},
  groups     = {Value based},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@PhdThesis{hausknecht2016cooperation,
  author = {Hausknecht, Matthew John},
  title  = {Cooperation and communication in multiagent deep reinforcement learning},
  year   = {2016},
  file   = {:HAUSKNECHT-DISSERTATION-2016.pdf:PDF},
  groups = {Communication},
}

@InProceedings{Son2019_Qtran,
  author     = {Son, Kyunghwan and Kim, Daewoo and Kang, Wan Ju and Hostallero, David Earl and Yi, Yung},
  booktitle  = {International Conference on Machine Learning},
  title      = {Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning},
  year       = {2019},
  month      = may,
  pages      = {5887--5896},
  volume     = {PMLR 97},
  file       = {:son2019qtran - Qtran_ Learning to Factorize with Transformation for Cooperative Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Multi-agent RL, Value factorisation},
  ranking    = {rank2},
  readstatus = {read},
}

@InProceedings{Foerster2016_DIAL,
  author     = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
  booktitle  = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  title      = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  year       = {2016},
  address    = {Red Hook, NY, USA},
  pages      = {2145–2153},
  publisher  = {Curran Associates Inc.},
  series     = {NIPS'16},
  abstract   = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
  comment    = {DIAL},
  file       = {:10.5555_3157096.3157336 - Learning to Communicate with Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Communication},
  location   = {Barcelona, Spain},
  numpages   = {9},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf},
}

@Article{schroeder2019_MACKRL,
  author     = {Schroeder de Witt, Christian and Foerster, Jakob and Farquhar, Gregory and Torr, Philip and Boehmer, Wendelin and Whiteson, Shimon},
  journal    = {Advances in Neural Information Processing Systems},
  title      = {Multi-agent common knowledge reinforcement learning},
  year       = {2019},
  pages      = {9927--9939},
  volume     = {32},
  comment    = {MACKRL},
  file       = {:schroeder2019multi - Multi Agent Common Knowledge Reinforcement Learning (1).pdf:PDF},
  groups     = {Multi-agent RL},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2019/file/f968fdc88852a4a3a27a81fe3f57bfc5-Paper.pdf},
}

@Article{Witt2020a,
  author        = {Schroeder de Witt, Christian and Bei Peng and Pierre-Alexandre Kamienny and Philip Torr and Wendelin Böhmer and Shimon Whiteson},
  title         = {Deep Multi-Agent Reinforcement Learning for Decentralized Continuous Cooperative Control},
  year          = {2020},
  month         = mar,
  abstract      = {Centralised training with decentralised execution (CTDE) is an important learning paradigm in multi-agent reinforcement learning (MARL). To make progress in CTDE, we introduce Multi-Agent MuJoCo (MAMuJoCo), a novel benchmark suite that, unlike StarCraft Multi-Agent Challenge (SMAC), the predominant benchmark environment, applies to continuous robotic control tasks. To demonstrate the utility of MAMuJoCo, we present a range of benchmark results on this new suite, including comparing the state-of-the-art actor-critic method MADDPG against two novel variants of existing methods. These new methods outperform MADDPG on a number of MAMuJoCo tasks. In addition, we show that, in these continuous cooperative MAMuJoCo tasks, value factorisation plays a greater role in performance than the underlying algorithmic choices. This motivates the necessity of extending the study of value factorisations from $Q$-learning to actor-critic algorithms.},
  archiveprefix = {arXiv},
  comment       = {COMIX
FacMADDPG},
  eprint        = {2003.06709},
  file          = {:Witt2020a - Deep Multi Agent Reinforcement Learning for Decentralized Continuous Cooperative Control.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Levine2016_Endtoend,
  author    = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal   = {The Journal of Machine Learning Research},
  title     = {End-to-end training of deep visuomotor policies},
  year      = {2016},
  number    = {1},
  pages     = {1334--1373},
  volume    = {17},
  file      = {:levine2016end - End to End Training of Deep Visuomotor Policies.pdf:PDF},
  groups    = {Robotics},
  publisher = {JMLR. org},
  ranking   = {rank5},
}

@InProceedings{matignon2007hysteretic,
  author       = {Matignon, La{\"e}titia and Laurent, Guillaume J and Le Fort-Piat, Nadine},
  booktitle    = {2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title        = {Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams},
  year         = {2007},
  organization = {IEEE},
  pages        = {64--69},
  file         = {:matignon2007hysteretic - Hysteretic Q Learning_ an Algorithm for Decentralized Reinforcement Learning in Cooperative Multi Agent Teams.pdf:PDF},
  groups       = {Multi-agent RL},
}

@Article{Sukhbaatar2017,
  author        = {Sainbayar Sukhbaatar and Zeming Lin and Ilya Kostrikov and Gabriel Synnaeve and Arthur Szlam and Rob Fergus},
  title         = {Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play},
  year          = {2017},
  month         = mar,
  abstract      = {We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.},
  archiveprefix = {arXiv},
  eprint        = {1703.05407},
  file          = {:Sukhbaatar2017 - Intrinsic Motivation and Automatic Curricula Via Asymmetric Self Play.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
}

@InProceedings{He2016,
  author    = {He, He and Boyd-Graber, Jordan and Kwok, Kevin and Daum\'e, III, Hal},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  title     = {Opponent Modeling in Deep Reinforcement Learning},
  year      = {2016},
  address   = {New York, New York, USA},
  editor    = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  month     = {20--22 Jun},
  pages     = {1804--1813},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {48},
  abstract  = {Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because of strategies’ complex interaction and the non-stationary nature. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent’s action, we encode observation of the opponents into a deep Q-Network (DQN), while retaining explicit modeling under multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents even without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.},
  comment   = {DRON},
  file      = {:He2016 - Opponent Modeling in Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v48/he16.pdf},
  ranking   = {rank1},
  url       = {http://proceedings.mlr.press/v48/he16.html},
}

@InProceedings{Sunehag2018_VDN,
  author     = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
  booktitle  = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
  year       = {2018},
  address    = {Richland, SC},
  pages      = {2085–2087},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '18},
  abstract   = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
  file       = {:10.5555_3237383.3238080 - Value Decomposition Networks for Cooperative Multi Agent Learning Based on Team Reward.pdf:PDF},
  groups     = {Multi-agent RL, Value factorisation},
  keywords   = {neural networks, reinforcement learning, collaborative, q-learning, dqn, multi-agent, value-decomposition},
  location   = {Stockholm, Sweden},
  numpages   = {3},
  readstatus = {read},
}

@InProceedings{10.5555/3306127.3332052,
  author    = {Samvelyan, Mikayel and Rashid, Tabish and Schroeder de Witt, Christian and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
  booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {The StarCraft Multi-Agent Challenge},
  year      = {2019},
  address   = {Richland, SC},
  pages     = {2186–2188},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '19},
  abstract  = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.},
  file      = {:10.5555_3306127.3332052 - The StarCraft Multi Agent Challenge (1).pdf:PDF},
  groups    = {MAS Reviews, Multi-Agent Environments},
  isbn      = {9781450363099},
  keywords  = {multi-agent learning, starcraft, reinforcement learning},
  location  = {Montreal QC, Canada},
  numpages  = {3},
  ranking   = {rank4},
}

@InProceedings{Bohmer2020,
  author    = {Böhmer, Wendelin and Kurin, Vitaly and Whiteson, Shimon},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Deep Coordination Graphs},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  month     = {13--18 Jul},
  pages     = {980--991},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {This paper introduces the deep coordination graph (DCG) for collaborative multi-agent reinforcement learning. DCG strikes a flexible trade-off between representational capacity and generalization by factoring the joint value function of all agents according to a coordination graph into payoffs between pairs of agents. The value can be maximized by local message passing along the graph, which allows training of the value function end-to-end with Q-learning. Payoff functions are approximated with deep neural networks that employ parameter sharing and low-rank approximations to significantly improve sample efficiency. We show that DCG can solve predator-prey tasks that highlight the relative overgeneralization pathology, as well as challenging StarCraft II micromanagement tasks.},
  file      = {:Boehmer2020 - Deep Coordination Graphs.pdf:PDF},
  groups    = {Multi-agent RL, Graphs, Centralised Training and Execution},
  pdf       = {http://proceedings.mlr.press/v119/boehmer20a/boehmer20a.pdf},
  ranking   = {rank2},
  url       = {http://proceedings.mlr.press/v119/boehmer20a.html},
}

@Article{Wei2018_MultiSoftQ,
  author        = {Ermo Wei and Drew Wicke and David Freelan and Sean Luke},
  title         = {Multiagent Soft Q-Learning},
  year          = {2018},
  month         = apr,
  abstract      = {Policy gradient methods are often applied to reinforcement learning in continuous multiagent games. These methods perform local search in the joint-action space, and as we show, they are susceptable to a game-theoretic pathology known as relative overgeneralization. To resolve this issue, we propose Multiagent Soft Q-learning, which can be seen as the analogue of applying Q-learning to continuous controls. We compare our method to MADDPG, a state-of-the-art approach, and show that our method achieves better coordination in multiagent cooperative tasks, converging to better local optima in the joint action space.},
  archiveprefix = {arXiv},
  booktitle     = {2018 AAAI Spring Symposium Series},
  eprint        = {1804.09817},
  file          = {:Wei2018 - Multiagent Soft Q Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
}

@Article{Wei2016_RelOvergen,
  author    = {Wei, Ermo and Luke, Sean},
  journal   = {The Journal of Machine Learning Research},
  title     = {Lenient learning in independent-learner stochastic cooperative games},
  year      = {2016},
  number    = {1},
  pages     = {2914--2955},
  volume    = {17},
  comment   = {Relative overgeneralization},
  file      = {:wei2016lenient - Lenient Learning in Independent Learner Stochastic Cooperative Games.pdf:PDF},
  groups    = {Multi-agent RL, Independent Learning},
  publisher = {JMLR. org},
  url       = {https://www.jmlr.org/papers/v17/15-417.html},
}

@PhdThesis{wiegand2003analysis,
  author  = {Wiegand, R Paul},
  school  = {Citeseer},
  title   = {An analysis of cooperative coevolutionary algorithms},
  year    = {2003},
  file    = {:wiegand2003analysis - An Analysis of Cooperative Coevolutionary Algorithms.pdf:PDF},
  groups  = {Multi-agent RL},
  ranking = {rank3},
}

@Article{WANG202068,
  author   = {Di Wang and Hongbin Deng and Zhenhua Pan},
  journal  = {Neurocomputing},
  title    = {MRCDRL: Multi-robot coordination with deep reinforcement learning},
  year     = {2020},
  issn     = {0925-2312},
  pages    = {68-76},
  volume   = {406},
  abstract = {This paper proposes a multi-robot cooperative algorithm based on deep reinforcement learning (MRCDRL). We use end-to-end methods to train directly from each robot-centered, relative perspective-generated image, and each robot’s reward as the input. During training, it is not necessary to specify the target position and movement path of each robot. MRCDRL learns the actions of each robot by training the neural network. MRCDRL uses the neural network structure that was modified from the Duel neural network structure. In the Duel network structure, there are two streams that each represents the state value function and the state-dependent action advantage function, and the results of the two streams are merged. The proposed method can solve the resource competition problem on the one hand and can solve the static and dynamic obstacle avoidance problems between multi-robot in real time on the other hand. Our new MRCDRL algorithm has higher accuracy and robustness than DQN and DDQN and can be effectively applied to multi-robot collaboration.},
  doi      = {https://doi.org/10.1016/j.neucom.2020.04.028},
  file     = {:1-s2.0-S0925231220305932-main.pdf:PDF},
  groups   = {Multi-robot},
  keywords = {Cooperative control, Machine learning, Image processing},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231220305932},
}

@Article{doi_10.1177/0278364917719333,
  author   = {Javier Alonso-Mora and Stuart Baker and Daniela Rus},
  journal  = {The International Journal of Robotics Research},
  title    = {Multi-robot formation control and object transport in dynamic environments via constrained optimization},
  year     = {2017},
  number   = {9},
  pages    = {1000-1021},
  volume   = {36},
  abstract = {We present a constrained optimization method for multi-robot formation control in dynamic environments, where the robots adjust the parameters of the formation, such as size and three-dimensional orientation, to avoid collisions with static and moving obstacles, and to make progress towards their goal. We describe two variants of the algorithm, one for local motion planning and one for global path planning. The local planner first computes a large obstacle-free convex region in a neighborhood of the robots, embedded in position-time space. Then, the parameters of the formation are optimized therein by solving a constrained optimization, via sequential convex programming. The robots navigate towards the optimized formation with individual controllers that account for their dynamics. The idea is extended to global path planning by sampling convex regions in free position space and connecting them if a transition in formation is possible - computed via the constrained optimization. The path of lowest cost to the goal is then found via graph search. The method applies to ground and aerial vehicles navigating in two- and three-dimensional environments among static and dynamic obstacles, allows for reconfiguration, and is efficient and scalable with the number of robots. In particular, we consider two applications, a team of aerial vehicles navigating in formation, and a small team of mobile manipulators that collaboratively carry an object. The approach is verified in experiments with a team of three mobile manipulators and in simulations with a team of up to sixteen Micro Air Vehicles (quadrotors).},
  doi      = {10.1177/0278364917719333},
  eprint   = {https://doi.org/10.1177/0278364917719333},
  file     = {:doi_10.1177_0278364917719333 - Multi Robot Formation Control and Object Transport in Dynamic Environments Via Constrained Optimization.pdf:PDF},
  groups   = {Multi-robot},
  url      = {https://doi.org/10.1177/0278364917719333},
}

@InProceedings{pmlr-v100-lin20a,
  author    = {Lin, Juntong and Yang, Xuyun and Zheng, Peiwei and Cheng, Hui},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Connectivity Guaranteed Multi-robot Navigation via Deep Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {661--670},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {This paper considers the multi-robot navigation problem where the geometric center of a multi-robot team aims to efficiently reach the waypoint without collisions in unknown complex environments while maintaining connectivity during the navigation. A novel Deep Reinforcement Learning (DRL)-based approach is proposed to derive end-to-end policies for the multi-robot navigation problem. In order to guarantee the connectivity during the navigation, a constraint satisfying parametric function (CSPF) is proposed to represent the navigation policy. Virtual policy extended environment (VP2E), an implementation framework of the CSPF is accompanied so as to make CSPF compatible with existing DRL techniques which rely on differentiable parametric functions. Both simulations and real-world experiments of a team of 3 holonomic robots are conducted to verify the effectiveness of the proposed DRL-based navigation method.},
  file      = {:pmlr-v100-lin20a - Connectivity Guaranteed Multi Robot Navigation Via Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-robot},
  pdf       = {http://proceedings.mlr.press/v100/lin20a/lin20a.pdf},
  url       = {http://proceedings.mlr.press/v100/lin20a.html},
}

@InProceedings{Long2018,
  author    = {Long, Pinxin and Fan, Tingxiang and Liao, Xinyi and Liu, Wenxi and Zhang, Hao and Pan, Jia},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning},
  year      = {2018},
  pages     = {6252-6259},
  doi       = {10.1109/ICRA.2018.8461113},
  file      = {:8461113 - Towards Optimally Decentralized Multi Robot Collision Avoidance Via Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-robot},
  ranking   = {rank4},
}

@Article{8744599,
  author  = {Viseras, Alberto and Garcia, Ricardo},
  journal = {IEEE Robotics and Automation Letters},
  title   = {DeepIG: Multi-Robot Information Gathering With Deep Reinforcement Learning},
  year    = {2019},
  number  = {3},
  pages   = {3059-3066},
  volume  = {4},
  doi     = {10.1109/LRA.2019.2924839},
  file    = {:8744599 - DeepIG_ Multi Robot Information Gathering with Deep Reinforcement Learning.pdf:PDF},
  groups  = {Multi-robot, Information gathering},
}

@InProceedings{9288300,
  author    = {Yao, Shunyi and Chen, Guangda and Pan, Lifan and Ma, Jun and Ji, Jianmin and Chen, Xiaoping},
  booktitle = {2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)},
  title     = {Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning},
  year      = {2020},
  pages     = {532-539},
  doi       = {10.1109/ICTAI50040.2020.00088},
  file      = {:9288300 - Multi Robot Collision Avoidance with Map Based Deep Reinforcement Learning.pdf:PDF},
  groups    = {Multi-robot},
}

@Article{Ma2020,
  author  = {Ma, Junchong and Lu, Huimin and Xiao, Junhao and Zeng, Zhiwen and Zheng, Zhiqiang},
  journal = {Journal of Intelligent & Robotic Systems},
  title   = {Multi-robot Target Encirclement Control with Collision Avoidance via Deep Reinforcement Learning},
  year    = {2020},
  month   = {08},
  volume  = {99},
  doi     = {10.1007/s10846-019-01106-x},
  file    = {:Ma2020 - Multi Robot Target Encirclement Control with Collision Avoidance Via Deep Reinforcement Learning.pdf:PDF},
  groups  = {Multi-robot},
}

@Article{7812634,
  author  = {Long, Pinxin and Liu, Wenxi and Pan, Jia},
  journal = {IEEE Robotics and Automation Letters},
  title   = {Deep-Learned Collision Avoidance Policy for Distributed Multiagent Navigation},
  year    = {2017},
  number  = {2},
  pages   = {656-663},
  volume  = {2},
  doi     = {10.1109/LRA.2017.2651371},
  file    = {:7812634 - Deep Learned Collision Avoidance Policy for Distributed Multiagent Navigation.pdf:PDF},
  groups  = {Multi-robot},
}

@Article{pierson2017deep,
  author    = {Pierson, Harry A and Gashler, Michael S},
  journal   = {Advanced Robotics},
  title     = {Deep learning in robotics: a review of recent research},
  year      = {2017},
  number    = {16},
  pages     = {821--835},
  volume    = {31},
  file      = {:pierson2017deep - Deep Learning in Robotics_ a Review of Recent Research.pdf:PDF},
  groups    = {MADRL in Robotics, Robotics},
  publisher = {Taylor \& Francis},
}

@InProceedings{elfakharany2020towards,
  author       = {Elfakharany, A and Yusof, R and Ismail, Z},
  booktitle    = {Journal of Physics: Conference Series},
  title        = {Towards Multi Robot Task Allocation and Navigation using Deep Reinforcement Learning},
  year         = {2020},
  number       = {1},
  organization = {IOP Publishing},
  pages        = {012045},
  volume       = {1447},
  file         = {:elfakharany2020towards - Towards Multi Robot Task Allocation and Navigation Using Deep Reinforcement Learning.pdf:PDF},
  groups       = {Multi-robot},
}

@InProceedings{7989250,
  author    = {Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Learning modular neural network policies for multi-task and multi-robot transfer},
  year      = {2017},
  pages     = {2169-2176},
  doi       = {10.1109/ICRA.2017.7989250},
  file      = {:7989250 - Learning Modular Neural Network Policies for Multi Task and Multi Robot Transfer.pdf:PDF},
  groups    = {Multi-robot, Multi-task robotics},
  ranking   = {rank2},
}

@InProceedings{Kahn2018,
  author    = {Kahn, Gregory and Villaflor, Adam and Ding, Bosen and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation},
  year      = {2018},
  pages     = {5129-5136},
  doi       = {10.1109/ICRA.2018.8460655},
  file      = {:8460655 - Self Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank2},
}

@InProceedings{pmlr-v100-yu20a,
  author    = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {1094--1100},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multitask learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.1.},
  file      = {:pmlr-v100-yu20a - Meta World_ a Benchmark and Evaluation for Multi Task and Meta Reinforcement Learning.pdf:PDF},
  groups    = {Multi-task robotics},
  pdf       = {http://proceedings.mlr.press/v100/yu20a/yu20a.pdf},
  url       = {http://proceedings.mlr.press/v100/yu20a.html},
}

@InProceedings{8202141,
  author    = {Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Collective robot reinforcement learning with distributed asynchronous guided policy search},
  year      = {2017},
  pages     = {79-86},
  doi       = {10.1109/IROS.2017.8202141},
  file      = {:8202141 - Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.pdf:PDF},
  groups    = {Multi-robot},
}

@InProceedings{pmlr-v100-dasari20a,
  author    = {Dasari, Sudeep and Ebert, Frederik and Tian, Stephen and Nair, Suraj and Bucher, Bernadette and Schmeckpeper, Karl and Singh, Siddharth and Levine, Sergey and Finn, Chelsea},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {RoboNet: Large-Scale Multi-Robot Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {885--897},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms’ ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data.1},
  file      = {:pmlr-v100-dasari20a - RoboNet_ Large Scale Multi Robot Learning.pdf:PDF},
  groups    = {Multi-robot},
  pdf       = {http://proceedings.mlr.press/v100/dasari20a/dasari20a.pdf},
  url       = {http://proceedings.mlr.press/v100/dasari20a.html},
}

@InProceedings{pmlr-v100-ahn20a,
  author    = {Ahn, Michael and Zhu, Henry and Hartikainen, Kristian and Ponte, Hugo and Gupta, Abhishek and Levine, Sergey and Kumar, Vikash},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {1300--1313},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {ROBEL is an open-source platform of cost-effective robots designed for reinforcement learning in the real world. ROBEL introduces two robots, each aimed to accelerate reinforcement learning research in different task domains: D’Claw is a three-fingered hand robot that facilitates learning dexterous manipulation tasks, and D’Kitty is a four-legged robot that facilitates learning agile legged locomotion tasks. These low-cost, modular robots are easy to maintain and are robust enough to sustain on-hardware reinforcement learning from scratch with over 14000 training hours registered on them to date. To leverage this platform, we propose an extensible set of continuous control benchmark tasks for each robot. These tasks feature dense and sparse task objectives, and additionally introduce score metrics for hardware-safety. We provide benchmark scores on an initial set of tasks using a variety of learning-based methods. Furthermore, we show that these results can be replicated across copies of the robots located in different institutions. Code, documentation, design files, detailed assembly instructions, trained policies, baseline details, task videos, and all supplementary materials required to reproduce the results are available at www.roboticsbenchmarks.org.},
  file      = {:pmlr-v100-ahn20a - ROBEL_ Robotics Benchmarks for Learning with Low Cost Robots.pdf:PDF},
  groups    = {Robotics},
  pdf       = {http://proceedings.mlr.press/v100/ahn20a/ahn20a.pdf},
  url       = {http://proceedings.mlr.press/v100/ahn20a.html},
}

@InProceedings{pmlr-v87-kahn18a,
  author    = {Kahn, Gregory and Villaflor, Adam and Abbeel, Pieter and Levine, Sergey},
  booktitle = {Proceedings of The 2nd Conference on Robot Learning},
  title     = {Composable Action-Conditioned Predictors: Flexible Off-Policy Learning for Robot Navigation},
  year      = {2018},
  editor    = {Aude Billard and Anca Dragan and Jan Peters and Jun Morimoto},
  month     = {29--31 Oct},
  pages     = {806--816},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {87},
  abstract  = {A general-purpose intelligent robot must be able to learn autonomously and be able to accomplish multiple tasks in order to be deployed in the real world. However, standard reinforcement learning approaches learn separate task-specific policies and assume the reward function for each task is known a priori. We propose a framework that learns event cues from off-policy data, and can flexibly combine these event cues at test time to accomplish different tasks. These event cue labels are not assumed to be known a priori, but are instead labeled using learned models, such as computer vision detectors, and then “backed up” in time using an action-conditioned predictive model. We show that a simulated robotic car and a real-world RC car can gather data and train fully autonomously without any human-provided labels beyond those needed to train the detectors, and then at test-time be able to accomplish a variety of different tasks. Videos of the experiments and code can be found at github.com/gkahn13/CAPs},
  file      = {:pmlr-v87-kahn18a - Composable Action Conditioned Predictors_ Flexible off Policy Learning for Robot Navigation.pdf:PDF},
  groups    = {Navigation},
  pdf       = {http://proceedings.mlr.press/v87/kahn18a/kahn18a.pdf},
  url       = {http://proceedings.mlr.press/v87/kahn18a.html},
}

@Article{Ibarz2021_Howto,
  author     = {Julian Ibarz and Jie Tan and Chelsea Finn and Mrinal Kalakrishnan and Peter Pastor and Sergey Levine},
  journal    = {The International Journal of Robotics Research},
  title      = {How to train your robot with deep reinforcement learning: lessons we have learned},
  year       = {2021},
  number     = {4-5},
  volume     = {40},
  abstract   = {Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low-level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time, real-world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn: as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.},
  doi        = {10.1177/0278364920987859},
  eprint     = {https://doi.org/10.1177/0278364920987859},
  file       = {:0278364920987859.pdf:PDF},
  groups     = {RL in Robotics},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1177/0278364920987859},
}

@InProceedings{Zhang2016,
  author    = {Zhang, Tianhao and Kahn, Gregory and Levine, Sergey and Abbeel, Pieter},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search},
  year      = {2016},
  pages     = {528-535},
  doi       = {10.1109/ICRA.2016.7487175},
  file      = {:7487175 - Learning Deep Control Policies for Autonomous Aerial Vehicles with MPC Guided Policy Search.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank4},
}

@InProceedings{charrow2015information,
  author       = {Charrow, Benjamin and Kahn, Gregory and Patil, Sachin and Liu, Sikang and Goldberg, Ken and Abbeel, Pieter and Michael, Nathan and Kumar, Vijay},
  booktitle    = {Robotics: Science and Systems},
  title        = {Information-Theoretic Planning with Trajectory Optimization for Dense 3D Mapping},
  year         = {2015},
  organization = {Rome},
  pages        = {3--12},
  volume       = {11},
  file         = {:charrow2015information - Information Theoretic Planning with Trajectory Optimization for Dense 3D Mapping..pdf:PDF},
  groups       = {Information gathering},
}

@InProceedings{7989379,
  author    = {Kahn, Gregory and Zhang, Tianhao and Levine, Sergey and Abbeel, Pieter},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {PLATO: Policy learning using adaptive trajectory optimization},
  year      = {2017},
  month     = {May},
  pages     = {3342-3349},
  abstract  = {Policy search can in principle acquire complex strategies for control of robots and other autonomous systems. When the policy is trained to process raw sensory inputs, such as images and depth maps, it can also acquire a strategy that combines perception and control. However, effectively processing such complex inputs requires an expressive policy class, such as a large neural network. These high-dimensional policies are difficult to train, especially when learning to control safety-critical systems. We propose PLATO, a continuous, reset-free reinforcement learning algorithm that trains complex control policies with supervised learning, using model-predictive control (MPC) to generate the supervision, hence never in need of running a partially trained and potentially unsafe policy. PLATO uses an adaptive training method to modify the behavior of MPC to gradually match the learned policy in order to generate training samples at states that are likely to be visited by the learned policy. PLATO also maintains the MPC cost as an objective to avoid highly undesirable actions that would result from strictly following the learned policy before it has been fully trained. We prove that this type of adaptive MPC expert produces supervision that leads to good long-horizon performance of the resulting policy. We also empirically demonstrate that MPC can still avoid dangerous on-policy actions in unexpected situations during training. Our empirical results on a set of challenging simulated aerial vehicle tasks demonstrate that, compared to prior methods, PLATO learns faster, experiences substantially fewer catastrophic failures (crashes) during training, and often converges to a better policy.},
  doi       = {10.1109/ICRA.2017.7989379},
  file      = {:7989379 - PLATO_ Policy Learning Using Adaptive Trajectory Optimization.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank1},
}

@InProceedings{Kang2019,
  author    = {Kang, Katie and Belkhale, Suneel and Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title     = {Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight},
  year      = {2019},
  month     = {May},
  pages     = {6008-6014},
  abstract  = {Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS.},
  doi       = {10.1109/ICRA.2019.8793735},
  file      = {:8793735 - Generalization through Simulation_ Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision Based Autonomous Flight.pdf:PDF},
  groups    = {Navigation, Sim to Real},
  issn      = {2577-087X},
  ranking   = {rank1},
}

@Article{9345970,
  author   = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {BADGR: An Autonomous Self-Supervised Learning-Based Navigation System},
  year     = {2021},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {1312-1319},
  volume   = {6},
  abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our reinforcement learning approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with autonomously-labeled off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr.},
  doi      = {10.1109/LRA.2021.3057023},
  file     = {:9345970 - BADGR_ an Autonomous Self Supervised Learning Based Navigation System.pdf:PDF},
  groups   = {Navigation},
}

@Article{Queralta2020,
  author        = {Jorge Peña Queralta and Jussi Taipalmaa and Bilge Can Pullinen and Victor Kathan Sarker and Tuan Nguyen Gia and Hannu Tenhunen and Moncef Gabbouj and Jenni Raitoharju and Tomi Westerlund},
  title         = {Collaborative Multi-Robot Systems for Search and Rescue: Coordination and Perception},
  year          = {2020},
  month         = aug,
  abstract      = {Autonomous or teleoperated robots have been playing increasingly important roles in civil applications in recent years. Across the different civil domains where robots can support human operators, one of the areas where they can have more impact is in search and rescue (SAR) operations. In particular, multi-robot systems have the potential to significantly improve the efficiency of SAR personnel with faster search of victims, initial assessment and mapping of the environment, real-time monitoring and surveillance of SAR operations, or establishing emergency communication networks, among other possibilities. SAR operations encompass a wide variety of environments and situations, and therefore heterogeneous and collaborative multi-robot systems can provide the most advantages. In this paper, we review and analyze the existing approaches to multi-robot SAR support, from an algorithmic perspective and putting an emphasis on the methods enabling collaboration among the robots as well as advanced perception through machine vision and multi-agent active perception. Furthermore, we put these algorithms in the context of the different challenges and constraints that various types of robots (ground, aerial, surface or underwater) encounter in different SAR environments (maritime, urban, wilderness or other post-disaster scenarios). This is, to the best of our knowledge, the first review considering heterogeneous SAR robots across different environments, while giving two complimentary points of view: control mechanisms and machine perception. Based on our review of the state-of-the-art, we discuss the main open research questions, and outline our insights on the current approaches that have potential to improve the real-world performance of multi-robot SAR systems.},
  archiveprefix = {arXiv},
  eprint        = {2008.12610},
  file          = {:Queralta2020 - Collaborative Multi Robot Systems for Search and Rescue_ Coordination and Perception.pdf:PDF},
  groups        = {Multi-robot, Information gathering, Heterogeneous},
  keywords      = {cs.RO, cs.LG, cs.MA},
  primaryclass  = {cs.RO},
  ranking       = {rank2},
}

@Article{9392290,
  author   = {Thananjeyan, Brijen and Balakrishna, Ashwin and Nair, Suraj and Luo, Michael and Srinivasan, Krishnan and Hwang, Minho and Gonzalez, Joseph E. and Ibarz, Julian and Finn, Chelsea and Goldberg, Ken},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones},
  year     = {2021},
  issn     = {2377-3766},
  month    = {July},
  number   = {3},
  pages    = {4915-4922},
  volume   = {6},
  abstract = {Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2–20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.},
  doi      = {10.1109/LRA.2021.3070252},
  file     = {:9392290 - Recovery RL_ Safe Reinforcement Learning with Learned Recovery Zones.pdf:PDF},
  groups   = {RL in Robotics},
}

@Article{almadhoun2019survey,
  author    = {Almadhoun, Randa and Taha, Tarek and Seneviratne, Lakmal and Zweiri, Yahya},
  journal   = {SN Applied Sciences},
  title     = {A survey on multi-robot coverage path planning for model reconstruction and mapping},
  year      = {2019},
  number    = {8},
  pages     = {1--24},
  volume    = {1},
  file      = {:almadhoun2019survey - A Survey on Multi Robot Coverage Path Planning for Model Reconstruction and Mapping.pdf:PDF},
  groups    = {Multi-robot, Coverage Path Planning},
  publisher = {Springer},
}

@InProceedings{8202312,
  author    = {Chen, Yu Fan and Everett, Michael and Liu, Miao and How, Jonathan P.},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Socially aware motion planning with deep reinforcement learning},
  year      = {2017},
  month     = {Sep.},
  pages     = {1343-1350},
  abstract  = {For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules (e.g., passing on the right). However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.},
  doi       = {10.1109/IROS.2017.8202312},
  file      = {:8202312 - Socially Aware Motion Planning with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation, Multi-robot},
  issn      = {2153-0866},
  ranking   = {rank2},
}

@InProceedings{Chen2017,
  author    = {Chen, Yu Fan and Liu, Miao and Everett, Michael and How, Jonathan P.},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning},
  year      = {2017},
  month     = {May},
  pages     = {285-292},
  abstract  = {Finding feasible, collision-free paths for multiagent systems can be challenging, particularly in non-communicating scenarios where each agent's intent (e.g. goal) is unobservable to the others. In particular, finding time efficient paths often requires anticipating interaction with neighboring agents, the process of which can be computationally prohibitive. This work presents a decentralized multiagent collision avoidance algorithm based on a novel application of deep reinforcement learning, which effectively offloads the online computation (for predicting interaction patterns) to an offline learning procedure. Specifically, the proposed approach develops a value network that encodes the estimated time to the goal given an agent's joint configuration (positions and velocities) with its neighbors. Use of the value network not only admits efficient (i.e., real-time implementable) queries for finding a collision-free velocity vector, but also considers the uncertainty in the other agents' motion. Simulation results show more than 26% improvement in paths quality (i.e., time to reach the goal) when compared with optimal reciprocal collision avoidance (ORCA), a state-of-the-art collision avoidance strategy.},
  doi       = {10.1109/ICRA.2017.7989037},
  file      = {:7989037 - Decentralized Non Communicating Multiagent Collision Avoidance with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation, Multi-robot},
  ranking   = {rank2},
}

@InProceedings{7989179,
  author    = {Paull, Liam and Tani, Jacopo and Ahn, Heejin and Alonso-Mora, Javier and Carlone, Luca and Cap, Michal and Chen, Yu Fan and Choi, Changhyun and Dusek, Jeff and Fang, Yajun and Hoehener, Daniel and Liu, Shih-Yuan and Novitzky, Michael and Okuyama, Igor Franzoni and Pazis, Jason and Rosman, Guy and Varricchio, Valerio and Wang, Hsueh-Cheng and Yershov, Dmitry and Zhao, Hang and Benjamin, Michael and Carr, Christopher and Zuber, Maria and Karaman, Sertac and Frazzoli, Emilio and Del Vecchio, Domitilla and Rus, Daniela and How, Jonathan and Leonard, John and Censi, Andrea},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Duckietown: An open, inexpensive and flexible platform for autonomy education and research},
  year      = {2017},
  month     = {May},
  pages     = {1497-1504},
  abstract  = {Duckietown is an open, inexpensive and flexible platform for autonomy education and research. The platform comprises small autonomous vehicles (“Duckiebots”) built from off-the-shelf components, and cities (“Duckietowns”) complete with roads, signage, traffic lights, obstacles, and citizens (duckies) in need of transportation. The Duckietown platform offers a wide range of functionalities at a low cost. Duckiebots sense the world with only one monocular camera and perform all processing onboard with a Raspberry Pi 2, yet are able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots, localize within a global map, navigate a city, and coordinate with other Duckiebots to avoid collisions. Duckietown is a useful tool since educators and researchers can save money and time by not having to develop all of the necessary supporting infrastructure and capabilities. All materials are available as open source, and the hope is that others in the community will adopt the platform for education and research.},
  doi       = {10.1109/ICRA.2017.7989179},
  file      = {:7989179 - Duckietown_ an Open, Inexpensive and Flexible Platform for Autonomy Education and Research.pdf:PDF},
  groups    = {Platforms},
}

@InProceedings{7989677,
  author    = {Lopez, Brett T. and How, Jonathan P.},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Aggressive 3-D collision avoidance for high-speed navigation},
  year      = {2017},
  month     = {May},
  pages     = {5759-5765},
  abstract  = {Autonomous robot navigation through unknown, cluttered environments at high-speeds is still an open problem. Quadrotor platforms with this capability have only begun to emerge with the advancements in light-weight, small form factor sensing and computing. Many of the existing platforms, however, require excessive computation time to perform collision avoidance, which ultimately limits the vehicle's top speed. This work presents an efficient perception and planning approach that significantly reduces the computation time by using instantaneous perception data for collision avoidance. Minimum-time, state and input constrained motion primitives are generated by sampling terminal states until a collision-free path is found. The worst case performance of the Triple Integrator Planner (TIP) is nearly an order of magnitude faster than the state-of-the-art. Experimental results demonstrate the algorithm's ability to plan and execute aggressive collision avoidance maneuvers in highly cluttered environments.},
  doi       = {10.1109/ICRA.2017.7989677},
  file      = {:7989677 - Aggressive 3 D Collision Avoidance for High Speed Navigation.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank1},
}

@Article{doi_10.1177/0278364917692864,
  author   = {Shayegan Omidshafiei and Ali–Akbar Agha–Mohammadi and Christopher Amato and Shih–Yuan Liu and Jonathan P How and John Vian},
  journal  = {The International Journal of Robotics Research},
  title    = {Decentralized control of multi-robot partially observable Markov decision processes using belief space macro-actions},
  year     = {2017},
  number   = {2},
  pages    = {231-258},
  volume   = {36},
  abstract = {This work focuses on solving general multi-robot planning problems in continuous spaces with partial observability given a high-level domain description. Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are general models for multi-robot coordination problems. However, representing and solving Dec-POMDPs is often intractable for large problems. This work extends the Dec-POMDP model to the Decentralized Partially Observable Semi-Markov Decision Process (Dec-POSMDP) to take advantage of the high-level representations that are natural for multi-robot problems and to facilitate scalable solutions to large discrete and continuous problems. The Dec-POSMDP formulation uses task macro-actions created from lower-level local actions that allow for asynchronous decision-making by the robots, which is crucial in multi-robot domains. This transformation from Dec-POMDPs to Dec-POSMDPs with a finite set of automatically-generated macro-actions allows use of efficient discrete-space search algorithms to solve them. The paper presents algorithms for solving Dec-POSMDPs, which are more scalable than previous methods since they can incorporate closed-loop belief space macro-actions in planning. These macro-actions are automatically constructed to produce robust solutions. The proposed algorithms are then evaluated on a complex multi-robot package delivery problem under uncertainty, showing that our approach can naturally represent realistic problems and provide high-quality solutions for large-scale problems.},
  doi      = {10.1177/0278364917692864},
  eprint   = {https://doi.org/10.1177/0278364917692864},
  file     = {:doi_10.1177_0278364917692864 - Decentralized Control of Multi Robot Partially Observable Markov Decision Processes Using Belief Space Macro Actions.pdf:PDF},
  groups   = {Multi-robot, Macro-actions},
  url      = {https://doi.org/10.1177/0278364917692864},
}

@Article{amato2019modeling,
  author     = {Amato, Christopher and Konidaris, George and Kaelbling, Leslie P and How, Jonathan P},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {Modeling and planning with macro-actions in decentralized POMDPs},
  year       = {2019},
  pages      = {817--859},
  volume     = {64},
  file       = {:amato2019modeling - Modeling and Planning with Macro Actions in Decentralized POMDPs (1).pdf:PDF},
  groups     = {Multi-robot, Macro-actions},
  readstatus = {skimmed},
}

@Article{Semnani2020,
  author   = {Semnani, Samaneh Hosseini and Liu, Hugh and Everett, Michael and de Ruiter, Anton and How, Jonathan P.},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Multi-Agent Motion Planning for Dense and Dynamic Environments via Deep Reinforcement Learning},
  year     = {2020},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {3221-3226},
  volume   = {5},
  abstract = {This letter introduces a hybrid algorithm of deep reinforcement learning (RL) and Force-based motion planning (FMP) to solve distributed motion planning problem in dense and dynamic environments. Individually, RL and FMP algorithms each have their own limitations. FMP is not able to produce time-optimal paths and existing RL solutions are not able to produce collision free paths in dense environments. Therefore, we first tried improving the performance of recent RL approaches by introducing a new reward function that not only eliminates the requirement of a pre supervised learning (SL) step but also decreases the chance of collision in crowded environments. That improved things, but there were still a lot of failure cases. So, we developed a hybrid approach to leverage the simpler FMP approach in stuck, simple and high-risk cases, and continue using RL for normal cases in which FMP can't produce optimal path. Also, we extend GA3CCADRL algorithm to 3D environment. Simulation results show that the proposed algorithm outperforms both deep RL and FMP algorithms and produces up to 50% more successful scenarios than deep RL and up to 75% less extra time to reach goal than FMP.},
  comment  = {GA3C-CADRL / GA3C-CADRL-NSL},
  doi      = {10.1109/LRA.2020.2974695},
  file     = {:9001226 - Multi Agent Motion Planning for Dense and Dynamic Environments Via Deep Reinforcement Learning.pdf:PDF},
  groups   = {Multi-robot},
  ranking  = {rank3},
}

@InProceedings{Everett2018,
  author    = {Everett, Michael and Chen, Yu Fan and How, Jonathan P.},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning},
  year      = {2018},
  month     = {Oct},
  pages     = {3052-3059},
  abstract  = {Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efficient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents' behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a fixed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed.},
  comment   = {GA3C-CADRL},
  doi       = {10.1109/IROS.2018.8593871},
  file      = {:8593871 - Motion Planning among Dynamic, Decision Making Agents with Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation},
  issn      = {2153-0866},
  ranking   = {rank2},
}

@Article{Everett2021,
  author   = {Everett, Michael and Chen, Yu Fan and How, Jonathan P.},
  journal  = {IEEE Access},
  title    = {Collision Avoidance in Pedestrian-Rich Environments With Deep Reinforcement Learning},
  year     = {2021},
  issn     = {2169-3536},
  pages    = {10357-10377},
  volume   = {9},
  abstract = {Collision avoidance algorithms are essential for safe and efficient robot operation among pedestrians. This work proposes using deep reinforcement (RL) learning as a framework to model the complex interactions and cooperation with nearby, decision-making agents, such as pedestrians and other robots. Existing RL-based works assume homogeneity of agent properties, use specific motion models over short timescales, or lack a principled method to handle a large, possibly varying number of agents. Therefore, this work develops an algorithm that learns collision avoidance among a variety of heterogeneous, non-communicating, dynamic agents without assuming they follow any particular behavior rules. It extends our previous work by introducing a strategy using Long Short-Term Memory (LSTM) that enables the algorithm to use observations of an arbitrary number of other agents, instead of a small, fixed number of neighbors. The proposed algorithm is shown to outperform a classical collision avoidance algorithm, another deep RL-based algorithm, and scales with the number of agents better (fewer collisions, shorter time to goal) than our previously published learning-based approach. Analysis of the LSTM provides insights into how observations of nearby agents affect the hidden state and quantifies the performance impact of various agent ordering heuristics. The learned policy generalizes to several applications beyond the training scenarios: formation control (arrangement into letters), demonstrations on a fleet of four multirotors and on a fully autonomous robotic vehicle capable of traveling at human walking speed among pedestrians.},
  doi      = {10.1109/ACCESS.2021.3050338},
  file     = {:09317723.pdf:PDF},
  groups   = {Navigation},
}

@Article{Kahn2017a,
  author        = {Gregory Kahn and Adam Villaflor and Vitchyr Pong and Pieter Abbeel and Sergey Levine},
  title         = {Uncertainty-Aware Reinforcement Learning for Collision Avoidance},
  year          = {2017},
  month         = feb,
  abstract      = {Reinforcement learning can enable complex, adaptive behavior to be learned automatically for autonomous robotic platforms. However, practical deployment of reinforcement learning methods must contend with the fact that the training process itself can be unsafe for the robot. In this paper, we consider the specific case of a mobile robot learning to navigate an a priori unknown environment while avoiding collisions. In order to learn collision avoidance, the robot must experience collisions at training time. However, high-speed collisions, even at training time, could damage the robot. A successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. To this end, we present an uncertainty-aware model-based learning algorithm that estimates the probability of collision together with a statistical estimate of uncertainty. By formulating an uncertainty-dependent cost function, we show that the algorithm naturally chooses to proceed cautiously in unfamiliar environments, and increases the velocity of the robot in settings where it has high confidence. Our predictive model is based on bootstrapped neural networks using dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. Our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time in an obstacle avoidance task for a simulated and real-world quadrotor, and a real-world RC car. Videos of the experiments can be found at https://sites.google.com/site/probcoll.},
  archiveprefix = {arXiv},
  eprint        = {1702.01182},
  file          = {:- Uncertainty Aware Reinforcement Learning for Collision Avoidance.pdf:PDF},
  groups        = {Navigation},
  keywords      = {cs.LG, cs.RO},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
}

@InProceedings{Tai2017,
  author    = {Tai, Lei and Paolo, Giuseppe and Liu, Ming},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation},
  year      = {2017},
  month     = {Sep.},
  pages     = {31-36},
  abstract  = {We present a learning-based mapless motion planner by taking the sparse 10-dimensional range findings and the target position with respect to the mobile robot coordinate frame as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the obstacle map of the navigation environment where both the highly precise laser sensor and the obstacle map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.},
  doi       = {10.1109/IROS.2017.8202134},
  file      = {:8202134 - Virtual to Real Deep Reinforcement Learning_ Continuous Control of Mobile Robots for Mapless Navigation.pdf:PDF},
  groups    = {Navigation, Sim to Real},
  issn      = {2153-0866},
  ranking   = {rank2},
}

@InProceedings{Tobin2017_RealityGap,
  author    = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Domain randomization for transferring deep neural networks from simulation to the real world},
  year      = {2017},
  month     = {Sep.},
  pages     = {23-30},
  abstract  = {Bridging the `reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  doi       = {10.1109/IROS.2017.8202133},
  file      = {:8202133 - Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.pdf:PDF},
  groups    = {Sim to Real},
  issn      = {2153-0866},
  ranking   = {rank5},
}

@InProceedings{Zhu2017,
  author    = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Target-driven visual navigation in indoor scenes using deep reinforcement learning},
  year      = {2017},
  month     = {May},
  pages     = {3357-3364},
  abstract  = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.},
  doi       = {10.1109/ICRA.2017.7989381},
  file      = {:7989381 - Target Driven Visual Navigation in Indoor Scenes Using Deep Reinforcement Learning.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank4},
}

@Article{Doncieux2020,
  author        = {Stephane Doncieux and Nicolas Bredeche and Léni Le Goff and Benoît Girard and Alexandre Coninx and Olivier Sigaud and Mehdi Khamassi and Natalia Díaz-Rodríguez and David Filliat and Timothy Hospedales and A. Eiben and Richard Duro},
  title         = {DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics},
  year          = {2020},
  month         = may,
  abstract      = {Robots are still limited to controlled conditions, that the robot designer knows with enough details to endow the robot with the appropriate models or behaviors. Learning algorithms add some flexibility with the ability to discover the appropriate behavior given either some demonstrations or a reward to guide its exploration with a reinforcement learning algorithm. Reinforcement learning algorithms rely on the definition of state and action spaces that define reachable behaviors. Their adaptation capability critically depends on the representations of these spaces: small and discrete spaces result in fast learning while large and continuous spaces are challenging and either require a long training period or prevent the robot from converging to an appropriate behavior. Beside the operational cycle of policy execution and the learning cycle, which works at a slower time scale to acquire new policies, we introduce the redescription cycle, a third cycle working at an even slower time scale to generate or adapt the required representations to the robot, its environment and the task. We introduce the challenges raised by this cycle and we present DREAM (Deferred Restructuring of Experience in Autonomous Machines), a developmental cognitive architecture to bootstrap this redescription process stage by stage, build new state representations with appropriate motivations, and transfer the acquired knowledge across domains or tasks or even across robots. We describe results obtained so far with this approach and end up with a discussion of the questions it raises in Neuroscience.},
  archiveprefix = {arXiv},
  eprint        = {2005.06223},
  file          = {:Doncieux2020 - DREAM Architecture_ a Developmental Approach to Open Ended Learning in Robotics.pdf:PDF},
  groups        = {Multi-task robotics},
  keywords      = {cs.AI, cs.LG, cs.NE, cs.RO},
  primaryclass  = {cs.AI},
}

@Article{JUN2016325,
  author   = {Jae-Yun Jun and Jean-Philippe Saut and Faïz Benamar},
  journal  = {Robotics and Autonomous Systems},
  title    = {Pose estimation-based path planning for a tracked mobile robot traversing uneven terrains},
  year     = {2016},
  issn     = {0921-8890},
  pages    = {325-339},
  volume   = {75},
  abstract = {A novel path-planning algorithm is proposed for a tracked mobile robot to traverse uneven terrains, which can efficiently search for stability sub-optimal paths. This algorithm consists of combining two RRT-like algorithms (the Transition-based RRT (T-RRT) and the Dynamic-Domain RRT (DD-RRT) algorithms) bidirectionally and of representing the robot–terrain interaction with the robot’s quasi-static tip-over stability measure (assuming that the robot traverses uneven terrains at low speed for safety). The robot’s stability is computed by first estimating the robot’s pose, which in turn is interpreted as a contact problem, formulated as a linear complementarity problem (LCP), and solved using the Lemke’s method (which guarantees a fast convergence). The present work compares the performance of the proposed algorithm to other RRT-like algorithms (in terms of planning time, rate of success in finding solutions and the associated cost values) over various uneven terrains and shows that the proposed algorithm can be advantageous over its counterparts in various aspects of the planning performance.},
  doi      = {https://doi.org/10.1016/j.robot.2015.09.014},
  file     = {:JUN2016325 - Pose Estimation Based Path Planning for a Tracked Mobile Robot Traversing Uneven Terrains.pdf:PDF},
  groups   = {Navigation},
  keywords = {Path planning, Rough terrain, Sampling-based motion planning, Mobile robot, Tip-over stability},
  url      = {https://www.sciencedirect.com/science/article/pii/S092188901500202X},
}

@Article{Sunderhauf2018_Limits,
  author     = {Niko Sünderhauf and Oliver Brock and Walter Scheirer and Raia Hadsell and Dieter Fox and Jürgen Leitner and Ben Upcroft and Pieter Abbeel and Wolfram Burgard and Michael Milford and Peter Corke},
  journal    = {The International Journal of Robotics Research},
  title      = {The limits and potentials of deep learning for robotics},
  year       = {2018},
  number     = {4-5},
  pages      = {405-420},
  volume     = {37},
  abstract   = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.},
  doi        = {10.1177/0278364918770733},
  eprint     = {https://doi.org/10.1177/0278364918770733},
  file       = {:doi_10.1177_0278364918770733 - The Limits and Potentials of Deep Learning for Robotics.pdf:PDF},
  groups     = {RL in Robotics},
  ranking    = {rank3},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1177/0278364918770733},
}

@Article{charrow2014approximate,
  author    = {Charrow, Benjamin and Kumar, Vijay and Michael, Nathan},
  journal   = {Autonomous Robots},
  title     = {Approximate representations for multi-robot control policies that maximize mutual information},
  year      = {2014},
  number    = {4},
  pages     = {383--400},
  volume    = {37},
  file      = {:charrow2014approximate - Approximate Representations for Multi Robot Control Policies That Maximize Mutual Information.pdf:PDF},
  groups    = {Information gathering, Multi-robot},
  publisher = {Springer},
}

@InCollection{schwager2017multi,
  author    = {Schwager, Mac and Dames, Philip and Rus, Daniela and Kumar, Vijay},
  booktitle = {Robotics research},
  publisher = {Springer},
  title     = {A multi-robot control policy for information gathering in the presence of unknown hazards},
  year      = {2017},
  pages     = {455--472},
  file      = {:schwager2017multi - A Multi Robot Control Policy for Information Gathering in the Presence of Unknown Hazards.pdf:PDF},
  groups    = {Information gathering, Multi-robot},
}

@InProceedings{7139865,
  author    = {Charrow, Benjamin and Liu, Sikang and Kumar, Vijay and Michael, Nathan},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Information-theoretic mapping using Cauchy-Schwarz Quadratic Mutual Information},
  year      = {2015},
  month     = {May},
  pages     = {4791-4798},
  abstract  = {We develop a computationally efficient control policy for active perception that incorporates explicit models of sensing and mobility to build 3D maps with ground and aerial robots. Like previous work, our policy maximizes an information-theoretic objective function between the discrete occupancy belief distribution (e.g., voxel grid) and future measurements that can be made by mobile sensors. However, our work is unique in three ways. First, we show that by using Cauchy-Schwarz Quadratic Mutual Information (CSQMI), we get significant gains in efficiency. Second, while most previous methods adopt a myopic, gradient-following approach that yields poor convergence properties, our algorithm searches over a set of paths and is less susceptible to local minima. In doing so, we explicitly incorporate models of sensors, and model the dependence (and independence) of measurements over multiple time steps in a path. Third, because we consider models of sensing and mobility, our method naturally applies to both ground and aerial vehicles. The paper describes the basic models, the problem formulation and the algorithm, and demonstrates applications via simulation and experimentation.},
  doi       = {10.1109/ICRA.2015.7139865},
  file      = {:7139865 - Information Theoretic Mapping Using Cauchy Schwarz Quadratic Mutual Information.pdf:PDF},
  groups    = {Information gathering},
  issn      = {1050-4729},
}

@InProceedings{Roberts_2017_ICCV,
  author    = {Roberts, Mike and Dey, Debadeepta and Truong, Anh and Sinha, Sudipta and Shah, Shital and Kapoor, Ashish and Hanrahan, Pat and Joshi, Neel},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {Submodular Trajectory Optimization for Aerial 3D Scanning},
  year      = {2017},
  month     = {Oct},
  file      = {:Roberts_2017_ICCV - Submodular Trajectory Optimization for Aerial 3D Scanning.pdf:PDF},
  groups    = {Information gathering},
}

@Article{smith2018distributed,
  author    = {Smith, Andrew J and Hollinger, Geoffrey A},
  journal   = {Autonomous Robots},
  title     = {Distributed inference-based multi-robot exploration},
  year      = {2018},
  number    = {8},
  pages     = {1651--1668},
  volume    = {42},
  file      = {:auro-si-distributed__1_.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Springer},
}

@Article{8968434,
  author   = {Schmid, Lukas and Pantic, Michael and Khanna, Raghav and Ott, Lionel and Siegwart, Roland and Nieto, Juan},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {An Efficient Sampling-Based Method for Online Informative Path Planning in Unknown Environments},
  year     = {2020},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {1500-1507},
  volume   = {5},
  abstract = {The ability to plan informative paths online is essential to robot autonomy. In particular, sampling-based approaches are often used as they are capable of using arbitrary information gain formulations. However, they are prone to local minima, resulting in sub-optimal trajectories, and sometimes do not reach global coverage. In this letter, we present a new RRT*-inspired online informative path planning algorithm. Our method continuously expands a single tree of candidate trajectories and rewires nodes to maintain the tree and refine intermediate paths. This allows the algorithm to achieve global coverage and maximize the utility of a path in a global context, using a single objective function. We demonstrate the algorithm's capabilities in the applications of autonomous indoor exploration as well as accurate Truncated Signed Distance Field (TSDF)-based 3D reconstruction on-board a Micro Aerial Vehicle (MAV). We study the impact of commonly used information gain and cost formulations in these scenarios and propose a novel TSDF-based 3D reconstruction gain and cost-utility formulation. Detailed evaluation in realistic simulation environments show that our approach outperforms sampling-based state of the art methods in these tasks. Experiments on a real MAV demonstrate the ability of our method to robustly plan in real-time, exploring an indoor environment with on-board sensing and computation. We make our framework available for future research.},
  doi      = {10.1109/LRA.2020.2969191},
  file     = {:8968434 - An Efficient Sampling Based Method for Online Informative Path Planning in Unknown Environments.pdf:PDF},
  groups   = {Information gathering},
}

@Article{8276241,
  author   = {Oleynikova, Helen and Taylor, Zachary and Siegwart, Roland and Nieto, Juan},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Safe Local Exploration for Replanning in Cluttered Unknown Environments for Microaerial Vehicles},
  year     = {2018},
  issn     = {2377-3766},
  month    = {July},
  number   = {3},
  pages    = {1474-1481},
  volume   = {3},
  abstract = {In order to enable microaerial vehicles (MAVs) to assist in complex, unknown, unstructured environments, they must be able to navigate with guaranteed safety, even when faced with a cluttered environment they have no prior knowledge of. While trajectory-optimization-based local planners have been shown to perform well in these cases, prior work either does not address how to deal with local minima in the optimization problem or solves it by using an optimistic global planner. We present a conservative trajectory-optimization-based local planner, coupled with a local exploration strategy that selects intermediate goals. We perform extensive simulations to show that this system performs better than the standard approach of using an optimistic global planner and also outperforms doing a single exploration step when the local planner is stuck. The method is validated through experiments in a variety of highly cluttered environments including a dense forest. These experiments show the complete system running in real time fully onboard an MAV, mapping and replanning at 4 Hz.},
  doi      = {10.1109/LRA.2018.2800109},
  file     = {:8276241 - Safe Local Exploration for Replanning in Cluttered Unknown Environments for Microaerial Vehicles.pdf:PDF},
  groups   = {Information gathering},
}

@Article{9293348,
  author   = {Li, Juncheng and Ran, Maopeng and Xie, Lihua},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Efficient Trajectory Planning for Multiple Non-Holonomic Mobile Robots via Prioritized Trajectory Optimization},
  year     = {2021},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {405-412},
  volume   = {6},
  abstract = {In this letter, we present a novel approach to efficiently generate collision-free optimal trajectories for multiple non-holonomic mobile robots in obstacle-rich environments. Our approach first employs a graph-based multi-agent path planner to find an initial discrete solution, and then refines this solution into smooth trajectories using nonlinear optimization. We divide the robot team into small groups and propose a prioritized trajectory optimization method to improve the scalability of the algorithm. Infeasible sub-problems may arise in some scenarios because of the decoupled optimization framework. To handle this problem, a novel grouping and priority assignment strategy is developed to increase the probability of finding feasible trajectories. Compared to the coupled trajectory optimization, the proposed approach reduces the computation time considerably with a small impact on the optimality of the plans. Simulations and hardware experiments verified the effectiveness and superiority of the proposed approach.},
  doi      = {10.1109/LRA.2020.3044834},
  file     = {:9293348 - Efficient Trajectory Planning for Multiple Non Holonomic Mobile Robots Via Prioritized Trajectory Optimization.pdf:PDF},
  groups   = {Navigation},
}

@Article{Nelson2017,
  author    = {Erik Nelson and Micah Corah and Nathan Michael},
  journal   = {Autonomous Robots},
  title     = {Environment model adaptation for mobile robot exploration},
  year      = {2017},
  month     = {nov},
  number    = {2},
  pages     = {257--272},
  volume    = {42},
  doi       = {10.1007/s10514-017-9669-2},
  file      = {:Nelson2017 - Environment Model Adaptation for Mobile Robot Exploration.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{bircher2018receding,
  author    = {Bircher, Andreas and Kamel, Mina and Alexis, Kostas and Oleynikova, Helen and Siegwart, Roland},
  journal   = {Autonomous Robots},
  title     = {Receding horizon path planning for 3D exploration and surface inspection},
  year      = {2018},
  number    = {2},
  pages     = {291--306},
  volume    = {42},
  file      = {:bircher2018receding - Receding Horizon Path Planning for 3D Exploration and Surface Inspection.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Springer},
}

@InProceedings{10.1007/978-981-15-9460-1_21,
  author    = {Goel, Kshitij and Corah, Micah and Boirum, Curtis and Michael, Nathan},
  booktitle = {Field and Service Robotics},
  title     = {Fast Exploration Using Multirotors: Analysis, Planning, and Experimentation},
  year      = {2021},
  address   = {Singapore},
  editor    = {Ishigami, Genya and Yoshida, Kazuya},
  pages     = {291--305},
  publisher = {Springer Singapore},
  abstract  = {This work presents a system and approach for the rapid exploration of unknown environments using aerial robots. High-speed flight with multirotor air vehicles is challenging due to limited sensing range, use of onboard computation, and constrained dynamics. For robots operating in unknown environments, the control system must guarantee collision-free operation, and for exploration tasks, the system should also select sensing actions to maximize information gain with respect to the environment. To this end, we present a motion primitive-based, receding-horizon planning approach that maximizes information gain, accounts for platform dynamics, and ensures safe operation. Analysis of motions parallel and perpendicular to frontiers given constraints on sensing and dynamics leads to bounds on safe velocities for exploration. This analysis and the bounds obtained inform the design of the motion primitive approach. Simulation experiments in a complex 3D environment demonstrate the utility of the motion primitive actions for rapid exploration and provide a comparison to a reduced motion primitive library that is appropriate for online planning. Experimental results on a hexarotor robot with the reduced library demonstrate rapid exploration at speeds above 2.25 m/s under a varying clutter in an outdoor environment which is comparable to and exceeding the existing state-of-the-art results.},
  file      = {:EasyChair-Preprint-1454.pdf:PDF},
  groups    = {Information gathering},
  isbn      = {978-981-15-9460-1},
}

@Article{vago2017habitability,
  author    = {Vago, Jorge L and Westall, Frances and Coates, Andrew J and Jaumann, Ralf and Korablev, Oleg and Ciarletti, Val{\'e}rie and Mitrofanov, Igor and Josset, Jean-Luc and De Sanctis, Maria Cristina and Bibring, Jean-Pierre and others},
  journal   = {Astrobiology},
  title     = {Habitability on early Mars and the search for biosignatures with the ExoMars Rover},
  year      = {2017},
  number    = {6-7},
  pages     = {471--510},
  volume    = {17},
  groups    = {Space Exploration},
  publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
  url       = {https://www.liebertpub.com/doi/pdfplus/10.1089/ast.2016.1533},
}

@InProceedings{sonsalla2017field,
  author    = {Sonsalla, Roland and Cordes, Florian and Christensen, Leif and Roehr, Thomas M and Stark, Tobias and Planthaber, Steffen and Maurus, Michael and Mallwitz, Martin and Kirchner, Elsa A},
  booktitle = {Proceedings of the 14th symposium on advanced space technologies in robotics and automation (ASTRA)},
  title     = {Field testing of a cooperative multi-robot sample return mission in mars analogue environment},
  year      = {2017},
  file      = {:sonsalla2017field - Field Testing of a Cooperative Multi Robot Sample Return Mission in Mars Analogue Environment.pdf:PDF},
  groups    = {Space Exploration},
}

@Article{Corah2021,
  author        = {Micah Corah and Nathan Michael},
  title         = {Volumetric Objectives for Multi-Robot Exploration of Three-Dimensional Environments},
  year          = {2021},
  month         = mar,
  abstract      = {Volumetric objectives for exploration and perception tasks seek to capture a sense of value (or reward) for hypothetical observations at one or more camera views for robots operating in unknown environments. For example, a volumetric objective may reward robots proportionally to the expected volume of unknown space to be observed. We identify connections between existing information-theoretic and coverage objectives in terms of expected coverage, particularly that mutual information without noise is a special case of expected coverage. Likewise, we provide the first comparison, of which we are aware, between information-based approximations and coverage objectives for exploration, and we find, perhaps surprisingly, that coverage objectives can significantly outperform information-based objectives in practice. Additionally, the analysis for information and coverage objectives demonstrates that Randomized Sequential Partitions -- a method for efficient distributed sensor planning -- applies for both classes of objectives, and we provide simulation results in a variety of environments for as many as 32 robots.},
  archiveprefix = {arXiv},
  eprint        = {2103.11625},
  file          = {:Corah2021 - Volumetric Objectives for Multi Robot Exploration of Three Dimensional Environments.pdf:PDF},
  groups        = {Information gathering},
  keywords      = {cs.RO},
  primaryclass  = {cs.RO},
}

@InProceedings{Sonsalla2014,
  author = {Sonsalla, Roland and Cordes, Florian and Christensen, Leif and Planthaber, Steffen and Albiez, Jan and Scholz, Ingo and Kirchner, Frank},
  title  = {Towards a Heterogeneous Modular Robotic Team in a Logistic Chain for Extraterrestrial Exploration},
  year   = {2014},
  month  = {01},
  file   = {:Sonsalla2014 - Towards a Heterogeneous Modular Robotic Team in a Logistic Chain for Extraterrestrial Exploration.pdf:PDF},
  groups = {Space Exploration, Heterogeneous},
}

@Article{Roehr_2018,
  author       = {Roehr, Thomas M},
  journal      = {Inteligencia Artificial},
  title        = {A Constraint-based Mission Planning Approach for Reconfigurable Multi-Robot Systems},
  year         = {2018},
  month        = {Sep.},
  number       = {62},
  pages        = {25–39},
  volume       = {21},
  abstractnote = {&amp;lt;p&amp;gt;The application of reconfigurable multi-robot systems introduces additional degrees of freedom to design robotic missions compared to classical multi-robot systems. To allow for autonomous operation of such systems, planning approaches have to be investigated that cannot only cope with the combinatorial challenge arising from the increased flexibility of modular systems, but also exploit this flexibility to improve for example the safety of operation. While the problem originates from the domain of robotics it is of general nature and significantly intersects with operations research. This paper suggests a constraint-based mission planning approach, and presents a set of revised definitions for reconfigurable multi-robot systems including the representation of the planning problem using spatially and temporally qualified resource constraints. Planning is performed using a multi-stage approach and a combined use of knowledge-based reasoning, constraint-based programming and integer linear programming. The paper concludes with the illustration of the solution of a planned example mission.&amp;lt;/p&amp;gt;},
  doi          = {10.4114/intartif.vol21iss62pp25-39},
  file         = {:editor-journal-editor-216.pdf:PDF},
  groups       = {Space Exploration},
  url          = {https://www.journal.iberamia.org/index.php/intartif/article/view/216},
}

@InProceedings{govindaraj2019pro,
  author    = {Govindaraj, Shashank and Gancet, Jeremi and Urbina, Diego and Brinkmann, Wiebke and Aouf, Nabil and Lacroix, Simon and Wolski, Mateusz and Colmenero, Francisco and Walshe, Michael and Ortega, Cristina and others},
  booktitle = {Proceedings of the 15th Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA-2019)},
  title     = {PRO-ACT: Planetary Robots Deployed for Assembly and Construction of Future Lunar ISRU and Supporting Infrastructures},
  year      = {2019},
  file      = {:govindaraj2019pro - PRO ACT_ Planetary Robots Deployed for Assembly and Construction of Future Lunar ISRU and Supporting Infrastructures.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{Govindaraj2020,
  author = {Govindaraj, Shashank and Nieto, Irene and But, Alexandru and Brinkmann, Wiebke and Dettmann, Alexander and Danter, Leon and Aouf, Nabil and Sotoodeh Bahraini, Masoud and Zenati, Abdelhafid and Savino, Heitor and Stelmachowski, Jakub and Colmenero, Francisco and Heredia Aguado, Enrique and Alonso, Mercedes and Purnell, Joe and Picton, Kevin and Lopes, Luís},
  title  = {Multi-Robot Cooperation for Lunar Base Assembly And Construction},
  year   = {2020},
  month  = {10},
  file   = {:11170_i-SAIRAS2020_PRO-ACT-Consortial-Paper.pdf:PDF},
  groups = {Space Exploration},
}

@InProceedings{brinkmann2019space,
  author    = {Brinkmann, Wiebke and Cordes, Florian and Koch, Christian Ernst Siegfried and Wirkus, Malte and Dominguez, Raul and Dettmann, Alexander and V{\"o}gele, Thomas and Kirchner, Frank},
  booktitle = {proceedings of 2019 Space Tech Industry Conference, Bremen, Germany},
  title     = {Space robotic systems and artificial intelligence in the context of the European space technology roadmap},
  year      = {2019},
  file      = {:brinkmann2019space - Space Robotic Systems and Artificial Intelligence in the Context of the European Space Technology Roadmap.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{waldmann2018europa,
  author    = {Waldmann, C and Winebrenner, D and Bachmayer, R and Hanff, H and Funke, O},
  booktitle = {Lunar and Planetary Science Conference},
  title     = {EUROPA Calling--A comprehensive technical concept for exploring ocean worlds},
  year      = {2018},
  number    = {2083},
  pages     = {2162},
  groups    = {Space Exploration},
}

@Article{8633953,
  author   = {Corah, Micah and O’Meadhra, Cormac and Goel, Kshitij and Michael, Nathan},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Communication-Efficient Planning and Mapping for Multi-Robot Exploration in Large Environments},
  year     = {2019},
  issn     = {2377-3766},
  month    = {April},
  number   = {2},
  pages    = {1715-1721},
  volume   = {4},
  abstract = {This letter presents a framework for planning and perception for multi-robot exploration in large and unstructured three-dimensional environments. We employ a Gaussian mixture model for global mapping to model complex environment geometries while maintaining a small memory footprint which enables distributed operation with a low volume of communication. We then generate a local occupancy grid for use in planning from the Gaussian mixture model using Monte Carlo ray tracing. Then, a finite-horizon, information-based planner uses this local map and optimizes sequences of observations locally while accounting for the global distribution of information in the robot state space which we model using a library of informative views. Simulation results demonstrate that the proposed system is able to maintain efficiency and completeness in exploration while only requiring a low rate of communication.},
  doi      = {10.1109/LRA.2019.2897368},
  file     = {:8633953 - Communication Efficient Planning and Mapping for Multi Robot Exploration in Large Environments.pdf:PDF},
  groups   = {Information gathering, Multi-robot},
}

@Article{9152817,
  author   = {Ni, Jianjun and Wang, Xiaotian and Tang, Min and Cao, Weidong and Shi, Pengfei and Yang, Simon X.},
  journal  = {IEEE Access},
  title    = {An Improved Real-Time Path Planning Method Based on Dragonfly Algorithm for Heterogeneous Multi-Robot System},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {140558-140568},
  volume   = {8},
  abstract = {Heterogeneous multi-robot system is one of the most important research directions in the robotic field. Real-time path planning for heterogeneous multi-robot system under unknown 3D environment is a new challenging research and a hot spot in this field. In this paper, an improved real-time path planning method is proposed for a heterogeneous multi-robot system, which is composed of many unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). In the proposed method, the 3D environment is modelled as a neuron topology map, based on the grid method combined with the bio-inspired neural network. Then a new 3D dynamic movement model for multi-robots is established based on an improved Dragonfly Algorithm (DA). Thus, the movements of the robots are optimized according to the activities of the neurons in the bio-inspired neural network to realize the real-time path planning. Furthermore, some simulations have been carried out. The results show that the proposed method can effectively guide the heterogeneous UAV/UGV system to the target, and has better performance than traditional methods in the real-time path planning tasks.},
  doi      = {10.1109/ACCESS.2020.3012886},
  file     = {:09152817.pdf:PDF},
  groups   = {Heterogeneous},
}

@Article{fabisch2020bolero,
  author    = {Fabisch, Alexander and Langosz, Malte and Kirchner, Frank},
  journal   = {International Journal of Advanced Robotic Systems},
  title     = {BOLeRo: Behavior optimization and learning for robots},
  year      = {2020},
  number    = {3},
  pages     = {1729881420913741},
  volume    = {17},
  file      = {:fabisch2020bolero - BOLeRo_ Behavior Optimization and Learning for Robots.pdf:PDF},
  groups    = {Robotics, Platforms},
  publisher = {SAGE Publications Sage UK: London, England},
}

@PhdThesis{hidalgo2018adaptive,
  author = {Hidalgo-Carri{\'o}, Javier},
  school = {Universit{\"a}t Bremen},
  title  = {Adaptive Localization and Mapping for Planetary Rovers},
  year   = {2018},
  file   = {:hidalgo2018adaptive - Adaptive Localization and Mapping for Planetary Rovers.pdf:PDF},
  groups = {Space Exploration},
}

@InProceedings{schwendnerentern,
  author    = {Schwendner, Jakob and Hidalgo-Carrio, Javier and Dom{\i}nguez, Ra{\'u}l and Planthaber, Steffen and Yoo, Yong-Ho and Asadi, Behnam and Machowinski, Janosch and Rauch, Christian and Kirchner, Frank},
  booktitle = {Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA). ESA/Estec Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA), 13th, May},
  title     = {Entern--environment modelling and navigation for robotic space-exploration},
  year      = {2015},
  pages     = {11--13},
  file      = {:schwendnerentern - Entern Environment Modelling and Navigation for Robotic Space Exploration.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{carrio2016envire,
  author    = {Carri{\'o}, Javier Hidalgo and Arnold, Sascha and B{\"o}ckmann, Arne and Born, Anna and Dom{\'\i}nguez, Ra{\'u}l and Kirchner, F},
  booktitle = {AI for Long-term Autonomy Workshop of the Int. Conf. on Robotics and Automation (ICRA)},
  title     = {EnviRe-Environment Representation for Long-term Autonomy},
  year      = {2016},
  file      = {:carrio2016envire - EnviRe Environment Representation for Long Term Autonomy.pdf:PDF},
  groups    = {Space Exploration},
}

@InBook{Starek2016,
  author    = {Starek, Joseph A. and A{\c{c}}{\i}kme{\c{s}}e, Beh{\c{c}}et and Nesnas, Issa A. and Pavone, Marco},
  editor    = {Feron, Eric},
  pages     = {1--48},
  publisher = {Springer Berlin Heidelberg},
  title     = {Spacecraft Autonomy Challenges for Next-Generation Space Missions},
  year      = {2016},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-662-47694-9},
  abstract  = {In early 2011, NASA's Office of the Chief Technologist (OCT) released a set of technology roadmaps with the aim of fostering the development of concepts and cross-cutting technologies addressing NASA's needs for the 2011--2021 decade and beyond. In an attempt to engage the external technical community and enhance the development program in light of scarce resources, NASA reached out to the National Research Council (NRC) to review the program's objectives and prioritize its list of technologies. In January 2012, the NRC released its report entitled ``Restoring NASA's Technological Edge and Paving the Way for a New Era in Space.'' While the NRC report provides a systematic and thorough ranking of the future technology needs for NASA, it does not discuss in detail the technical aspects of the prioritized technologies (which clearly lie beyond its scope). This chapter, building upon the NRC report and an earlier assessment of NASA's needs in terms of guidance, navigation, and control technologies, aims at providing such technical details for a selected number of high-priority technologies in the autonomous systems area. Specifically, this chapter focuses on technology area TA04 ``Robotics, Tele-Robotics, and Autonomous Systems'' and discusses in some detail the technical aspects and challenges associated with three high-priority TA04 technologies: ``Relative Guidance Algorithms,'' ``Extreme Terrain Mobility,'' and ``Small Body/Microgravity Mobility.'' The result is a unified presentation of key autonomy challenges for next-generation space missions.},
  booktitle = {Advances in Control System Technology for Aerospace Applications},
  doi       = {10.1007/978-3-662-47694-9_1},
  file      = {:Starek2016 - Spacecraft Autonomy Challenges for Next Generation Space Missions.pdf:PDF},
  groups    = {Space Exploration},
  ranking   = {rank4},
  url       = {https://doi.org/10.1007/978-3-662-47694-9_1},
}

@InProceedings{Dominguez2014,
  author    = {Raúl Domínguez, Jakob Schwendner, Frank Kirchner},
  booktitle = {Symposium on Advanced Space Technologies in Robotics and Automation (ASTRA)},
  title     = {On-Board Simulator for Autonomy Enhancement in Robotic Space Missions},
  year      = {2015},
  file      = {:Dominguez2014 - On Board Simulator for Autonomy Enhancement in Robotic Space Missions.pdf:PDF},
  groups    = {Space Exploration},
}

@Article{Corah_2018,
  author    = {Micah Corah and Nathan Michael},
  journal   = {Autonomous Robots},
  title     = {Distributed matroid-constrained submodular maximization for multi-robot exploration: theory and practice},
  year      = {2018},
  month     = {jul},
  number    = {2},
  pages     = {485--501},
  volume    = {43},
  doi       = {10.1007/s10514-018-9778-6},
  file      = {:corah2018.pdf:PDF},
  groups    = {Information gathering, Multi-robot},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Wang2019a,
  author    = {Chaoqun Wang and Wenzheng Chi and Yuxiang Sun and Max Q.-H. Meng},
  journal   = {{IEEE} Transactions on Automation Science and Engineering},
  title     = {Autonomous Robotic Exploration by Incremental Road Map Construction},
  year      = {2019},
  month     = {oct},
  number    = {4},
  pages     = {1720--1731},
  volume    = {16},
  doi       = {10.1109/tase.2019.2894748},
  file      = {:wang2019.pdf:PDF},
  groups    = {Information gathering},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Pergola2016,
  author    = {Pierpaolo Pergola and Vittorio Cipolla},
  journal   = {International Journal of Intelligent Unmanned Systems},
  title     = {Mission architecture for Mars exploration based on small satellites and planetary drones},
  year      = {2016},
  month     = {jul},
  number    = {3},
  pages     = {142--162},
  volume    = {4},
  doi       = {10.1108/ijius-12-2015-0014},
  file      = {:pergola2016.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {Emerald},
}

@Article{Roehr2013,
  author    = {Thomas M. Roehr and Florian Cordes and Frank Kirchner},
  journal   = {Journal of Field Robotics},
  title     = {Reconfigurable Integrated Multirobot Exploration System ({RIMRES}): Heterogeneous Modular Reconfigurable Robots for Space Exploration},
  year      = {2013},
  month     = {aug},
  number    = {1},
  pages     = {3--34},
  volume    = {31},
  doi       = {10.1002/rob.21477},
  file      = {:roehr2013.pdf:PDF},
  groups    = {Space Exploration, Heterogeneous},
  publisher = {Wiley},
}

@Article{Zheng2011,
  author    = {Y. Zheng and L. Wang and Y. Zhu},
  journal   = {{IET} Control Theory {\&} Applications},
  title     = {Consensus of heterogeneous multi-agent systems},
  year      = {2011},
  month     = {nov},
  number    = {16},
  pages     = {1881--1888},
  volume    = {5},
  doi       = {10.1049/iet-cta.2011.0033},
  file      = {:Zheng2011 - Consensus of Heterogeneous Multi Agent Systems.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institution of Engineering and Technology ({IET})},
  ranking   = {rank2},
}

@Article{Kim2011,
  author    = {Hongkeun Kim and Hyungbo Shim and Jin Heon Seo},
  journal   = {{IEEE} Transactions on Automatic Control},
  title     = {Output Consensus of Heterogeneous Uncertain Linear Multi-Agent Systems},
  year      = {2011},
  month     = {jan},
  number    = {1},
  pages     = {200--206},
  volume    = {56},
  doi       = {10.1109/tac.2010.2088710},
  file      = {:kim2011.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank3},
}

@InProceedings{Doriya2015,
  author    = {Rajesh Doriya and Siddharth Mishra and Swati Gupta},
  booktitle = {International Conference on Computing, Communication {\&} Automation},
  title     = {A brief survey and analysis of multi-robot communication and coordination},
  year      = {2015},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/ccaa.2015.7148524},
  file      = {:doriya2015.pdf:PDF},
  groups    = {Multi-Robot Communication},
}

@InBook{Parker2016_MRS,
  author    = {Parker, Lynne E. and Rus, Daniela and Sukhatme, Gaurav S.},
  editor    = {Siciliano, Bruno and Khatib, Oussama},
  pages     = {1335--1384},
  publisher = {Springer International Publishing},
  title     = {Multiple Mobile Robot Systems},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-32552-1},
  abstract  = {Within the context of multiple mobile, and networked robot systems, this chapter explores the current state of the art. After a brief introduction, we first examine architectures for multirobot cooperation, exploring the alternative approaches that have been developed. Next, we explore communications issues and their impact on multirobot teams in Sect. 53.3, followed by a discussion of networked mobile robots in Sect. 53.4. Following this we discuss swarm robot systems in Sect. 53.5 and modular robot systems in Sect. 53.6. While swarm and modular systems typically assume large numbers of homogeneous robots, other types of multirobot systems include heterogeneous robots. We therefore next discuss heterogeneity in cooperative robot teams in Sect. 53.7. Once robot teams allow for individual heterogeneity, issues of task allocation become important; Sect. 53.8 therefore discusses common approaches to task allocation. Section 53.9 discusses the challenges of multirobot learning, and some representative approaches. We outline some of the typical application domains which serve as test beds for multirobot systems research in Sect. 53.10. Finally, we conclude in Sect. 53.11 with some summary remarks and suggestions for further reading.},
  booktitle = {Springer Handbook of Robotics},
  doi       = {10.1007/978-3-319-32552-1_53},
  file      = {:Parker2016 - Multiple Mobile Robot Systems.pdf:PDF},
  groups    = {Multi-robot},
  ranking   = {rank3},
  url       = {https://doi.org/10.1007/978-3-319-32552-1_53},
}

@Article{Rosa2015,
  author    = {Lorenzo Rosa and Marco Cognetti and Andrea Nicastro and Pol Alvarez and Giuseppe Oriolo},
  journal   = {{IFAC}-{PapersOnLine}},
  title     = {Multi-task Cooperative Control in a Heterogeneous Ground-Air Robot Team},
  year      = {2015},
  number    = {5},
  pages     = {53--58},
  volume    = {48},
  doi       = {10.1016/j.ifacol.2015.06.463},
  file      = {:Rosa2015 - Multi Task Cooperative Control in a Heterogeneous Ground Air Robot Team.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Elsevier {BV}},
}

@InProceedings{Stegagno2013,
  author    = {Paolo Stegagno and Marco Cognetti and Lorenzo Rosa and Pietro Peliti and Giuseppe Oriolo},
  booktitle = {2013 {IEEE} International Conference on Robotics and Automation},
  title     = {Relative localization and identification in a heterogeneous multi-robot system},
  year      = {2013},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra.2013.6630822},
  file      = {:Stegagno2013 - Relative Localization and Identification in a Heterogeneous Multi Robot System.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Mathew2015,
  author    = {Neil Mathew and Stephen L. Smith and Steven L. Waslander},
  journal   = {{IEEE} Transactions on Automation Science and Engineering},
  title     = {Planning Paths for Package Delivery in Heterogeneous Multirobot Teams},
  year      = {2015},
  month     = {oct},
  number    = {4},
  pages     = {1298--1308},
  volume    = {12},
  doi       = {10.1109/tase.2015.2461213},
  file      = {:2015TASE_Package_Delivery.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank2},
}

@Article{Krizmancic2020,
  author    = {Marko Krizmancic and Barbara Arbanas and Tamara Petrovic and Frano Petric and Stjepan Bogdan},
  journal   = {{IEEE} Robotics and Automation Letters},
  title     = {Cooperative Aerial-Ground Multi-Robot System for Automated Construction Tasks},
  year      = {2020},
  month     = {apr},
  number    = {2},
  pages     = {798--805},
  volume    = {5},
  doi       = {10.1109/lra.2020.2965855},
  file      = {:Krizmancic2020 - Cooperative Aerial Ground Multi Robot System for Automated Construction Tasks.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Roldan2016,
  author    = {Juan Rold{\'{a}}n and Pablo Garcia-Aunon and Mario Garz{\'{o}}n and Jorge de Le{\'{o}}n and Jaime del Cerro and Antonio Barrientos},
  journal   = {Sensors},
  title     = {Heterogeneous Multi-Robot System for Mapping Environmental Variables of Greenhouses},
  year      = {2016},
  month     = {jul},
  number    = {7},
  pages     = {1018},
  volume    = {16},
  doi       = {10.3390/s16071018},
  file      = {:sensors-16-01018.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {{MDPI} {AG}},
}

@Article{Rahimi2014,
  author    = {Reihane Rahimi and Farzaneh Abdollahi and Karo Naqshi},
  journal   = {Robotics and Autonomous Systems},
  title     = {Time-varying formation control of a collaborative heterogeneous multi agent system},
  year      = {2014},
  month     = {dec},
  number    = {12},
  pages     = {1799--1805},
  volume    = {62},
  doi       = {10.1016/j.robot.2014.07.005},
  file      = {:Rahimi2014 - Time Varying Formation Control of a Collaborative Heterogeneous Multi Agent System.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Elsevier {BV}},
}

@Article{Nguyen2020,
  author     = {Thanh Thi Nguyen and Ngoc Duy Nguyen and Saeid Nahavandi},
  journal    = {{IEEE} Transactions on Cybernetics},
  title      = {Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications},
  year       = {2020},
  month      = {sep},
  number     = {9},
  pages      = {3826--3839},
  volume     = {50},
  doi        = {10.1109/tcyb.2020.2977374},
  file       = {:Nguyen2020 - Deep Reinforcement Learning for Multiagent Systems_ a Review of Challenges, Solutions, and Applications.pdf:PDF},
  groups     = {MAS Reviews},
  publisher  = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{8460759,
  author    = {Manjanna, Sandeep and Li, Alberto Quattrini and Smith, Ryan N. and Rekleitis, Ioannis and Dudek, Gregory},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Heterogeneous Multi-Robot System for Exploration and Strategic Water Sampling},
  year      = {2018},
  month     = {May},
  pages     = {4873-4880},
  abstract  = {Physical sampling of water for off-site analysis is necessary for many applications like monitoring the quality of drinking water in reservoirs, understanding marine ecosystems, and measuring contamination levels in fresh-water systems. In this paper, the focus is on algorithms for efficient measurement and sampling using a multi-robot, data-driven, water-sampling behavior, where autonomous surface vehicles plan and execute water sampling using the chlorophyll density as a cue for plankton-rich water samples. We use two Autonomous Surface Vehicles (ASVs), one equipped with a water quality sensor and the other equipped with a water-sampling apparatus. The ASV with the sensor acts as an explorer, measuring and building a spatial map of chlorophyll density in the given region of interest. The ASV equipped with the water sampling apparatus makes decisions in real time on where to sample the water based on the suggestions made by the explorer robot. We evaluate the system in the context of measuring chlorophyll distributions. We do this both in simulation based on real geophysical data from MODIS measurements, and on real robots in a water reservoir. We demonstrate the effectiveness of the proposed approach in several ways including in terms of mean error in the interpolated data as a function of distance traveled.},
  doi       = {10.1109/ICRA.2018.8460759},
  file      = {:8460759 - Heterogeneous Multi Robot System for Exploration and Strategic Water Sampling.pdf:PDF},
  groups    = {Heterogeneous},
  issn      = {2577-087X},
}

@Article{schillinger2018simultaneous,
  author    = {Schillinger, Philipp and B{\"u}rger, Mathias and Dimarogonas, Dimos V},
  journal   = {The international journal of robotics research},
  title     = {Simultaneous task allocation and planning for temporal logic goals in heterogeneous multi-robot systems},
  year      = {2018},
  number    = {7},
  pages     = {818--838},
  volume    = {37},
  file      = {:schillinger2018simultaneous - Simultaneous Task Allocation and Planning for Temporal Logic Goals in Heterogeneous Multi Robot Systems.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Sage Publications Sage UK: London, England},
}

@Article{Verma2021,
  author    = {Janardan Kumar Verma and Virender Ranga},
  journal   = {Journal of Intelligent {\&} Robotic Systems},
  title     = {Multi-Robot Coordination Analysis, Taxonomy, Challenges and Future Scope},
  year      = {2021},
  month     = {apr},
  number    = {1},
  volume    = {102},
  doi       = {10.1007/s10846-021-01378-2},
  file      = {:Verma2021 - Multi Robot Coordination Analysis, Taxonomy, Challenges and Future Scope.pdf:PDF},
  groups    = {Multi-robot},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Mayya2021,
  author    = {Siddharth Mayya and Diego S. D{\textquotesingle}antonio and David Saldana and Vijay Kumar},
  journal   = {{IEEE} Robotics and Automation Letters},
  title     = {Resilient Task Allocation in Heterogeneous Multi-Robot Systems},
  year      = {2021},
  month     = {apr},
  number    = {2},
  pages     = {1327--1334},
  volume    = {6},
  doi       = {10.1109/lra.2021.3057559},
  file      = {:Mayya2021 - Resilient Task Allocation in Heterogeneous Multi Robot Systems.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Amigoni2017,
  author    = {Francesco Amigoni and Jacopo Banfi and Nicola Basilico},
  journal   = {{IEEE} Intelligent Systems},
  title     = {Multirobot Exploration of Communication-Restricted Environments: A Survey},
  year      = {2017},
  month     = {nov},
  number    = {6},
  pages     = {48--57},
  volume    = {32},
  doi       = {10.1109/mis.2017.4531226},
  file      = {:Amigoni2017 - Multirobot Exploration of Communication Restricted Environments_ a Survey.pdf:PDF},
  groups    = {Multi-robot, Information gathering},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Yang2020,
  author    = {Qin Yang and Ramviyas Parasuraman},
  booktitle = {2020 {IEEE} International Symposium on Safety, Security, and Rescue Robotics ({SSRR})},
  title     = {Needs-driven Heterogeneous Multi-Robot Cooperation in Rescue Missions},
  year      = {2020},
  month     = {nov},
  publisher = {{IEEE}},
  doi       = {10.1109/ssrr50563.2020.9292570},
  file      = {:Yang2020 - Needs Driven Heterogeneous Multi Robot Cooperation in Rescue Missions.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Schuster2020,
  author    = {Martin J. Schuster and Marcus G. Muller and Sebastian G. Brunner and Hannah Lehner and Peter Lehner and Ryo Sakagami and Andreas Domel and Lukas Meyer and Bernhard Vodermayer and Riccardo Giubilato and Mallikarjuna Vayugundla and Josef Reill and Florian Steidle and Ingo von Bargen and Kristin Bussmann and Rico Belder and Philipp Lutz and Wolfgang Sturzl and Michal Smisek and Moritz Maier and Samantha Stoneman and Andre Fonseca Prince and Bernhard Rebele and Maximilian Durner and Emanuel Staudinger and Siwei Zhang and Robert Pohlmann and Esther Bischoff and Christian Braun and Susanne Schroder and Enrico Dietz and Sven Frohmann and Anko Borner and Heinz-Wilhelm Hubers and Bernard Foing and Rudolph Triebel and Alin O. Albu-Schaffer and Armin Wedler},
  journal   = {{IEEE} Robotics and Automation Letters},
  title     = {The {ARCHES} Space-Analogue Demonstration Mission: Towards Heterogeneous Teams of Autonomous Robots for Collaborative Scientific Sampling in Planetary Exploration},
  year      = {2020},
  month     = {oct},
  number    = {4},
  pages     = {5315--5322},
  volume    = {5},
  doi       = {10.1109/lra.2020.3007468},
  file      = {:Schuster2020 - The ARCHES Space Analogue Demonstration Mission_ Towards Heterogeneous Teams of Autonomous Robots for Collaborative Scientific Sampling in Planetary Exploration.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{giubilato2020gpgm,
  author       = {Giubilato, Riccardo and Le Gentil, Cedric and Vayugundla, Mallikarjuna and Vidal-Calleja, Teresa and Triebel, Rudolph},
  booktitle    = {IROS Workshop on Planetary Exploration Robots: Challenges and Opportunities (PLANROBO20)},
  title        = {GPGM-SLAM: Towards a Robust SLAM System for Unstructured Planetary Environments with Gaussian Process Gradient Maps},
  year         = {2020},
  organization = {ETH Zurich, Department of Mechanical and Process Engineering},
  file         = {:giubilato2020gpgm - GPGM SLAM_ Towards a Robust SLAM System for Unstructured Planetary Environments with Gaussian Process Gradient Maps (1).pdf:PDF},
  groups       = {Space Exploration, Information gathering},
}

@Article{wedler2021german,
  author    = {Wedler, Armin and Schuster, Martin J and M{\"u}ller, Marcus G and Vodermayer, Bernhard and Meyer, Lukas and Giubilato, Riccardo and Vayugundla, Mallikarjuna and Smisek, Michal and D{\"o}mel, Andreas and Steidle, Florian and others},
  journal   = {Philosophical Transactions of the Royal Society A},
  title     = {German Aerospace Center's advanced robotic technology for future lunar scientific missions},
  year      = {2021},
  number    = {2188},
  pages     = {20190574},
  volume    = {379},
  file      = {:wedler2021german - German Aerospace Center's Advanced Robotic Technology for Future Lunar Scientific Missions.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {The Royal Society Publishing},
}

@InProceedings{dlr132829,
  author    = {Martin J. Schuster and Marcus G. M{\"u}ller and Sebastian G. Brunner and Hannah Lehner and Peter Lehner and Andreas D{\"o}mel and Mallikarjuna Vayugundla and Florian Steidle and Philipp Lutz and Ryo Sakagami and Lukas Meyer and Rico Belder and Michal Smisek and Wolfgang St{\"u}rzl and Rudolph Triebel and Armin Wedler},
  booktitle = {Workshop on Informed Scientific Sampling in Large-scale Outdoor Environments at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Towards Heterogeneous Robotic Teams for Collaborative Scientific Sampling in Lunar and Planetary Environments},
  year      = {2019},
  abstract  = {Teams of mobile robots will play a crucial role in future scientific missions to explore the surfaces of extraterrestrial bodies such as Moon or Mars. Taking scientific samples is an expensive task when operating far away in challenging, previously unknown environments, especially in hard-to-reach areas, such as craters, pits, and subterranean caves. In contrast to current single-robot missions, future robotic teams will increase efficiency via increased autonomy and parallelization, improve robustness via functional redundancy, as well as benefit from complementary capabilities of the individual robots. In this work, we present our heterogeneous robotic team consisting of flying and driving robots that we plan to deploy on a scientific sampling demonstration mission in a Moon-analogue environment on Mt. Etna, Sicily, Italy in 2020 as part of the ARCHES project. We first describe the robots' individual capabilities and then highlight their tasks in the joint mission scenario. In addition, we present preliminary experiments on important subtasks: the analysis of volcanic rocks via spectral images, collaborative multi-robot 6D SLAM in a Moon-analogue environment as well as with a rover and a drone in a Mars-like scenario, and demonstrations of autonomous robotic sample-return missions therein.},
  file      = {:dlr132829 - Towards Heterogeneous Robotic Teams for Collaborative Scientific Sampling in Lunar and Planetary Environments.pdf:PDF},
  groups    = {Space Exploration},
  keywords  = {scientific sampling; multi-robot; planetary exploration;},
  url       = {https://elib.dlr.de/132829/},
}

@InProceedings{dlr122782,
  author    = {Armin Wedler and Martina Wilde and Andreas D{\"o}mel and Marcus Gerhard M{\"u}ller and Josef Reill and Martin Schuster and Wolfgang St{\"u}rzl and Rudolph Triebel and Heinrich Gmeiner and Bernhard Vodermayer and Kristin Bussmann, and Mallikarjuna Vayugundla and Sebastian Brunner and Hannah Lehner and Peter Lehner and Anko B{\"o}rner and Rainer Krenn and Armin Dammann and Uwe-Carsten Fiebig and Emanuel Staudinger and Frank Wenzh{\"o}fer and Sascha Fl{\"o}gel and Stefan Sommer and Tamim Asfour and Michael Flad and S{\"o}ren Hohmann and Martin Brandauer and Alin Olimpiu Albu-Sch{\"a}ffer},
  booktitle = {69th International Astronautical Congress (IAC)},
  title     = {From single autonomous robots toward cooperative robotic interactions for future planetary exploration missions},
  year      = {2018},
  month     = {October},
  publisher = {International Astronautical Federation (IAF)},
  series    = {Preceedings of the 69th International Astronautical Congress (IAC)},
  abstract  = {Mobile robotics will play a key role in future space, ocean and deep sea exploration activities. Besides the actual development of the robotic systems, the different ways of commanding those systems, using technologies varying from teleoperation with the human in the loop, through shared autonomy towards highly autonomous systems, will be the main challenges of these missions. This paper describes the robotic activities of the DLR institutions within the Helmholtz projects ROBEX and ARCHES, dealing with robots for autonomous space and ocean exploration applications. Furthermore, it describes the challenges, the overlap and the synergies of those domains, the different approaches of operating robots from far distances in extreme environments and gives an outlook on future mission possibilities.},
  file      = {:dlr122782 - From Single Autonomous Robots toward Cooperative Robotic Interactions for Future Planetary Exploration Missions.pdf:PDF},
  groups    = {Space Exploration},
  journal   = {Proceedings of the International Astronautical Congress, IAC},
  keywords  = {robotics, autonomy, exploration, shared autonomy, teleoperation},
  url       = {https://elib.dlr.de/122782/},
}

@PhdThesis{dlr132830,
  author   = {Martin J. Schuster},
  school   = {University of Bremen},
  title    = {Collaborative Localization and Mapping for Autonomous Planetary Exploration: Distributed Stereo Vision-Based 6D SLAM in GNSS-Denied Environments},
  year     = {2019},
  month    = {August},
  abstract = {Mobile robots are a crucial element of present and future scientific missions to explore the surfaces of foreign celestial bodies such as Moon and Mars. The deployment of teams of robots allows to improve efficiency and robustness in such challenging environments. As long communication round-trip times to Earth render the teleoperation of robotic systems inefficient to impossible, on-board autonomy is a key to success. The robots operate in Global Navigation Satellite System (GNSS)-denied environments and thus have to rely on space-suitable on-board sensors such as stereo camera systems. They need to be able to localize themselves online, to model their surroundings, as well as to share information about the environment and their position therein. These capabilities constitute the basis for the local autonomy of each system as well as for any coordinated joint action within the team, such as collaborative autonomous exploration. In this thesis, we present a novel approach for stereo vision-based on-board and online Simultaneous Localization and Mapping (SLAM) for multi-robot teams given the challenges imposed by planetary exploration missions. We combine distributed local and decentralized global estimation methods to get the best of both worlds: A local reference filter on each robot provides real-time local state estimates required for robot control and fast reactive behaviors. We designed a novel graph topology to incorporate these state estimates into an online incremental graph optimization to compute global pose and map estimates that serve as input to higher-level autonomy functions. In order to model the 3D geometry of the environment, we generate dense 3D point cloud and probabilistic voxel-grid maps from noisy stereo data. We distribute the computational load and reduce the required communication bandwidth between robots by locally aggregating high-bandwidth vision data into partial maps that are then exchanged between robots and composed into global models of the environment. We developed methods for intra- and inter-robot map matching to recognize previously visited locations in semi- and unstructured environments based on their estimated local geometry, which is mostly invariant to light conditions as well as different sensors and viewpoints in heterogeneous multi-robot teams. A decoupling of observable and unobservable states in the local filter allows us to introduce a novel optimization: Enforcing all submaps to be gravity-aligned, we can reduce the dimensionality of the map matching from 6D to 4D. In addition to map matches, the robots use visual fiducial markers to detect each other. In this context, we present a novel method for modeling the errors of the loop closure transformations that are estimated from these detections. We demonstrate the robustness of our methods by integrating them on a total of five different ground-based and aerial mobile robots that were deployed in a total of 31 real-world experiments for quantitative evaluations in semi- and unstructured indoor and outdoor settings. In addition, we validated our SLAM framework through several different demonstrations at four public events in Moon and Mars-like environments. These include, among others, autonomous multi-robot exploration tests at a Moon-analogue site on top of the volcano Mt. Etna, Italy, as well as the collaborative mapping of a Mars-like environment with a heterogeneous robotic team of flying and driving robots in more than 35 public demonstration runs.},
  file     = {:dlr132830 - Collaborative Localization and Mapping for Autonomous Planetary Exploration_ Distributed Stereo Vision Based 6D SLAM in GNSS Denied Environments.pdf:PDF},
  groups   = {Space Exploration},
  keywords = {SLAM, collaborative SLAM, distributed SLAM, localization, mapping, stereo-vision, multi-robot, planetary exploration, autonomous robots},
  url      = {https://elib.dlr.de/132830/},
}

@Article{Chang2020,
  author        = {Yuan Chang and Chao Yan and Xingyu Liu and Xiangke Wang and Han Zhou and Xiaojia Xiang and Dengqing Tang},
  title         = {Time-Efficient Mars Exploration of Simultaneous Coverage and Charging with Multiple Drones},
  year          = {2020},
  month         = nov,
  abstract      = {This paper presents a time-efficient scheme for Mars exploration by the cooperation of multiple drones and a rover. To maximize effective coverage of the Mars surface in the long run, a comprehensive framework has been developed with joint consideration for limited energy, sensor model, communication range and safety radius, which we call TIME-SC2 (TIme-efficient Mars Exploration of Simultaneous Coverage and Charging). First, we propose a multi-drone coverage control algorithm by leveraging emerging deep reinforcement learning and design a novel information map to represent dynamic system states. Second, we propose a near-optimal charging scheduling algorithm to navigate each drone to an individual charging slot, and we have proven that there always exists feasible solutions. The attractiveness of this framework not only resides on its ability to maximize exploration efficiency, but also on its high autonomy that has greatly reduced the non-exploring time. Extensive simulations have been conducted to demonstrate the remarkable performance of TIME-SC2 in terms of time-efficiency, adaptivity and flexibility.},
  archiveprefix = {arXiv},
  eprint        = {2011.07759},
  file          = {:Chang2020 - Time Efficient Mars Exploration of Simultaneous Coverage and Charging with Multiple Drones.pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.AI},
  primaryclass  = {cs.RO},
}

@InProceedings{Klodt2015,
  author    = {Lukas Klodt and Saman Khodaverdian and Volker Willert},
  booktitle = {2015 {IEEE} Conference on Control Applications ({CCA})},
  title     = {Motion control for {UAV}-{UGV} cooperation with visibility constraint},
  year      = {2015},
  month     = {sep},
  publisher = {{IEEE}},
  doi       = {10.1109/cca.2015.7320804},
  file      = {:Klodt2015 - Motion Control for UAV UGV Cooperation with Visibility Constraint.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Li2016,
  author    = {Jianqiang Li and Genqiang Deng and Chengwen Luo and Qiuzhen Lin and Qiao Yan and Zhong Ming},
  journal   = {{IEEE} Transactions on Vehicular Technology},
  title     = {A Hybrid Path Planning Method in Unmanned Air/Ground Vehicle ({UAV}/{UGV}) Cooperative Systems},
  year      = {2016},
  month     = {dec},
  number    = {12},
  pages     = {9585--9596},
  volume    = {65},
  doi       = {10.1109/tvt.2016.2623666},
  file      = {:li2016.pdf:PDF},
  groups    = {Heterogeneous},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank2},
}

@Article{Liu2018,
  author    = {Chi Harold Liu and Zheyu Chen and Jian Tang and Jie Xu and Chengzhe Piao},
  journal   = {{IEEE} Journal on Selected Areas in Communications},
  title     = {Energy-Efficient {UAV} Control for Effective and Fair Communication Coverage: A Deep Reinforcement Learning Approach},
  year      = {2018},
  month     = {sep},
  number    = {9},
  pages     = {2059--2070},
  volume    = {36},
  doi       = {10.1109/jsac.2018.2864373},
  groups    = {Navigation},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank2},
}

@Article{ROPERO2019260,
  author   = {Fernando Ropero and Pablo Muñoz and María D. R-Moreno},
  journal  = {Engineering Applications of Artificial Intelligence},
  title    = {TERRA: A path planning algorithm for cooperative UGV–UAV exploration},
  year     = {2019},
  issn     = {0952-1976},
  pages    = {260-272},
  volume   = {78},
  abstract = {In this paper, we consider the scenario of exploring a planetary surface with a system formed by an Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV). The goal is to reach a set of target points minimizing the travelling distance. Some expected key problems in planetary explorations are the UGVs functionality constraints to reach some target points as a single robot system and the UAVs energy constraints to reach all the target points on its own. We present an approach based on the coordination of a hybrid UGV–UAV system, in which both robots work together for reaching all the target points. Our strategy proposes the UGV as a moving charging station to solve the UAV energy constraint problem, and the UAV as the robotic system in charge of reaching the target points to solve the UGV functionality constraints. To overcome this problem, we formulate a strategy merging combinatorial classic techniques and modern evolutionary approaches aiming to optimize the travelling distance. Our solution has been tested in several simulation runs with different target points distributions. The results demonstrate that our approach is able to generate a coordinated plan for optimizing the hybrid UGV–UAV system in the exploration scenario.},
  doi      = {https://doi.org/10.1016/j.engappai.2018.11.008},
  file     = {:ROPERO2019260 - TERRA_ a Path Planning Algorithm for Cooperative UGV–UAV Exploration.pdf:PDF},
  groups   = {Space Exploration, Heterogeneous},
  keywords = {Exploration, Cooperation, Routing, Heterogeneous robots},
  url      = {https://www.sciencedirect.com/science/article/pii/S095219761830246X},
}

@Article{Stamenkovic2019,
  author    = {V. Stamenkovi{\'{c}} and L. W. Beegle and K. Zacny and D. D. Arumugam and P. Baglioni and N. Barba and J. Baross and M. S. Bell and R. Bhartia and J. G. Blank and P. J. Boston and D. Breuer and W. Brinckerhoff and M. S. Burgin and I. Cooper and V. Cormarkovic and A. Davila and R. M. Davis and C. Edwards and G. Etiope and W. W. Fischer and D. P. Glavin and R. E. Grimm and F. Inagaki and J. L. Kirschvink and A. Kobayashi and T. Komarek and M. Malaska and J. Michalski and B. M{\'{e}}nez and M. Mischna and D. Moser and J. Mustard and T. C. Onstott and V. J. Orphan and M. R. Osburn and J. Plaut and A.-C. Plesa and N. Putzig and K. L. Rogers and L. Rothschild and M. Russell and H. Sapers and B. Sherwood Lollar and T. Spohn and J. D. Tarnas and M. Tuite and D. Viola and L. M. Ward and B. Wilcox and R. Woolley},
  journal   = {Nature Astronomy},
  title     = {The next frontier for planetary and human exploration},
  year      = {2019},
  month     = {jan},
  number    = {2},
  pages     = {116--120},
  volume    = {3},
  doi       = {10.1038/s41550-018-0676-9},
  file      = {:10.1038@s41550-018-0676-9.pdf:PDF},
  groups    = {Space Exploration},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Agha2021,
  author        = {Ali Agha and Kyohei Otsu and Benjamin Morrell and David D. Fan and Rohan Thakker and Angel Santamaria-Navarro and Sung-Kyun Kim and Amanda Bouman and Xianmei Lei and Jeffrey Edlund and Muhammad Fadhil Ginting and Kamak Ebadi and Matthew Anderson and Torkom Pailevanian and Edward Terry and Michael Wolf and Andrea Tagliabue and Tiago Stegun Vaquero and Matteo Palieri and Scott Tepsuporn and Yun Chang and Arash Kalantari and Fernando Chavez and Brett Lopez and Nobuhiro Funabiki and Gregory Miles and Thomas Touma and Alessandro Buscicchio and Jesus Tordesillas and Nikhilesh Alatur and Jeremy Nash and William Walsh and Sunggoo Jung and Hanseob Lee and Christoforos Kanellakis and John Mayo and Scott Harper and Marcel Kaufmann and Anushri Dixit and Gustavo Correa and Carlyn Lee and Jay Gao and Gene Merewether and Jairo Maldonado-Contreras and Gautam Salhotra and Maira Saboia Da Silva and Benjamin Ramtoula and Yuki Kubo and Seyed Fakoorian and Alexander Hatteland and Taeyeon Kim and Tara Bartlett and Alex Stephens and Leon Kim and Chuck Bergh and Eric Heiden and Thomas Lew and Abhishek Cauligi and Tristan Heywood and Andrew Kramer and Henry A. Leopold and Chris Choi and Shreyansh Daftry and Olivier Toupet and Inhwan Wee and Abhishek Thakur and Micah Feras and Giovanni Beltrame and George Nikolakopoulos and David Shim and Luca Carlone and Joel Burdick},
  title         = {NeBula: Quest for Robotic Autonomy in Challenging Environments; TEAM CoSTAR at the DARPA Subterranean Challenge},
  year          = {2021},
  month         = mar,
  abstract      = {This paper presents and discusses algorithms, hardware, and software architecture developed by the TEAM CoSTAR (Collaborative SubTerranean Autonomous Robots), competing in the DARPA Subterranean Challenge. Specifically, it presents the techniques utilized within the Tunnel (2019) and Urban (2020) competitions, where CoSTAR achieved 2nd and 1st place, respectively. We also discuss CoSTAR's demonstrations in Martian-analog surface and subsurface (lava tubes) exploration. The paper introduces our autonomy solution, referred to as NeBula (Networked Belief-aware Perceptual Autonomy). NeBula is an uncertainty-aware framework that aims at enabling resilient and modular autonomy solutions by performing reasoning and decision making in the belief space (space of probability distributions over the robot and world states). We discuss various components of the NeBula framework, including: (i) geometric and semantic environment mapping; (ii) a multi-modal positioning system; (iii) traversability analysis and local planning; (iv) global motion planning and exploration behavior; (i) risk-aware mission planning; (vi) networking and decentralized reasoning; and (vii) learning-enabled adaptation. We discuss the performance of NeBula on several robot types (e.g. wheeled, legged, flying), in various environments. We discuss the specific results and lessons learned from fielding this solution in the challenging courses of the DARPA Subterranean Challenge competition.},
  archiveprefix = {arXiv},
  eprint        = {2103.11470},
  file          = {:Agha2021 - NeBula_ Quest for Robotic Autonomy in Challenging Environments\; TEAM CoSTAR at the DARPA Subterranean Challenge.pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.AI},
  primaryclass  = {cs.RO},
}

@InProceedings{thakker2021autonomous,
  author       = {Thakker, Rohan and Alatur, Nikhilesh and Paton, Michael and Otsu, Kyohei and Toupet, Olivier and Agha-mohammadi, Ali-akbar},
  booktitle    = {Experimental Robotics: The 17th International Symposium},
  title        = {Autonomous Off-road Navigation over Extreme Terrains with Perceptually-challenging Conditions},
  year         = {2021},
  organization = {Springer Nature},
  pages        = {161},
  file         = {:thakker2021autonomous - Autonomous off Road Navigation Over Extreme Terrains with Perceptually Challenging Conditions.pdf:PDF},
  groups       = {Space Exploration},
}

@Article{Abcouwer2020,
  author        = {Neil Abcouwer and Shreyansh Daftry and Siddarth Venkatraman and Tyler del Sesto and Olivier Toupet and Ravi Lanka and Jialin Song and Yisong Yue and Masahiro Ono},
  title         = {Machine Learning Based Path Planning for Improved Rover Navigation (Pre-Print Version)},
  year          = {2020},
  month         = nov,
  abstract      = {Enhanced AutoNav (ENav), the baseline surface navigation software for NASA's Perseverance rover, sorts a list of candidate paths for the rover to traverse, then uses the Approximate Clearance Evaluation (ACE) algorithm to evaluate whether the most highly ranked paths are safe. ACE is crucial for maintaining the safety of the rover, but is computationally expensive. If the most promising candidates in the list of paths are all found to be infeasible, ENav must continue to search the list and run time-consuming ACE evaluations until a feasible path is found. In this paper, we present two heuristics that, given a terrain heightmap around the rover, produce cost estimates that more effectively rank the candidate paths before ACE evaluation. The first heuristic uses Sobel operators and convolution to incorporate the cost of traversing high-gradient terrain. The second heuristic uses a machine learning (ML) model to predict areas that will be deemed untraversable by ACE. We used physics simulations to collect training data for the ML model and to run Monte Carlo trials to quantify navigation performance across a variety of terrains with various slopes and rock distributions. Compared to ENav's baseline performance, integrating the heuristics can lead to a significant reduction in ACE evaluations and average computation time per planning cycle, increase path efficiency, and maintain or improve the rate of successful traverses. This strategy of targeting specific bottlenecks with ML while maintaining the original ACE safety checks provides an example of how ML can be infused into planetary science missions and other safety-critical software.},
  archiveprefix = {arXiv},
  eprint        = {2011.06022},
  file          = {:Abcouwer2020 - Machine Learning Based Path Planning for Improved Rover Navigation (Pre Print Version).pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.LG, I.2.6; I.2.9; I.2.8},
  primaryclass  = {cs.RO},
}

@InProceedings{Toupet2020,
  author    = {Olivier Toupet and Tyler Del Sesto and Masahiro Ono and Steven Myint and Joshua vander Hook and Michael McHenry},
  booktitle = {2020 {IEEE} Aerospace Conference},
  title     = {A {ROS}-based Simulator for Testing the Enhanced Autonomous Navigation of the Mars 2020 Rover},
  year      = {2020},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/aero47225.2020.9172345},
  groups    = {Space Exploration},
}

@PhdThesis{matheron2020,
  author = {Guillaume Matheron},
  title  = {Integrating Motion Planning into Reinforcement Learning to solve hard exploration problems},
  year   = {2020},
  file   = {:matheron2020 - Integrating Motion Planning into Reinforcement Learning to Solve Hard Exploration Problems.pdf:PDF},
  groups = {Information gathering},
}

@Article{Lamarre2020,
  author    = {Lamarre, Olivier and Asghar, Ahmad Bila and Kelly, Jonathan},
  title     = {Impact of Traversability Uncertainty on Global Navigation Planning in Planetary Environments},
  year      = {2020},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  doi       = {10.3929/ETHZ-B-000450119},
  file      = {:Lamarre2020 - Impact of Traversability Uncertainty on Global Navigation Planning in Planetary Environments.pdf:PDF},
  groups    = {Space Exploration},
  keywords  = {Space robotics},
  language  = {en},
  publisher = {ETH Zurich},
}

@Article{Seewald2020,
  author    = {Seewald, Adam},
  title     = {Beyond Traditional Energy Planning: the Weight of Computations in Planetary Exploration},
  year      = {2020},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  doi       = {10.3929/ETHZ-B-000450120},
  file      = {:Seewald2020 - Beyond Traditional Energy Planning_ the Weight of Computations in Planetary Exploration.pdf:PDF},
  groups    = {Space Exploration},
  keywords  = {Space robotics},
  language  = {en},
  publisher = {ETH Zurich},
}

@Article{Kodgule2019,
  author        = {Suhit Kodgule and Alberto Candela and David Wettergreen},
  title         = {Non-myopic Planetary Exploration Combining In Situ and Remote Measurements},
  year          = {2019},
  month         = apr,
  abstract      = {Remote sensing can provide crucial information for planetary rovers. However, they must validate these orbital observations with in situ measurements. Typically, this involves validating hyperspectral data using a spectrometer on-board the field robot. In order to achieve this, the robot must visit sampling locations that jointly improve a model of the environment while satisfying sampling constraints. However, current planners follow sub-optimal greedy strategies that are not scalable to larger regions. We demonstrate how the problem can be effectively defined in an MDP framework and propose a planning algorithm based on Monte Carlo Tree Search, which is devoid of the common drawbacks of existing planners and also provides superior performance. We evaluate our approach using hyperspectral imagery of a well-studied geologic site in Cuprite, Nevada.},
  archiveprefix = {arXiv},
  eprint        = {1904.12255},
  file          = {:Kodgule2019 - Non Myopic Planetary Exploration Combining in Situ and Remote Measurements.pdf:PDF},
  groups        = {Space Exploration},
  keywords      = {cs.RO, cs.AI, cs.LG},
  primaryclass  = {cs.RO},
}

@InProceedings{Candela2020,
  author    = {Alberto Candela and Suhit Kodgule and Kevin Edelson and Srinivasan Vijayarangan and David R. Thompson and Eldar Noe Dobrea and David Wettergreen},
  booktitle = {2020 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  title     = {Planetary Rover Exploration Combining Remote and In Situ Measurements for Active Spectroscopic Mapping},
  year      = {2020},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra40945.2020.9196973},
  file      = {:Candela2020 - Planetary Rover Exploration Combining Remote and in Situ Measurements for Active Spectroscopic Mapping.pdf:PDF},
  groups    = {Space Exploration},
}

@InProceedings{Ono2020,
  author    = {Masahiro Ono and Brandon Rothrock and Kyohei Otsu and Shoya Higa and Yumi Iwashita and Annie Didier and Tanvir Islam and Christopher Laporte and Vivian Sun and Kathryn Stack and Jacek Sawoniewicz and Shreyansh Daftry and Virisha Timmaraju and Sami Sahnoune and Chris A. Mattmann and Olivier Lamarre and Sourish Ghosh and Dicong Qiu and Shunichiro Nomura and Hiya Roy and Hemanth Sarabu and Gabrielle Hedrick and Larkin Folsom and Sean Suehr and Hyoshin Park},
  booktitle = {2020 {IEEE} Aerospace Conference},
  title     = {{MAARS}: Machine learning-based Analytics for Automated Rover Systems},
  year      = {2020},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/aero47225.2020.9172271},
  file      = {:09172271.pdf:PDF},
  groups    = {Space Exploration},
}

@Article{Chen2020,
  author        = {Fanfei Chen and John D. Martin and Yewei Huang and Jinkun Wang and Brendan Englot},
  title         = {Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs},
  year          = {2020},
  month         = jul,
  abstract      = {We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward-simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.},
  archiveprefix = {arXiv},
  eprint        = {2007.12640},
  file          = {:Chen2020 - Autonomous Exploration under Uncertainty Via Deep Reinforcement Learning on Graphs.pdf:PDF},
  groups        = {Information gathering},
  keywords      = {cs.RO, cs.LG},
  primaryclass  = {cs.RO},
}

@InProceedings{10.5555/3237383.3237907,
  author    = {Hong, Zhang-Wei and Su, Shih-Yang and Shann, Tzu-Yun and Chang, Yi-Hsiang and Lee, Chun-Yi},
  booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {A Deep Policy Inference Q-Network for Multi-Agent Systems},
  year      = {2018},
  address   = {Richland, SC},
  pages     = {1388–1396},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '18},
  abstract  = {We present DPIQN, a deep policy inference Q-network that targets multi-agent systems composed of controllable agents, collaborators, and opponents that interact with each other. We focus on one challenging issue in such systems---modeling agents with varying strategies---and propose to employ "policy features'' learned from raw observations (e.g., raw images) of collaborators and opponents by inferring their policies. DPIQN incorporates the learned policy features as a hidden vector into its own deep Q-network (DQN), such that it is able to predict better Q values for the controllable agents than the state-of-the-art deep reinforcement learning models. We further propose an enhanced version of DPIQN, called deep recurrent policy inference Q-network (DRPIQN), for handling partial observability. Both DPIQN and DRPIQN are trained by an adaptive training procedure, which adjusts the network's attention to learn the policy features and its own Q-values at different phases of the training process. We present a comprehensive analysis of DPIQN and DRPIQN, and highlight their effectiveness and generalizability in various multi-agent settings. Our models are evaluated in a classic soccer game involving both competitive and collaborative scenarios. Experimental results performed on 1 vs. 1 and 2 vs. 2 games show that DPIQN and DRPIQN demonstrate superior performance to the baseline DQN and deep recurrent Q-network (DRQN) models. We also explore scenarios in which collaborators or opponents dynamically change their policies, and show that DPIQN and DRPIQN do lead to better overall performance in terms of stability and mean scores.},
  comment   = {DPIQN and DRPIQN},
  file      = {:10.5555_3237383.3237907 - A Deep Policy Inference Q Network for Multi Agent Systems.pdf:PDF},
  groups    = {Multi-agent RL},
  keywords  = {deep reinforcement learning, multi-agent learning, opponent modeling},
  location  = {Stockholm, Sweden},
  numpages  = {9},
}

@Article{Foerster2016_SolveRiddles,
  author        = {Jakob N. Foerster and Yannis M. Assael and Nando de Freitas and Shimon Whiteson},
  title         = {Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks},
  year          = {2016},
  month         = feb,
  abstract      = {We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.},
  archiveprefix = {arXiv},
  eprint        = {1602.02672},
  file          = {:Foerster2016 - Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q Networks.pdf:PDF},
  groups        = {Communication, Independent Learning},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
  priority      = {prio1},
  ranking       = {rank1},
  readstatus    = {skimmed},
}

@InProceedings{Kurek2016,
  author    = {Mateusz Kurek and Wojciech Jaskowski},
  booktitle = {2016 {IEEE} Conference on Computational Intelligence and Games ({CIG})},
  title     = {Heterogeneous team deep q-learning in low-dimensional multi-agent environments},
  year      = {2016},
  month     = {sep},
  publisher = {{IEEE}},
  doi       = {10.1109/cig.2016.7860413},
  file      = {:Kurek2016 - Heterogeneous Team Deep Q Learning in Low Dimensional Multi Agent Environments.pdf:PDF},
  groups    = {Heterogeneous},
}

@Article{Xiao2020,
  author    = {Jian Xiao and Gang Wang and Ying Zhang and Lei Cheng},
  journal   = {{IEEE} Access},
  title     = {A Distributed Multi-Agent Dynamic Area Coverage Algorithm Based on Reinforcement Learning},
  year      = {2020},
  pages     = {33511--33521},
  volume    = {8},
  doi       = {10.1109/access.2020.2967225},
  groups    = {Coverage Path Planning},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Li2020,
  author    = {Sheng Li and Jayesh K. Gupta and Peter Morales and Ross Allen and Mykel J. Kochenderfer},
  title     = {Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning},
  year      = {2021},
  pages     = {764–772},
  abstract  = {Multi-agent reinforcement learning (MARL) requires coordination to efficiently solve certain tasks. Fully centralized control is often infeasible in such domains due to the size of joint action spaces. Coordination graph based formalization allows reasoning about the joint action based on the structure of interactions. However, they often require domain expertise in their design. This paper introduces the deep implicit coordination graph (DICG) architecture for such scenarios. DICG consists of a module for inferring the dynamic coordination graph structure which is then used by a graph neural network based module to learn to implicitly reason about the joint actions or values. DICG allows learning the tradeoff between full centralization and decentralization via standard actor-critic methods to significantly improve coordination for domains with large number of agents. We apply DICG to both centralized-training-centralized-execution and centralized-training-decentralized-execution regimes. We demonstrate that DICG solves the relative overgeneralization pathology in predatory-prey tasks as well as outperforms various MARL baselines on the challenging StarCraft II Multi-agent Challenge (SMAC) and traffic junction environments.},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  file      = {:Li2020 - Deep Implicit Coordination Graphs for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Graphs},
  priority  = {prio1},
}

@InProceedings{Jiang2020Graph,
  author    = {Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},
  booktitle = {International Conference on Learning Representations},
  title     = {Graph Convolutional Reinforcement Learning},
  year      = {2020},
  file      = {:Jiang2020Graph - Graph Convolutional Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Graphs},
  url       = {https://openreview.net/forum?id=HkxdQkSYDB},
}

@Article{Meneghetti2020a,
  author        = {Douglas De Rizzo Meneghetti and Reinaldo Augusto da Costa Bianchi},
  title         = {Specializing Inter-Agent Communication in Heterogeneous Multi-Agent Reinforcement Learning using Agent Class Information},
  year          = {2020},
  month         = dec,
  abstract      = {Inspired by recent advances in agent communication with graph neural networks, this work proposes the representation of multi-agent communication capabilities as a directed labeled heterogeneous agent graph, in which node labels denote agent classes and edge labels, the communication type between two classes of agents. We also introduce a neural network architecture that specializes communication in fully cooperative heterogeneous multi-agent tasks by learning individual transformations to the exchanged messages between each pair of agent classes. By also employing encoding and action selection modules with parameter sharing for environments with heterogeneous agents, we demonstrate comparable or superior performance in environments where a larger number of agent classes operates.},
  archiveprefix = {arXiv},
  eprint        = {2012.07617},
  file          = {:Meneghetti2020a - Specializing Inter Agent Communication in Heterogeneous Multi Agent Reinforcement Learning Using Agent Class Information.pdf:PDF},
  groups        = {Graphs, Communication, Heterogeneous MADRL},
  keywords      = {cs.AI, cs.LG, cs.MA},
  primaryclass  = {cs.AI},
  priority      = {prio2},
}

@Article{Zhou2020,
  author        = {Ming Zhou and Jun Luo and Julian Villella and Yaodong Yang and David Rusu and Jiayu Miao and Weinan Zhang and Montgomery Alban and Iman Fadakar and Zheng Chen and Aurora Chongxi Huang and Ying Wen and Kimia Hassanzadeh and Daniel Graves and Dong Chen and Zhengbang Zhu and Nhat Nguyen and Mohamed Elsayed and Kun Shao and Sanjeevan Ahilan and Baokuan Zhang and Jiannan Wu and Zhengang Fu and Kasra Rezaee and Peyman Yadmellat and Mohsen Rohani and Nicolas Perez Nieves and Yihan Ni and Seyedershad Banijamali and Alexander Cowen Rivers and Zheng Tian and Daniel Palenicek and Haitham bou Ammar and Hongbo Zhang and Wulong Liu and Jianye Hao and Jun Wang},
  title         = {SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving},
  year          = {2020},
  month         = oct,
  abstract      = {Multi-agent interaction is a fundamental aspect of autonomous driving in the real world. Despite more than a decade of research and development, the problem of how to competently interact with diverse road users in diverse scenarios remains largely unsolved. Learning methods have much to offer towards solving this problem. But they require a realistic multi-agent simulator that generates diverse and competent driving interactions. To meet this need, we develop a dedicated simulation platform called SMARTS (Scalable Multi-Agent RL Training School). SMARTS supports the training, accumulation, and use of diverse behavior models of road users. These are in turn used to create increasingly more realistic and diverse interactions that enable deeper and broader research on multi-agent interaction. In this paper, we describe the design goals of SMARTS, explain its basic architecture and its key features, and illustrate its use through concrete multi-agent experiments on interactive scenarios. We open-source the SMARTS platform and the associated benchmark tasks and evaluation metrics to encourage and empower research on multi-agent learning for autonomous driving. Our code is available at https://github.com/huawei-noah/SMARTS.},
  archiveprefix = {arXiv},
  eprint        = {2010.09776},
  file          = {:Zhou2020 - SMARTS_ Scalable Multi Agent Reinforcement Learning Training School for Autonomous Driving.pdf:PDF},
  groups        = {Navigation, Multi-robot},
  keywords      = {cs.MA, cs.AI, cs.GT, cs.LG, cs.SY, eess.SY},
  primaryclass  = {cs.MA},
  ranking       = {rank2},
}

@Article{Mitchell2019,
  author        = {Rupert Mitchell and Jenny Fletcher and Jacopo Panerati and Amanda Prorok},
  title         = {Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous Multi-Lane Driving},
  year          = {2019},
  month         = nov,
  abstract      = {Autonomous driving promises to transform road transport. Multi-vehicle and multi-lane scenarios, however, present unique challenges due to constrained navigation and unpredictable vehicle interactions. Learning-based methods---such as deep reinforcement learning---are emerging as a promising approach to automatically design intelligent driving policies that can cope with these challenges. Yet, the process of safely learning multi-vehicle driving behaviours is hard: while collisions---and their near-avoidance---are essential to the learning process, directly executing immature policies on autonomous vehicles raises considerable safety concerns. In this article, we present a safe and efficient framework that enables the learning of driving policies for autonomous vehicles operating in a shared workspace, where the absence of collisions cannot be guaranteed. Key to our learning procedure is a sim2real approach that uses real-world online policy adaptation in a mixed-reality setup, where other vehicles and static obstacles exist in the virtual domain. This allows us to perform safe learning by simulating (and learning from) collisions between the learning agent(s) and other objects in virtual reality. Our results demonstrate that, after only a few runs in mixed-reality, collisions are significantly reduced.},
  archiveprefix = {arXiv},
  eprint        = {1911.11699},
  file          = {:Mitchell2019 - Multi Vehicle Mixed Reality Reinforcement Learning for Autonomous Multi Lane Driving.pdf:PDF},
  groups        = {Navigation},
  keywords      = {cs.RO, cs.AI, cs.LG, cs.MA, I.2.6; I.2.9},
  primaryclass  = {cs.RO},
}

@Article{Omidshafiei2019,
  author    = {Shayegan Omidshafiei and Dong-Ki Kim and Miao Liu and Gerald Tesauro and Matthew Riemer and Christopher Amato and Murray Campbell and Jonathan P. How},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Learning to Teach in Cooperative Multiagent Reinforcement Learning},
  year      = {2019},
  month     = {jul},
  pages     = {6128--6136},
  volume    = {33},
  doi       = {10.1609/aaai.v33i01.33016128},
  file      = {:4570-Article Text-7609-1-10-20190707.pdf:PDF},
  groups    = {Multi-agent RL},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  ranking   = {rank2},
}

@Article{Foerster2018_COMA,
  author       = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Counterfactual Multi-Agent Policy Gradients},
  year         = {2018},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state. &lt;/p&gt;},
  comment      = {COMA},
  file         = {:Foerster_Farquhar_Afouras_Nardelli_Whiteson_2018 - Counterfactual Multi Agent Policy Gradients.pdf:PDF},
  groups       = {Multi-agent RL, Credit Assignment},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11794},
}

@InProceedings{10.5555/3306127.3332052,
  author    = {Samvelyan, Mikayel and Rashid, Tabish and Schroeder de Witt, Christian and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
  booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {The StarCraft Multi-Agent Challenge},
  year      = {2019},
  address   = {Richland, SC},
  pages     = {2186–2188},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '19},
  abstract  = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.},
  file      = {:10.5555_3306127.3332052 - The StarCraft Multi Agent Challenge (2).pdf:PDF},
  groups    = {Multi-agent RL},
  isbn      = {9781450363099},
  keywords  = {multi-agent learning, starcraft, reinforcement learning},
  location  = {Montreal QC, Canada},
  numpages  = {3},
  ranking   = {rank2},
}

@Article{Shalev2016_AutonomousDriving,
  author        = {Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
  title         = {Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},
  year          = {2016},
  month         = oct,
  abstract      = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further.},
  archiveprefix = {arXiv},
  eprint        = {1610.03295},
  file          = {:- Safe, Multi Agent, Reinforcement Learning for Autonomous Driving.pdf:PDF},
  groups        = {Navigation, Multi-agent Systems},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
  ranking       = {rank3},
}

@Article{Wang2020d,
  author        = {Rose E. Wang and Michael Everett and Jonathan P. How},
  journal       = {Reinforcement Learning for Real Life (RL4RealLife) Workshop in the 36th International Conference on Machine Learning, Long Beach, California, USA, 2019},
  title         = {R-MADDPG for Partially Observable Environments and Limited Communication},
  year          = {2020},
  month         = feb,
  abstract      = {There are several real-world tasks that would benefit from applying multiagent reinforcement learning (MARL) algorithms, including the coordination among self-driving cars. The real world has challenging conditions for multiagent learning systems, such as its partial observable and nonstationary nature. Moreover, if agents must share a limited resource (e.g. network bandwidth) they must all learn how to coordinate resource use. This paper introduces a deep recurrent multiagent actor-critic framework (R-MADDPG) for handling multiagent coordination under partial observable set-tings and limited communication. We investigate recurrency effects on performance and communication use of a team of agents. We demonstrate that the resulting framework learns time dependencies for sharing missing observations, handling resource limitations, and developing different communication patterns among agents.},
  archiveprefix = {arXiv},
  eprint        = {2002.06684},
  file          = {:Wang2020a - R MADDPG for Partially Observable Environments and Limited Communication.pdf:PDF},
  groups        = {Multi-agent RL, Communication},
  keywords      = {cs.MA, cs.AI},
  primaryclass  = {cs.MA},
  priority      = {prio2},
  ranking       = {rank1},
}

@InProceedings{Jiang2018_ATOC,
  author     = {Jiang, Jiechuan and Lu, Zongqing},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Learning Attentional Communication for Multi-Agent Cooperation},
  year       = {2018},
  editor     = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {31},
  comment    = {ATOC},
  file       = {:NEURIPS2018_6a8018b3 - Learning Attentional Communication for Multi Agent Cooperation (1).pdf:PDF},
  groups     = {Multi-agent RL, Communication},
  ranking    = {rank3},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper/2018/file/6a8018b3a00b69c008601b8becae392b-Paper.pdf},
}

@Article{Stone2000_MASMachineLearning,
  author    = {Peter Stone and Manuela Veloso},
  journal   = {Autonomous Robots},
  title     = {Multiagent Systems: A Survey from a Machine Learning Perspective},
  year      = {2000},
  number    = {3},
  pages     = {345--383},
  volume    = {8},
  comment   = {Survey MAS organizing them along two axes: degree of heterogeneity and degree of communication (homogeneous
non-communicating agents; heterogeneous non-communicating agents; and heterogeneous communicating
agents)},
  doi       = {10.1023/a:1008942012299},
  file      = {:Stone2000 - Multiagent Systems_ a Survey from a Machine Learning Perspective.pdf:PDF},
  groups    = {MAS Reviews},
  publisher = {Springer Science and Business Media {LLC}},
  ranking   = {rank4},
}

@Article{Wakilpoor2020,
  author        = {Ceyer Wakilpoor and Patrick J. Martin and Carrie Rebhuhn and Amanda Vu},
  title         = {Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment Mapping},
  year          = {2020},
  month         = oct,
  abstract      = {Reinforcement learning in heterogeneous multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in homogeneous settings and simple benchmarks. In this work, we present an actor-critic algorithm that allows a team of heterogeneous agents to learn decentralized control policies for covering an unknown environment. This task is of interest to national security and emergency response organizations that would like to enhance situational awareness in hazardous areas by deploying teams of unmanned aerial vehicles. To solve this multi-agent coverage path planning problem in unknown environments, we augment a multi-agent actor-critic architecture with a new state encoding structure and triplet learning loss to support heterogeneous agent learning. We developed a simulation environment that includes real-world environmental factors such as turbulence, delayed communication, and agent loss, to train teams of agents as well as probe their robustness and flexibility to such disturbances.},
  archiveprefix = {arXiv},
  eprint        = {2010.02663},
  file          = {:Wakilpoor2020 - Heterogeneous Multi Agent Reinforcement Learning for Unknown Environment Mapping.pdf:PDF},
  groups        = {Heterogeneous MADRL},
  keywords      = {cs.MA, cs.AI},
  primaryclass  = {cs.MA},
  readstatus    = {skimmed},
}

@InProceedings{10.5555/3237383.3237451,
  author    = {Palmer, Gregory and Tuyls, Karl and Bloembergen, Daan and Savani, Rahul},
  booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Lenient Multi-Agent Deep Reinforcement Learning},
  year      = {2018},
  address   = {Richland, SC},
  pages     = {443–451},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '18},
  abstract  = {Much of the success of single agent deep reinforcement learning (DRL) in recent years can be attributed to the use of experience replay memories (ERM), which allow Deep Q-Networks (DQNs) to be trained efficiently through sampling stored state transitions. However, care is required when using ERMs for multi-agent deep reinforcement learning (MA-DRL), as stored transitions can become outdated when agents update their policies in parallel citefoerster2017stabilising. In this work we apply leniency citepanait2006lenient to MA-DRL. Lenient agents map state-action pairs to decaying temperature values that control the amount of leniency applied towards negative policy updates that are sampled from the ERM. This introduces optimism in the value-function update, and has been shown to facilitate cooperation in tabular fully-cooperative multi-agent reinforcement learning problems. We evaluate our Lenient-DQN (LDQN) empirically against the related Hysteretic-DQN (HDQN) algorithm citeomidshafiei2017deep as well as a modified version we call scheduled -HDQN, that uses average reward learning near terminal states. Evaluations take place in extended variations of the Coordinated Multi-Agent Object Transportation Problem (CMOTP) citebucsoniu2010multi. We find that LDQN agents are more likely to converge to the optimal policy in a stochastic reward CMOTP compared to standard and scheduled-HDQN agents.},
  file      = {:10.5555_3237383.3237451 - Lenient Multi Agent Deep Reinforcement Learning (1).pdf:PDF},
  groups    = {Multi-agent RL},
  keywords  = {leniency, multi-agent deep reinforcement learning},
  location  = {Stockholm, Sweden},
  numpages  = {9},
  ranking   = {rank1},
}

@InProceedings{Raileanu2018,
  author    = {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Modeling Others using Oneself in Multi-Agent Reinforcement Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {4257--4266},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.},
  file      = {:Raileanu2018 - Modeling Others Using Oneself in Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf},
  ranking   = {rank1},
  url       = {http://proceedings.mlr.press/v80/raileanu18a.html},
}

@Article{Heinrich2016_NSFP,
  author        = {Johannes Heinrich and David Silver},
  title         = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},
  year          = {2016},
  month         = mar,
  abstract      = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.},
  archiveprefix = {arXiv},
  comment       = {NFSP},
  eprint        = {1603.01121},
  file          = {:Heinrich2016 - Deep Reinforcement Learning from Self Play in Imperfect Information Games.pdf:PDF},
  groups        = {Multi-agent RL, Fictituous Play},
  keywords      = {cs.LG, cs.AI, cs.GT},
  primaryclass  = {cs.LG},
  ranking       = {rank3},
}

@InProceedings{Rabinowitz2018,
  author    = {Rabinowitz, Neil and Perbet, Frank and Song, Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Machine Theory of Mind},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {4218--4227},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {–} a ToMnet {–} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test of recognising that others can hold false beliefs about the world.},
  file      = {:Rabinowitz2018 - Machine Theory of Mind.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v80/rabinowitz18a/rabinowitz18a.pdf},
  ranking   = {rank2},
  url       = {http://proceedings.mlr.press/v80/rabinowitz18a.html},
}

@Book{Oliehoek2016_DecPOMDP,
  author     = {Oliehoek, Frans A and Amato, Christopher},
  publisher  = {Springer},
  title      = {A concise introduction to decentralized POMDPs},
  year       = {2016},
  file       = {:oliehoek2016concise - A Concise Introduction to Decentralized POMDPs.pdf:PDF},
  groups     = {MAS Reviews},
  ranking    = {rank4},
  readstatus = {skimmed},
  url        = {https://www.ccis.northeastern.edu/home/camato/publications/OliehoekAmato16book.pdf},
}

@InProceedings{9196684,
  author     = {Xiao, Yuchen and Hoffman, Joshua and Xia, Tian and Amato, Christopher},
  booktitle  = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title      = {Learning Multi-Robot Decentralized Macro-Action-Based Policies via a Centralized Q-Net},
  year       = {2020},
  month      = {May},
  pages      = {10695-10701},
  abstract   = {In many real-world multi-robot tasks, high-quality solutions often require a team of robots to perform asynchronous actions under decentralized control. Decentralized multi-agent reinforcement learning methods have difficulty learning decentralized policies because of the environment appearing to be non-stationary due to other agents also learning at the same time. In this paper, we address this challenge by proposing a macro-action-based decentralized multi-agent double deep recurrent Q-net (MacDec-MADDRQN) which trains each decentralized Q-net using a centralized Q-net for action selection. A generalized version of MacDec-MADDRQN with two separate training environments, called Parallel-MacDec-MADDRQN, is also presented to leverage either centralized or decentralized exploration. The advantages and the practical nature of our methods are demonstrated by achieving near-centralized results in simulation and having real robots accomplish a warehouse tool delivery task in an efficient way.},
  doi        = {10.1109/ICRA40945.2020.9196684},
  file       = {:9196684 - Learning Multi Robot Decentralized Macro Action Based Policies Via a Centralized Q Net.pdf:PDF},
  groups     = {Multi-robot, Macro-actions},
  issn       = {2577-087X},
  readstatus = {read},
}

@InProceedings{Xiao2020a,
  author    = {Xiao, Yuchen and Hoffman, Joshua and Amato, Christopher},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Macro-Action-Based Deep Multi-Agent Reinforcement Learning},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {1146--1161},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {In real-world multi-robot systems, performing high-quality, collaborative behaviors requires robots to asynchronously reason about high-level action selection at varying time durations. Macro-Action Decentralized Partially Observable Markov Decision Processes (MacDec-POMDPs) provide a general framework for asynchronous decision making under uncertainty in fully cooperative multi-agent tasks. However, multi-agent deep reinforcement learning methods have only been developed for (synchronous) primitive-action problems. This paper proposes two Deep Q-Network (DQN) based methods for learning decentralized and centralized macro-action-value functions with novel macro-action trajectory replay buffers introduced for each case. Evaluations on benchmark problems and a larger domain demonstrate the advantage of learning with macro-actions over primitive-actions and the scalability of our approaches.},
  file      = {:Xiao2020a - Macro Action Based Deep Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Macro-actions},
  pdf       = {http://proceedings.mlr.press/v100/xiao20a/xiao20a.pdf},
  url       = {http://proceedings.mlr.press/v100/xiao20a.html},
}

@Article{Wang2020e,
  author        = {Tonghan Wang and Tarun Gupta and Anuj Mahajan and Bei Peng and Shimon Whiteson and Chongjie Zhang},
  title         = {RODE: Learning Roles to Decompose Multi-Agent Tasks},
  year          = {2020},
  month         = oct,
  abstract      = {Role-based learning holds the promise of achieving scalable multi-agent learning by decomposing complex tasks using roles. However, it is largely unclear how to efficiently discover such a set of roles. To solve this problem, we propose to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi-level learning hierarchy -- the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. We further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, our method (1) outperforms the current state-of-the-art MARL algorithms on 10 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and (2) achieves rapid transfer to new environments with three times the number of agents. Demonstrative videos are available at https://sites.google.com/view/rode-marl .},
  archiveprefix = {arXiv},
  eprint        = {2010.01523},
  file          = {:Wang2020b - RODE_ Learning Roles to Decompose Multi Agent Tasks.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank1},
}

@Article{Liu2021,
  author        = {Bo Liu and Qiang Liu and Peter Stone and Animesh Garg and Yuke Zhu and Animashree Anandkumar},
  title         = {Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team Composition},
  year          = {2021},
  month         = may,
  abstract      = {In real-world multiagent systems, agents with different capabilities may join or leave without altering the team's overarching goals. Coordinating teams with such dynamic composition is challenging: the optimal team strategy varies with the composition. We propose COPA, a coach-player framework to tackle this problem. We assume the coach has a global view of the environment and coordinates the players, who only have partial views, by distributing individual strategies. Specifically, we 1) adopt the attention mechanism for both the coach and the players; 2) propose a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with the players. We validate our methods on a resource collection task, a rescue game, and the StarCraft micromanagement tasks. We demonstrate zero-shot generalization to new team compositions. Our method achieves comparable or better performance than the setting where all players have a full view of the environment. Moreover, we see that the performance remains high even when the coach communicates as little as 13% of the time using the adaptive communication strategy.},
  archiveprefix = {arXiv},
  eprint        = {2105.08692},
  file          = {:Liu2021 - Coach Player Multi Agent Reinforcement Learning for Dynamic Team Composition.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Lazaridou2020,
  author        = {Angeliki Lazaridou and Marco Baroni},
  title         = {Emergent Multi-Agent Communication in the Deep Learning Era},
  year          = {2020},
  month         = jun,
  abstract      = {The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they also can develop a shared language to interact. From a scientific perspective, understanding the conditions under which language evolves in communities of deep agents and its emergent features can shed light on human language evolution. From an applied perspective, endowing deep networks with the ability to solve problems interactively by communicating with each other and with us should make them more flexible and useful in everyday life. This article surveys representative recent language emergence studies from both of these two angles.},
  archiveprefix = {arXiv},
  eprint        = {2006.02419},
  file          = {:Lazaridou2020 - Emergent Multi Agent Communication in the Deep Learning Era.pdf:PDF},
  groups        = {Communication},
  keywords      = {cs.CL, cs.AI},
  primaryclass  = {cs.CL},
  priority      = {prio2},
  ranking       = {rank1},
  readstatus    = {read},
}

@InProceedings{Chen2020a,
  author    = {Chen, Dian and Zhou, Brady and Koltun, Vladlen and Kr\"ahenb\"uhl, Philipp},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Learning by Cheating},
  year      = {2020},
  editor    = {Leslie Pack Kaelbling and Danica Kragic and Komei Sugiura},
  month     = {30 Oct--01 Nov},
  pages     = {66--75},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art.},
  file      = {:pmlr-v100-chen20a - Learning by Cheating.pdf:PDF},
  groups    = {Navigation},
  pdf       = {http://proceedings.mlr.press/v100/chen20a/chen20a.pdf},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v100/chen20a.html},
}

@InProceedings{Haarnoja2017_SoftQLearning,
  author    = {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Reinforcement Learning with Deep Energy-Based Policies},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {06--11 Aug},
  pages     = {1352--1361},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  file      = {:pmlr-v70-haarnoja17a - Reinforcement Learning with Deep Energy Based Policies.pdf:PDF},
  groups    = {Soft Actor Critic},
  pdf       = {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v70/haarnoja17a.html},
}

@Article{OpenAI2019_DOTA2,
  author        = {OpenAI and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique P. d. O. Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
  title         = {Dota 2 with Large Scale Deep Reinforcement Learning},
  year          = {2019},
  month         = dec,
  abstract      = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arXiv},
  eprint        = {1912.06680},
  file          = {:OpenAI2019 - Dota 2 with Large Scale Deep Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
  readstatus    = {skimmed},
}

@InProceedings{Singh2019_IC3Net,
  author     = {Amanpreet Singh and Tushar Jain and Sainbayar Sukhbaatar},
  booktitle  = {International Conference on Learning Representations},
  title      = {Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks},
  year       = {2019},
  abstract   = {Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multi-agent scenarios, but have been restricted to fully-cooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards foreach agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability.},
  comment    = {IC3Net},
  file       = {:Singh2018 - Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks.pdf:PDF},
  groups     = {Communication},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=rye7knCqK7},
}

@InProceedings{Kottur2017,
  author     = {Satwik Kottur and Jos{\'{e}} Moura and Stefan Lee and Dhruv Batra},
  booktitle  = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  title      = {Natural Language Does Not Emerge `Naturally' in Multi-Agent Dialog},
  year       = {2017},
  publisher  = {Association for Computational Linguistics},
  doi        = {10.18653/v1/d17-1321},
  file       = {:Kottur2017 - Natural Language Does Not Emerge `Naturally' in Multi Agent Dialog.pdf:PDF},
  groups     = {Communication},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@InProceedings{Hoshen2017_VAIN,
  author     = {Hoshen, Yedid},
  booktitle  = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title      = {VAIN: Attentional Multi-Agent Predictive Modeling},
  year       = {2017},
  address    = {Red Hook, NY, USA},
  pages      = {2698–2708},
  publisher  = {Curran Associates Inc.},
  series     = {NIPS'17},
  abstract   = {Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.},
  file       = {:10.5555_3294996.3295030 - VAIN_ Attentional Multi Agent Predictive Modeling.pdf:PDF},
  groups     = {Communication, Interaction Networks},
  location   = {Long Beach, California, USA},
  numpages   = {11},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/748ba69d3e8d1af87f84fee909eef339-Abstract.html},
}

@InProceedings{Das2018,
  author     = {Das, Abhishek and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle  = {Proceedings of The 2nd Conference on Robot Learning},
  title      = {Neural Modular Control for Embodied Question Answering},
  year       = {2018},
  editor     = {Aude Billard and Anca Dragan and Jan Peters and Jun Morimoto},
  month      = {29--31 Oct},
  pages      = {53--62},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {87},
  abstract   = {We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. ‘exit room’, ‘find kitchen’, ‘find refrigerator’, etc.). We use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies. On the challenging EQA [1] benchmark in House3D [2], requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.},
  file       = {:pmlr-v87-das18a - Neural Modular Control for Embodied Question Answering.pdf:PDF},
  groups     = {Hierarchical RL, Macro-actions},
  pdf        = {http://proceedings.mlr.press/v87/das18a/das18a.pdf},
  ranking    = {rank2},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v87/das18a.html},
}

@InProceedings{Zhang2018_NetworkedAgents,
  author     = {Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Basar, Tamer},
  booktitle  = {Proceedings of the 35th International Conference on Machine Learning},
  title      = {Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents},
  year       = {2018},
  editor     = {Dy, Jennifer and Krause, Andreas},
  month      = {10--15 Jul},
  pages      = {5872--5881},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {80},
  abstract   = {We consider the fully decentralized multi-agent reinforcement learning (MARL) problem, where the agents are connected via a time-varying and possibly sparse communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. To maximize the globally averaged return over the network, we propose two fully decentralized actor-critic algorithms, which are applicable to large-scale MARL problems in an online fashion. Convergence guarantees are provided when the value functions are approximated within the class of linear functions. Our work appears to be the first theoretical study of fully decentralized MARL algorithms for networked agents that use function approximation.},
  file       = {:pmlr-v80-zhang18n - Fully Decentralized Multi Agent Reinforcement Learning with Networked Agents.pdf:PDF},
  groups     = {Multi-agent RL, Communication},
  pdf        = {http://proceedings.mlr.press/v80/zhang18n/zhang18n.pdf},
  ranking    = {rank3},
  readstatus = {skimmed},
  url        = {http://proceedings.mlr.press/v80/zhang18n.html},
}

@InProceedings{8619581,
  author    = {Zhang, Kaiqing and Yang, Zhuoran and Basar, Tamer},
  booktitle = {2018 IEEE Conference on Decision and Control (CDC)},
  title     = {Networked Multi-Agent Reinforcement Learning in Continuous Spaces},
  year      = {2018},
  month     = {Dec},
  pages     = {2771-2776},
  abstract  = {Many real-world tasks on practical control systems involve the learning and decision-making of multiple agents, under limited communications and observations. In this paper, we study the problem of networked multi-agent reinforcement learning (MARL), where multiple agents perform reinforcement learning in a common environment, and are able to exchange information via a possibly time-varying communication network. In particular, we focus on a collaborative MARL setting where each agent has individual reward functions, and the objective of all the agents is to maximize the network-wide averaged long-term return. To this end, we propose a fully decentralized actor-critic algorithm that only relies on neighbor-to-neighbor communications among agents. To promote the use of the algorithm on practical control systems, we focus on the setting with continuous state and action spaces, and adopt the newly proposed expected policy gradient to reduce the variance of the gradient estimate. We provide convergence guarantees for the algorithm when linear function approximation is employed, and corroborate our theoretical results via simulations.},
  doi       = {10.1109/CDC.2018.8619581},
  file      = {:zhang2018.pdf:PDF},
  groups    = {Multi-agent RL, Communication},
  issn      = {2576-2370},
}

@Article{Zhang2019,
  author        = {Kaiqing Zhang and Zhuoran Yang and Tamer Başar},
  title         = {Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},
  year          = {2019},
  month         = nov,
  abstract      = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
  archiveprefix = {arXiv},
  eprint        = {1911.10635},
  file          = {:Zhang2019 - Multi Agent Reinforcement Learning_ a Selective Overview of Theories and Algorithms.pdf:PDF},
  groups        = {MAS Reviews},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank3},
}

@InProceedings{pittir39099,
  author    = {Akshat Agarwal and Sumit Kumar and Katia Sycara and Michael Lewis},
  booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS'2020)},
  title     = {Learning Transferable Cooperative Behavior in Multi-Agent Team},
  year      = {2020},
  address   = {Aukland, NZ},
  publisher = {IFMAS},
  abstract  = {While multi-agent interactions can be naturally modeled as a graph, the environment has traditionally been considered as a black box. To better utilize the inherent structure of our environment, we propose to create a shared agent-entity graph, where agents and environmental entities form vertices, and edges exist between the vertices which can communicate with each other, allowing agents to selectively attend to different parts of the environment, while also introducing invariance to the number of agents or entities present in the system as well as permutation invariance. We present stateof- the-art results on coverage, formation and line control tasks for multi-agent teams in a fully decentralized execution framework.},
  file      = {:pittir39099 - Learning Transferable Cooperative Behavior in Multi Agent Team (1).pdf:PDF},
  groups    = {Graphs, Communication},
  journal   = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS'2020)},
  priority  = {prio2},
  ranking   = {rank1},
  url       = {http://d-scholarship.pitt.edu/39099/},
}

@InProceedings{8929168,
  author    = {Luo, Tianze and Subagdja, Budhitama and Wang, Di and Tan, Ah-Hwee},
  booktitle = {2019 IEEE International Conference on Agents (ICA)},
  title     = {Multi-Agent Collaborative Exploration through Graph-based Deep Reinforcement Learning},
  year      = {2019},
  month     = {Oct},
  pages     = {2-7},
  abstract  = {Autonomous exploration by a single or multiple agents in an unknown environment leads to various applications in automation, such as cleaning, search and rescue, etc. Traditional methods normally take frontier locations and segmented regions of the environment into account to efficiently allocate target locations to different agents to visit. They may employ ad hoc solutions to allocate the task to the agents, but the allocation may not be efficient. In the literature, few studies focused on enhancing the traditional methods by applying machine learning models for agent performance improvement. In this paper, we propose a graph-based deep reinforcement learning approach to effectively perform multi-agent exploration. Specifically, we first design a hierarchical map segmentation method to transform the environment exploration problem to the graph domain, wherein each node of the graph corresponds to a segmented region in the environment and each edge indicates the distance between two nodes. Subsequently, based on the graph structure, we apply a Graph Convolutional Network (GCN) to allocate the exploration target to each agent. Our experiments show that our proposed model significantly improves the efficiency of map explorations across varying sizes of collaborative agents over the traditional methods.},
  doi       = {10.1109/AGENTS.2019.8929168},
  file      = {:8929168 - Multi Agent Collaborative Exploration through Graph Based Deep Reinforcement Learning.pdf:PDF},
  groups    = {Graphs},
}

@Article{Meneghetti2020,
  author        = {Douglas De Rizzo Meneghetti and Reinaldo Augusto da Costa Bianchi},
  title         = {Towards Heterogeneous Multi-Agent Reinforcement Learning with Graph Neural Networks},
  year          = {2020},
  month         = sep,
  abstract      = {This work proposes a neural network architecture that learns policies for multiple agent classes in a heterogeneous multi-agent reinforcement setting. The proposed network uses directed labeled graph representations for states, encodes feature vectors of different sizes for different entity classes, uses relational graph convolution layers to model different communication channels between entity types and learns distinct policies for different agent classes, sharing parameters wherever possible. Results have shown that specializing the communication channels between entity classes is a promising step to achieve higher performance in environments composed of heterogeneous entities.},
  archiveprefix = {arXiv},
  doi           = {10.5753/eniac.2020.12161},
  eprint        = {2009.13161},
  file          = {:Meneghetti2020 - Towards Heterogeneous Multi Agent Reinforcement Learning with Graph Neural Networks.pdf:PDF},
  groups        = {Graphs, Communication, Heterogeneous MADRL},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
  priority      = {prio2},
  readstatus    = {skimmed},
}

@Article{Ryu2020,
  author    = {Heechang Ryu and Hayong Shin and Jinkyoo Park},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Multi-Agent Actor-Critic with Hierarchical Graph Attention Network},
  year      = {2020},
  month     = {apr},
  number    = {05},
  pages     = {7236--7243},
  volume    = {34},
  doi       = {10.1609/aaai.v34i05.6214},
  file      = {:6214-Article Text-9439-1-10-20200516.pdf:PDF},
  groups    = {Graphs},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@InProceedings{Havrylov2017_EmergenceLang,
  author     = {Havrylov, Serhii and Titov, Ivan},
  booktitle  = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title      = {Emergence of Language with Multi-Agent Games: Learning to Communicate with Sequences of Symbols},
  year       = {2017},
  address    = {Red Hook, NY, USA},
  pages      = {2146–2156},
  publisher  = {Curran Associates Inc.},
  series     = {NIPS'17},
  abstract   = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator (Jang et al., 2017)) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
  file       = {:10.5555_3294771.3294976 - Emergence of Language with Multi Agent Games_ Learning to Communicate with Sequences of Symbols.pdf:PDF},
  groups     = {Discrete language, Communication, Language-Grounded Communication},
  location   = {Long Beach, California, USA},
  numpages   = {11},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/70222949cc0db89ab32c9969754d4758-Abstract.html},
}

@InProceedings{Resnick2020_Emergent,
  author     = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
  booktitle  = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Capacity, Bandwidth, and Compositionality in Emergent Language Learning},
  year       = {2020},
  address    = {Richland, SC},
  pages      = {1125–1133},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '20},
  abstract   = {Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
  file       = {:10.5555_3398761.3398892 - Capacity, Bandwidth, and Compositionality in Emergent Language Learning.pdf:PDF},
  groups     = {NLP},
  isbn       = {9781450375184},
  keywords   = {multi-agent communication, compositionality, emergent languages},
  location   = {Auckland, New Zealand},
  numpages   = {9},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://nyuscholars.nyu.edu/en/publications/capacity-bandwidth-and-compositionality-in-emergent-language-lear},
}

@InProceedings{harding-graesser-etal-2019-emergent,
  author    = {Harding Graesser, Laura and Cho, Kyunghyun and Kiela, Douwe},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Emergent Linguistic Phenomena in Multi-Agent Communication Games},
  year      = {2019},
  address   = {Hong Kong, China},
  month     = nov,
  pages     = {3700--3710},
  publisher = {Association for Computational Linguistics},
  abstract  = {We describe a multi-agent communication framework for examining high-level linguistic phenomena at the community-level. We demonstrate that complex linguistic behavior observed in natural language can be reproduced in this simple setting: i) the outcome of contact between communities is a function of inter- and intra-group connectivity; ii) linguistic contact either converges to the majority protocol, or in balanced cases leads to novel creole languages of lower complexity; and iii) a linguistic continuum emerges where neighboring languages are more mutually intelligible than farther removed languages. We conclude that at least some of the intricate properties of language evolution need not depend on complex evolved linguistic capabilities, but can emerge from simple social exchanges between perceptually-enabled agents playing communication games.},
  doi       = {10.18653/v1/D19-1384},
  file      = {:harding-graesser-etal-2019-emergent - Emergent Linguistic Phenomena in Multi Agent Communication Games.pdf:PDF},
  groups    = {Communication},
  ranking   = {rank1},
  url       = {https://www.aclweb.org/anthology/D19-1384},
}

@InProceedings{Cao2018_Negotiation,
  author     = {Kris Cao and Angeliki Lazaridou and Marc Lanctot and Joel Z Leibo and Karl Tuyls and Stephen Clark},
  booktitle  = {International Conference on Learning Representations},
  title      = {Emergent Communication through Negotiation},
  year       = {2018},
  file       = {:cao2018emergent - Emergent Communication through Negotiation (1).pdf:PDF},
  groups     = {Communication, Discrete language},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=Hk6WhagRW},
}

@InProceedings{evtimova2018emergent,
  author     = {Katrina Evtimova and Andrew Drozdov and Douwe Kiela and Kyunghyun Cho},
  booktitle  = {International Conference on Learning Representations},
  title      = {Emergent Communication in a Multi-Modal, Multi-Step Referential Game},
  year       = {2018},
  comment    = {Multi-step communication of bit-strings in referential game with image on the sender side and text on the receiver side.},
  file       = {:evtimova2018emergent - Emergent Communication in a Multi Modal, Multi Step Referential Game.pdf:PDF},
  groups     = {Communication, Discrete language},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=rJGZq6g0-},
}

@InProceedings{Lazaridou2018_Emergence,
  author     = {Angeliki Lazaridou and Karl Moritz Hermann and Karl Tuyls and Stephen Clark},
  booktitle  = {International Conference on Learning Representations (ICLR)},
  title      = {Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input},
  year       = {2018},
  file       = {:lazaridou2018emergence - Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input.pdf:PDF},
  groups     = {Discrete language, Communication},
  priority   = {prio1},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=HJGv1Z-AW},
}

@InProceedings{kharitonov2020entropy,
  author       = {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
  booktitle    = {International Conference on Machine Learning},
  title        = {Entropy minimization in emergent languages},
  year         = {2020},
  organization = {PMLR},
  pages        = {5220--5230},
  file         = {:kharitonov2020entropy - Entropy Minimization in Emergent Languages.pdf:PDF},
  groups       = {Communication, Speaker-Listener},
  priority     = {prio2},
}

@InProceedings{Lowe2019_Pitfalls,
  author     = {Lowe, Ryan and Foerster, Jakob and Boureau, Y-Lan and Pineau, Joelle and Dauphin, Yann},
  booktitle  = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {On the Pitfalls of Measuring Emergent Communication},
  year       = {2019},
  address    = {Richland, SC},
  pages      = {693–701},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '19},
  abstract   = {How do we know if communication is emerging in a multi-agent system? The vast majority of recent papers on emergent communication show that adding a communication channel leads to an increase in reward or task success. This is a useful indicator, but provides only a coarse measure of the agent's learned communication abilities. As we move towards more complex environments, it becomes imperative to have a set of finer tools that allow qualitative and quantitative insights into the emergence of communication. This may be especially useful to allow humans to monitor agents' behaviour, whether for fault detection, assessing performance, or even building trust. In this paper, we examine a few intuitive existing metrics for measuring communication, and show that they can be misleading. Specifically, by training deep reinforcement learning agents to play simple matrix games augmented with a communication channel, we find a scenario where agents appear to communicate (their messages provide information about their subsequent action), and yet the messages do not impact the environment or other agent in any way. We explain this phenomenon using ablation studies and by visualizing the representations of the learned policies. We also survey some commonly used metrics for measuring emergent communication, and provide recommendations as to when these metrics should be used.},
  file       = {:10.5555_3306127.3331757 - On the Pitfalls of Measuring Emergent Communication.pdf:PDF},
  groups     = {Communication},
  keywords   = {deep learning, learning agent capabilities, multi-agent learning},
  location   = {Montreal QC, Canada},
  numpages   = {9},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.5555/3306127.3331757},
}

@InProceedings{chaabouni-etal-2020-compositionality,
  author     = {Chaabouni, Rahma and Kharitonov, Eugene and Bouchacourt, Diane and Dupoux, Emmanuel and Baroni, Marco},
  booktitle  = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title      = {Compositionality and Generalization In Emergent Languages},
  year       = {2020},
  address    = {Online},
  month      = jul,
  pages      = {4427--4442},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.},
  comment    = {Introduces metrics to measure compositionality},
  doi        = {10.18653/v1/2020.acl-main.407},
  file       = {:chaabouni-etal-2020-compositionality - Compositionality and Generalization in Emergent Languages.pdf:PDF},
  groups     = {Communication},
  priority   = {prio2},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://www.aclweb.org/anthology/2020.acl-main.407},
}

@Article{Crandall2018,
  author     = {Jacob W. Crandall and Mayada Oudah and Tennom and Fatimah Ishowo-Oloko and Sherief Abdallah and Jean-Fran{\c{c}}ois Bonnefon and Manuel Cebrian and Azim Shariff and Michael A. Goodrich and Iyad Rahwan},
  journal    = {Nature Communications},
  title      = {Cooperating with machines},
  year       = {2018},
  month      = {jan},
  number     = {1},
  volume     = {9},
  doi        = {10.1038/s41467-017-02597-8},
  file       = {:Crandall2018 - Cooperating with Machines.pdf:PDF},
  groups     = {Communication},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank1},
  readstatus = {skimmed},
}

@InProceedings{Lowe2020_S2P,
  author     = {Ryan Lowe and Abhinav Gupta and Jakob Foerster and Douwe Kiela and Joelle Pineau},
  booktitle  = {International Conference on Learning Representations},
  title      = {On the interaction between supervision and self-play in emergent communication},
  year       = {2020},
  file       = {:Lowe2020On - On the Interaction between Supervision and Self Play in Emergent Communication.pdf:PDF},
  groups     = {Communication},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=rJxGLlBtwH},
}

@InProceedings{Zhang2019_VBC,
  author     = {Zhang, Sai Qian and Zhang, Qi and Lin, Jieyu},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control},
  year       = {2019},
  editor     = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {32},
  comment    = {VBC
communication integrated in QMIX, with tricks to make it more efficient},
  file       = {:NEURIPS2019_14cfdb59 - Efficient Communication in Multi Agent Reinforcement Learning Via Variance Based Control.pdf:PDF},
  groups     = {Multi-agent RL, Communication},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/14cfdb59b5bda1fc245aadae15b1984a-Abstract.html},
}

@InProceedings{Christianos2020,
  author    = {Filippos Christianos and Lukas Sch{\"a}fer and Albrecht, {Stefano V}},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS 2020)},
  title     = {Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning},
  year      = {2020},
  month     = dec,
  note      = {Thirty-fourth Conference on Neural Information Processing Systems, NeurIPS 2020 ; Conference date: 06-12-2020 Through 12-12-2020},
  pages     = {10707--10717},
  publisher = {Curran Associates Inc},
  abstract  = {Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms two baselines and two state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.},
  day       = {6},
  file      = {:Christianos2020 - Shared Experience Actor Critic for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Exploration in MARL},
  language  = {English},
  url       = {https://www.research.ed.ac.uk/en/publications/shared-experience-actor-critic-for-multi-agent-reinforcement-lear},
}

@InProceedings{Zhou2020_LICA,
  author        = {Meng Zhou and Ziyu Liu and Pengwei Sui and Yixuan Li and Yuk Ying Chung},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning},
  year          = {2020},
  editor        = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  month         = jul,
  pages         = {11853--11864},
  publisher     = {Curran Associates, Inc.},
  volume        = {33},
  abstract      = {We present a multi-agent actor-critic method that aims to implicitly address the credit assignment problem under fully cooperative settings. Our key motivation is that credit assignment among agents may not require an explicit formulation as long as (1) the policy gradients derived from a centralized critic carry sufficient information for the decentralized agents to maximize their joint action value through optimal cooperation and (2) a sustained level of exploration is enforced throughout training. Under the centralized training with decentralized execution (CTDE) paradigm, we achieve the former by formulating the centralized critic as a hypernetwork such that a latent state representation is integrated into the policy gradients through its multiplicative association with the stochastic policies; to achieve the latter, we derive a simple technique called adaptive entropy regularization where magnitudes of the entropy gradients are dynamically rescaled based on the current policy stochasticity to encourage consistent levels of exploration. Our algorithm, referred to as LICA, is evaluated on several benchmarks including the multi-agent particle environments and a set of challenging StarCraft II micromanagement tasks, and we show that LICA significantly outperforms previous methods.},
  archiveprefix = {arXiv},
  comment       = {LICA},
  eprint        = {2007.02529},
  file          = {:Zhou2020_LICA - Learning Implicit Credit Assignment for Cooperative Multi Agent Reinforcement Learning.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2020/file/8977ecbb8cb82d77fb091c7a7f186163-Paper.pdf},
  groups        = {Multi-agent RL, Value factorisation, Credit Assignment},
  keywords      = {cs.LG, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  ranking       = {rank2},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2020/file/8977ecbb8cb82d77fb091c7a7f186163-Paper.pdf},
}

@InProceedings{James2019_Sim2Real,
  author    = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks},
  year      = {2019},
  month     = {June},
  file      = {:James_2019_CVPR - Sim to Real Via Sim to Sim_ Data Efficient Robotic Grasping Via Randomized to Canonical Adaptation Networks.pdf:PDF},
  groups    = {Sim to Real},
  ranking   = {rank3},
  url       = {https://openaccess.thecvf.com/content_CVPR_2019/html/James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.html},
}

@InProceedings{Chebotar2019_Sim2Real,
  author    = {Yevgen Chebotar and Ankur Handa and Viktor Makoviychuk and Miles Macklin and Jan Issac and Nathan Ratliff and Dieter Fox},
  booktitle = {2019 International Conference on Robotics and Automation ({ICRA})},
  title     = {Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra.2019.8793789},
  file      = {:Chebotar2019 - Closing the Sim to Real Loop_ Adapting Simulation Randomization with Real World Experience.pdf:PDF},
  groups    = {Sim to Real},
  ranking   = {rank3},
}

@InProceedings{Peng2018,
  author    = {Xue Bin Peng and Marcin Andrychowicz and Wojciech Zaremba and Pieter Abbeel},
  booktitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  title     = {Sim-to-Real Transfer of Robotic Control with Dynamics Randomization},
  year      = {2018},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icra.2018.8460528},
  file      = {:peng2018.pdf:PDF},
  groups    = {Sim to Real},
  ranking   = {rank4},
}

@Article{Andrychowicz2020_Sim2Real,
  author   = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
  journal  = {The International Journal of Robotics Research},
  title    = {Learning dexterous in-hand manipulation},
  year     = {2020},
  number   = {1},
  pages    = {3-20},
  volume   = {39},
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.},
  doi      = {10.1177/0278364919887447},
  eprint   = {https://doi.org/10.1177/0278364919887447},
  file     = {:doi_10.1177_0278364919887447 - Learning Dexterous in Hand Manipulation.pdf:PDF},
  groups   = {Sim to Real},
  ranking  = {rank4},
  url      = {https://doi.org/10.1177/0278364919887447},
}

@Article{Sutton1999,
  author   = {Richard S. Sutton and Doina Precup and Satinder Singh},
  journal  = {Artificial Intelligence},
  title    = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  year     = {1999},
  issn     = {0004-3702},
  number   = {1},
  pages    = {181-211},
  volume   = {112},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
  doi      = {https://doi.org/10.1016/S0004-3702(99)00052-1},
  file     = {:SUTTON1999181 - Between MDPs and Semi MDPs_ a Framework for Temporal Abstraction in Reinforcement Learning.pdf:PDF},
  groups   = {RL, Macro-actions},
  keywords = {Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes},
  ranking  = {rank4},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370299000521},
}

@InProceedings{Colas2020,
  author     = {Colas, C\'{e}dric and Karch, Tristan and Lair, Nicolas and Dussoux, Jean-Michel and Moulin-Frier, Cl\'{e}ment and Dominey, Peter and Oudeyer, Pierre-Yves},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Language as a Cognitive Tool to Imagine Goals in Curiosity Driven Exploration},
  year       = {2020},
  editor     = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages      = {3761--3774},
  publisher  = {Curran Associates, Inc.},
  volume     = {33},
  comment    = {IMAGINE},
  file       = {:NEURIPS2020_274e6fcf - Language As a Cognitive Tool to Imagine Goals in Curiosity Driven Exploration.pdf:PDF},
  groups     = {Curiosity, ZPD, Language-Augmented RL},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf},
}

@Article{Chan2019,
  author        = {Harris Chan and Yuhuai Wu and Jamie Kiros and Sanja Fidler and Jimmy Ba},
  title         = {ACTRCE: Augmenting Experience via Teacher's Advice For Multi-Goal Reinforcement Learning},
  year          = {2019},
  month         = feb,
  abstract      = {Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failed experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, and even small amount of advice is sufficient for the agent to achieve good performance.},
  archiveprefix = {arXiv},
  eprint        = {1902.04546},
  file          = {:Chan2019 - ACTRCE_ Augmenting Experience Via Teacher's Advice for Multi Goal Reinforcement Learning.pdf:PDF},
  groups        = {Language-Augmented RL, Goal-conditioned RL},
  keywords      = {cs.LG, cs.AI, cs.NE, stat.ML},
  primaryclass  = {cs.LG},
}

@InProceedings{NEURIPS2019_0af78794,
  author     = {Jiang, YiDing and Gu, Shixiang (Shane) and Murphy, Kevin P and Finn, Chelsea},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Language as an Abstraction for Hierarchical Deep Reinforcement Learning},
  year       = {2019},
  editor     = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {32},
  file       = {:NEURIPS2019_0af78794 - Language As an Abstraction for Hierarchical Deep Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL, Language-Augmented RL},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2019/file/0af787945872196b42c9f73ead2565c8-Paper.pdf},
}

@InProceedings{Cideron2020,
  author    = {Geoffrey Cideron and Mathieu Seurin and Florian Strub and Olivier Pietquin},
  booktitle = {2020 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
  title     = {{HIGhER}: Improving instruction following with Hindsight Generation for Experience Replay},
  year      = {2020},
  month     = {dec},
  publisher = {{IEEE}},
  doi       = {10.1109/ssci47803.2020.9308603},
  file      = {:Cideron2020 - HIGhER_ Improving Instruction Following with Hindsight Generation for Experience Replay.pdf:PDF},
  groups    = {Language-Augmented RL},
}

@Article{Zhou_Small_2021,
  author       = {Zhou, Li and Small, Kevin},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Inverse Reinforcement Learning with Natural Language Goals},
  year         = {2021},
  month        = {May},
  number       = {12},
  pages        = {11116-11124},
  volume       = {35},
  abstractnote = {Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.},
  file         = {:Zhou_Small_2021 - Inverse Reinforcement Learning with Natural Language Goals.pdf:PDF},
  groups       = {Language-Augmented RL},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17326},
}

@InProceedings{Nguyen2021,
  author     = {Khanh Nguyen and Dipendra Misra and Robert Schapire and Miro Dudík and Patrick Shafto},
  booktitle  = {Proceedings of the 38th International Conference on Machine Learning},
  title      = {Interactive Learning from Activity Description},
  year       = {2021},
  month      = feb,
  abstract   = {We present a novel interactive learning protocol that enables training request-fulfilling agents by verbally describing their activities. Unlike imitation learning (IL), our protocol allows the teaching agent to provide feedback in a language that is most appropriate for them. Compared with reward in reinforcement learning (RL), the description feedback is richer and allows for improved sample complexity. We develop a probabilistic framework and an algorithm that practically implements our protocol. Empirical results in two challenging request-fulfilling problems demonstrate the strengths of our approach: compared with RL baselines, it is more sample-efficient; compared with IL baselines, it achieves competitive success rates without requiring the teaching agent to be able to demonstrate the desired behavior using the learning agent's actions. Apart from empirical evaluation, we also provide theoretical guarantees for our algorithm under certain assumptions about the teacher and the environment.},
  file       = {:Nguyen2021 - Interactive Learning from Activity Description.pdf:PDF},
  groups     = {Language-Augmented RL},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v139/nguyen21e.html},
}

@InProceedings{Hill2020,
  author        = {Felix Hill and Andrew Lampinen and Rosalia Schneider and Stephen Clark and Matthew Botvinick and James L. McClelland and Adam Santoro},
  booktitle     = {International Conference on Learning Representations},
  title         = {Environmental drivers of systematicity and generalization in a situated agent},
  year          = {2020},
  abstract      = {The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we consider tests of out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room. We first describe a comparatively generic agent architecture that exhibits strong performance on these tests. We then identify three aspects of the training regime and environment that make a significant difference to its performance: (a) the number of object/word experiences in the training set; (b) the visual invariances afforded by the agent's perspective, or frame of reference; and (c) the variety of visual input inherent in the perceptual aspect of the agent's perception. Our findings indicate that the degree of generalisation that networks exhibit can depend critically on particulars of the environment in which a given task is instantiated. They further suggest that the propensity for neural networks to generalise in systematic ways may increase if, like human children, those networks have access to many frames of richly varying, multi-modal observations as they learn.},
  archiveprefix = {arXiv},
  eprint        = {1910.00571},
  file          = {:Hill2019 - Environmental Drivers of Systematicity and Generalization in a Situated Agent.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
  readstatus    = {read},
}

@InProceedings{pmlr-v70-andreas17a,
  author    = {Jacob Andreas and Dan Klein and Sergey Levine},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Modular Multitask Reinforcement Learning with Policy Sketches},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {06--11 Aug},
  pages     = {166--175},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.},
  file      = {:pmlr-v70-andreas17a - Modular Multitask Reinforcement Learning with Policy Sketches.pdf:PDF},
  groups    = {Meta-Learning, Macro-actions},
  pdf       = {http://proceedings.mlr.press/v70/andreas17a/andreas17a.pdf},
  ranking   = {rank3},
  url       = {http://proceedings.mlr.press/v70/andreas17a.html},
}

@InProceedings{Shridhar2021,
  author    = {Mohit Shridhar and Xingdi Yuan and Marc-Alexandre Cote and Yonatan Bisk and Adam Trischler and Matthew Hausknecht},
  booktitle = {International Conference on Learning Representations},
  title     = {ALFWorld: Aligning Text and Embodied Environments for Interactive Learning},
  year      = {2021},
  file      = {:Shridhar2021 - ALFWorld_ Aligning Text and Embodied Environments for Interactive Learning.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio3},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=0IOX0YcCdTn},
}

@InProceedings{akakzia_hal-03121146,
  author      = {Akakzia, Ahmed and Colas, C{\'e}dric and Oudeyer, Pierre-Yves and Chetouani, Mohamed and Sigaud, Olivier},
  booktitle   = {{ICLR 2021 - Ninth International Conference on Learning Representation}},
  title       = {{Grounding Language to Autonomously-Acquired Skills via Goal Generation}},
  year        = {2021},
  address     = {Vienna / Virtual, Austria},
  month       = May,
  comment     = {LGB},
  file        = {:akakzia_hal-03121146 - Grounding Language to Autonomously Acquired Skills Via Goal Generation.pdf:PDF},
  groups      = {Language-Augmented RL},
  hal_id      = {hal-03121146},
  hal_version = {v1},
  keywords    = {Reinforcement learning ; Artificial intelligence and robotics},
  pdf         = {https://hal.inria.fr/hal-03121146/file/2006.07185.pdf},
  readstatus  = {read},
  url         = {https://hal.inria.fr/hal-03121146},
}

@InProceedings{Luketina2019_Language,
  author     = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rocktäschel, Tim},
  booktitle  = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
  title      = {A Survey of Reinforcement Learning Informed by Natural Language},
  year       = {2019},
  month      = {7},
  pages      = {6309--6317},
  publisher  = {International Joint Conferences on Artificial Intelligence Organization},
  doi        = {10.24963/ijcai.2019/880},
  file       = {:Luketina2019 - A Survey of Reinforcement Learning Informed by Natural Language.pdf:PDF},
  groups     = {Language-Augmented RL},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://doi.org/10.24963/ijcai.2019/880},
}

@InProceedings{Karch2021,
  author        = {Tristan Karch and Laetitia Teodorescu and Katja Hofmann and Clément Moulin-Frier and Pierre-Yves Oudeyer},
  booktitle     = {Advances in Neural Information Processing Systems 34 pre-proceedings},
  title         = {Grounding Spatio-Temporal Language with Transformers},
  year          = {2021},
  month         = jun,
  abstract      = {Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents. We also release our code under open-source license as well as pretrained models and datasets to encourage the wider community to build upon and extend our work in the future.},
  archiveprefix = {arXiv},
  eprint        = {2106.08858},
  file          = {:Karch2021 - Grounding Spatio Temporal Language with Transformers.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.AI, cs.CL, cs.LG},
  primaryclass  = {cs.AI},
  readstatus    = {read},
}

@InProceedings{Wang2020b,
  author    = {Wang, Tonghan and Dong, Heng and Lesser, Victor and Zhang, Chongjie},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {{ROMA}: Multi-Agent Reinforcement Learning with Emergent Roles},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {9876--9886},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at https://sites.google.com/view/romarl/.},
  file      = {:Wang2020 - ROMA_ Multi Agent Reinforcement Learning with Emergent Roles.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v119/wang20f/wang20f.pdf},
  ranking   = {rank2},
  url       = {http://proceedings.mlr.press/v119/wang20f.html},
}

@InProceedings{Wang2021,
  author    = {Jianhao Wang and Zhizhou Ren and Terry Liu and Yang Yu and Chongjie Zhang},
  booktitle = {International Conference on Learning Representations},
  title     = {QPLEX: Duplex Dueling Multi-Agent Q-Learning},
  year      = {2021},
  file      = {:Wang2021 - QPLEX_ Duplex Dueling Multi Agent Q Learning.pdf:PDF},
  groups    = {Multi-agent RL, Value factorisation},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=Rcmk0xxIQV},
}

@Book{Vygotsky1934,
  author  = {Lev S. Vygotsky},
  title   = {Thought and Language},
  year    = {1934},
  groups  = {Developmental psychology, Language},
  ranking = {rank5},
}

@InProceedings{Andreas2018,
  author    = {Jacob Andreas and Dan Klein and Sergey Levine},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  title     = {Learning with Latent Language},
  year      = {2018},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/n18-1197},
  file      = {:Andreas2018 - Learning with Latent Language.pdf:PDF},
  groups    = {Language-Augmented RL},
  ranking   = {rank2},
}

@InProceedings{Chang2004,
  author    = {Chang, Yu-han and Ho, Tracey and Kaelbling, Leslie},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {All learning is Local: Multi-agent Learning in Global Reward Games},
  year      = {2004},
  editor    = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
  publisher = {MIT Press},
  volume    = {16},
  file      = {:Chang2004 - All Learning Is Local_ Multi Agent Learning in Global Reward Games.pdf:PDF},
  groups    = {Credit Assignment},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper/2003/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf},
}

@InProceedings{Du2019_LIIR,
  author     = {Du, Yali and Han, Lei and Fang, Meng and Dai, Tianhong and Liu, Ji and Tao, Dacheng},
  booktitle  = {Advances in Neural Information Processing Systems 32},
  title      = {LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning},
  year       = {2019},
  volume     = {32},
  abstract   = {A great challenge in cooperative decentralized multi-agent reinforcement learning
(MARL) is generating diversified behaviors for each individual agent when receiving
only a team reward. Prior studies have paid many efforts on reward shaping or designing
a centralized critic that can discriminatively credit the agents. In this paper, we
propose to merge the two directions and learn each agent an intrinsic reward function
which diversely stimulates the agents at each time step. Specifically, the intrinsic
reward for a specific agent will be involved in computing a distinct proxy critic
for the agent to direct the updating of its individual policy. Meanwhile, the parameterized
intrinsic reward function will be updated towards maximizing the expected accumulated
team reward from the environment so that the objective is consistent with the original
MARL problem. The proposed method is referred to as learning individual intrinsic
reward (LIIR) in MARL. We compare LIIR with a number of state-of-the-art MARL methods
on battle games in StarCraft II. The results demonstrate the effectiveness of LIIR,
and we show LIIR can assign each individual agent an insightful intrinsic reward per
time step.},
  file       = {:10.5555_3454287.3454683 - LIIR_ Learning Individual Intrinsic Reward in Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Multi-agent RL, Intrinsic rewards in MARL},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2019/hash/07a9d3fed4c5ea6b17e80258dee231fa-Abstract.html},
}

@InProceedings{Narasimhan2015,
  author    = {Karthik Narasimhan and Tejas D Kulkarni and Regina Barzilay},
  booktitle = {In Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  title     = {Language understanding for textbased games using deep reinforcement learning},
  year      = {2015},
  file      = {:Narasimhan2015 - Language Understanding for Textbased Games Using Deep Reinforcement Learning (1).pdf:PDF},
  groups    = {Language-Augmented RL},
}

@Article{Sun2021b,
  author        = {Charles Sun and Jędrzej Orbik and Coline Devin and Brian Yang and Abhishek Gupta and Glen Berseth and Sergey Levine},
  title         = {Fully Autonomous Real-World Reinforcement Learning for Mobile Manipulation},
  year          = {2021},
  month         = jul,
  abstract      = {We study how robots can autonomously learn skills that require a combination of navigation and grasping. While reinforcement learning in principle provides for automated robotic skill learning, in practice reinforcement learning in the real world is challenging and often requires extensive instrumentation and supervision. Our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. Our proposed system, ReLMM, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. Our method employs a modularized policy with components for manipulation and navigation, where manipulation policy uncertainty drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. We evaluate our method on a room cleanup task, where the robot must navigate to and pick up items scattered on the floor. After a grasp curriculum training phase, ReLMM can learn navigation and grasping together fully automatically, in around 40 hours of autonomous real-world training.},
  archiveprefix = {arXiv},
  eprint        = {2107.13545},
  file          = {:Sun2021 - Fully Autonomous Real World Reinforcement Learning for Mobile Manipulation.pdf:PDF},
  groups        = {RL in Robotics},
  keywords      = {cs.LG, cs.RO},
  primaryclass  = {cs.LG},
}

@InProceedings{Gupta2021,
  author    = {Gupta, Tarun and Mahajan, Anuj and Peng, Bei and Boehmer, Wendelin and Whiteson, Shimon},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {3930--3941},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {VDN and QMIX are two popular value-based algorithms for cooperative MARL that learn a centralized action value function as a monotonic mixing of per-agent utilities. While this enables easy decentralization of the learned policy, the restricted joint action value function can prevent them from solving tasks that require significant coordination between agents at a given timestep. We show that this problem can be overcome by improving the joint exploration of all agents during training. Specifically, we propose a novel MARL approach called Universal Value Exploration (UneVEn) that learns a set of related tasks simultaneously with a linear decomposition of universal successor features. With the policies of already solved related tasks, the joint exploration process of all agents can be improved to help them achieve better coordination. Empirical results on a set of exploration games, challenging cooperative predator-prey tasks requiring significant coordination among agents, and StarCraft II micromanagement benchmarks show that UneVEn can solve tasks where other state-of-the-art MARL methods fail.},
  comment   = {UneVEn},
  file      = {:Gupta2021 - UneVEn_ Universal Value Exploration for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Multi-task, Exploration in MARL},
  pdf       = {http://proceedings.mlr.press/v139/gupta21a/gupta21a.pdf},
  ranking   = {rank2},
  url       = {http://proceedings.mlr.press/v139/gupta21a.html},
}

@InProceedings{Iqbal2021,
  author    = {Iqbal, Shariq and De Witt, Christian A Schroeder and Peng, Bei and Boehmer, Wendelin and Whiteson, Shimon and Sha, Fei},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Randomized Entity-wise Factorization for Multi-Agent Reinforcement Learning},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {4596--4606},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Multi-agent settings in the real world often involve tasks with varying types and quantities of agents and non-agent entities; however, common patterns of behavior often emerge among these agents/entities. Our method aims to leverage these commonalities by asking the question: “What is the expected utility of each agent when only considering a randomly selected sub-group of its observed entities?” By posing this counterfactual question, we can recognize state-action trajectories within sub-groups of entities that we may have encountered in another task and use what we learned in that task to inform our prediction in the current one. We then reconstruct a prediction of the full returns as a combination of factors considering these disjoint groups of entities and train this “randomly factorized" value function as an auxiliary objective for value-based multi-agent reinforcement learning. By doing so, our model can recognize and leverage similarities across tasks to improve learning efficiency in a multi-task setting. Our approach, Randomized Entity-wise Factorization for Imagined Learning (REFIL), outperforms all strong baselines by a significant margin in challenging multi-task StarCraft micromanagement settings.},
  file      = {:Iqbal2021 - Randomized Entity Wise Factorization for Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL, Value factorisation},
  pdf       = {http://proceedings.mlr.press/v139/iqbal21a/iqbal21a.pdf},
  url       = {http://proceedings.mlr.press/v139/iqbal21a.html},
}

@InProceedings{Wang2020c,
  author     = {Tonghan Wang and Jianhao Wang and Chongyi Zheng and Chongjie Zhang},
  booktitle  = {International Conference on Learning Representations},
  title      = {Learning Nearly Decomposable Value Functions Via Communication Minimization},
  year       = {2020},
  comment    = {NDQ},
  file       = {:Wang2020 - Learning Nearly Decomposable Value Functions Via Communication Minimization.pdf:PDF},
  groups     = {Multi-agent RL, Communication},
  priority   = {prio2},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=HJx-3grYDB},
}

@Article{Ding2020_I2C,
  author    = {Ding, Ziluo and Huang, Tiejun and Lu, Zongqing},
  title     = {Learning Individually Inferred Communication for Multi-Agent Cooperation},
  year      = {2020},
  pages     = {22069--22079},
  volume    = {33},
  abstract  = {Communication lays the foundation for human cooperation. It is also crucial for multi-agent cooperation. However, existing work focuses on broadcast communication, which is not only impractical but also leads to information redundancy that could even impair the learning process. To tackle these difficulties, we propose Individually Inferred Communication (I2C), a simple yet effective model to enable agents to learn a prior for agent-agent communication. The prior knowledge is learned via causal inference and realized by a feed-forward neural network that maps the agent's local observation to a belief about who to communicate with. The influence of one agent on another is inferred via the joint action-value function in multi-agent reinforcement learning and quantified to label the necessity of agent-agent communication. Furthermore, the agent policy is regularized to better exploit communicated messages. Empirically, we show that I2C can not only reduce communication overhead but also improve the performance in a variety of multi-agent cooperative scenarios, comparing to existing methods. The code is available at https://github.com/PKU-AI-Edge/I2C.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  file      = {:Ding2020 - Learning Individually Inferred Communication for Multi Agent Cooperation.pdf:PDF},
  groups    = {Communication},
  priority  = {prio2},
  publisher = {Curran Associates, Inc.},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/fb2fcd534b0ff3bbed73cc51df620323-Paper.pdf},
}

@InProceedings{Sun2021a,
  author     = {Sun, Chuxiong and Wu, Bo and Wang, Rui and Hu, Xiaohui and Yang, Xiaoya and Cong, Cong},
  booktitle  = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Intrinsic Motivated Multi-Agent Communication},
  year       = {2021},
  pages      = {1668--1670},
  comment    = {IMMAC},
  file       = {:Sun2021 - Intrinsic Motivated Multi Agent Communication.pdf:PDF},
  groups     = {Communication},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.5555/3463952.3464195},
}

@Article{Li2020a,
  author        = {Chenghao Li and Xiaoteng Ma and Chongjie Zhang and Jun Yang and Li Xia and Qianchuan Zhao},
  title         = {SOAC: The Soft Option Actor-Critic Architecture},
  year          = {2020},
  month         = jun,
  abstract      = {The option framework has shown great promise by automatically extracting temporally-extended sub-tasks from a long-horizon task. Methods have been proposed for concurrently learning low-level intra-option policies and high-level option selection policy. However, existing methods typically suffer from two major challenges: ineffective exploration and unstable updates. In this paper, we present a novel and stable off-policy approach that builds on the maximum entropy model to address these challenges. Our approach introduces an information-theoretical intrinsic reward for encouraging the identification of diverse and effective options. Meanwhile, we utilize a probability inference model to simplify the optimization problem as fitting optimal trajectories. Experimental results demonstrate that our approach significantly outperforms prior on-policy and off-policy methods in a range of Mujoco benchmark tasks while still providing benefits for transfer learning. In these tasks, our approach learns a diverse set of options, each of whose state-action space has strong coherence.},
  archiveprefix = {arXiv},
  eprint        = {2006.14363},
  file          = {:Li2020a - SOAC_ the Soft Option Actor Critic Architecture.pdf:PDF},
  groups        = {Macro-actions},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
}

@Article{Gronauer2022_MADRLSurvey,
  author     = {Sven Gronauer and Klaus Diepold},
  journal    = {Artificial Intelligence Review},
  title      = {Multi-agent deep reinforcement learning: a survey},
  year       = {2022},
  month      = {apr},
  doi        = {10.1007/s10462-021-09996-w},
  file       = {:Gronauer2021 - Multi Agent Deep Reinforcement Learning_ a Survey.pdf:PDF},
  groups     = {MAS Reviews},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@Article{Seita2019,
  author        = {Daniel Seita and David Chan and Roshan Rao and Chen Tang and Mandi Zhao and John Canny},
  title         = {ZPD Teaching Strategies for Deep Reinforcement Learning from Demonstrations},
  year          = {2019},
  month         = oct,
  abstract      = {Learning from demonstrations is a popular tool for accelerating and reducing the exploration requirements of reinforcement learning. When providing expert demonstrations to human students, we know that the demonstrations must fall within a particular range of difficulties called the "Zone of Proximal Development (ZPD)". If they are too easy the student learns nothing, but if they are too difficult the student is unable to follow along. This raises the question: Given a set of potential demonstrators, which among them is best suited for teaching any particular learner? Prior work, such as the popular Deep Q-learning from Demonstrations (DQfD) algorithm has generally focused on single demonstrators. In this work we consider the problem of choosing among multiple demonstrators of varying skill levels. Our results align with intuition from human learners: it is not always the best policy to draw demonstrations from the best performing demonstrator (in terms of reward). We show that careful selection of teaching strategies can result in sample efficiency gains in the learner's environment across nine Atari games},
  archiveprefix = {arXiv},
  eprint        = {1910.12154},
  file          = {:Seita2019 - ZPD Teaching Strategies for Deep Reinforcement Learning from Demonstrations.pdf:PDF},
  groups        = {Meta-Learning, ZPD},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@InProceedings{Steels2005,
  author    = {Luc Steels},
  booktitle = {In: AISB'05: Proceedings of the Second International Symposium on the Emergence and Evolution of Linguistic Communication (EELC'05), pages 143-150, Hatfield, 2005. University of Hertfordshire.},
  title     = {What triggers the emergence of grammar?},
  year      = {2005},
  note      = {In: AISB'05: Proceedings of the Second International Symposium on the Emergence and Evolution of Linguistic Communication (EELC'05), pages 143-150, Hatfield, 2005. University of Hertfordshire.},
  file      = {:Steels2005 - What Triggers the Emergence of Grammar_.pdf:PDF},
  groups    = {Language, Language origins and evolution},
  language  = {English},
}

@Article{Steels1995,
  author   = {Steels, Luc},
  journal  = {Artificial Life},
  title    = {{A Self-Organizing Spatial Vocabulary}},
  year     = {1995},
  issn     = {1064-5462},
  month    = {04},
  number   = {3},
  pages    = {319-332},
  volume   = {2},
  abstract = {{Language is a shared set of conventions for mapping meanings to utterances. This paper explores self-organization as the primary mechanism for the formation of a vocabulary. It reports on a computational experiment in which a group of distributed agents develop ways to identify each other using names or spatial descriptions. It is also shown that the proposed mechanism copes with the acquisition of an existing vocabulary by new agents entering the community and with an expansion of the set of meanings.}},
  doi      = {10.1162/artl.1995.2.3.319},
  eprint   = {https://direct.mit.edu/artl/article-pdf/2/3/319/1661525/artl.1995.2.3.319.pdf},
  file     = {:Steels1995 - A Self Organizing Spatial Vocabulary.pdf:PDF},
  groups   = {Language, Language origins and evolution},
  priority = {prio1},
  ranking  = {rank3},
  url      = {https://doi.org/10.1162/artl.1995.2.3.319},
}

@InProceedings{Andreas2019,
  author     = {Jacob Andreas},
  booktitle  = {International Conference on Learning Representations (ICLR)},
  title      = {Measuring Compositionality in Representation Learning},
  year       = {2019},
  abstract   = {Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.},
  file       = {:Andreas2019 - Measuring Compositionality in Representation Learning.pdf:PDF},
  groups     = {Machine Learning},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=HJz05o0qK7},
}

@InProceedings{Wang2020_IMAC,
  author     = {Wang, Rundong and He, Xu and Yu, Runsheng and Qiu, Wei and An, Bo and Rabinovich, Zinovi},
  booktitle  = {Proceedings of the 37th International Conference on Machine Learning},
  title      = {Learning Efficient Multi-agent Communication: An Information Bottleneck Approach},
  year       = {2020},
  editor     = {III, Hal Daumé and Singh, Aarti},
  month      = {13--18 Jul},
  pages      = {9908--9918},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {119},
  abstract   = {We consider the problem of the limited-bandwidth communication for multi-agent reinforcement learning, where agents cooperate with the assistance of a communication protocol and a scheduler. The protocol and scheduler jointly determine which agent is communicating what message and to whom. Under the limited bandwidth constraint, a communication protocol is required to generate informative messages. Meanwhile, an unnecessary communication connection should not be established because it occupies limited resources in vain. In this paper, we develop an Informative Multi-Agent Communication (IMAC) method to learn efficient communication protocols as well as scheduling. First, from the perspective of communication theory, we prove that the limited bandwidth constraint requires low-entropy messages throughout the transmission. Then inspired by the information bottleneck principle, we learn a valuable and compact communication protocol and a weight-based scheduler. To demonstrate the efficiency of our method, we conduct extensive experiments in various cooperative and competitive multi-agent tasks with different numbers of agents and different bandwidths. We show that IMAC converges faster and leads to efficient communication among agents under the limited bandwidth as compared to many baseline methods.},
  comment    = {IMAC},
  file       = {:Wang2020 - Learning Efficient Multi Agent Communication_ an Information Bottleneck Approach (1).pdf:PDF},
  groups     = {Communication},
  pdf        = {http://proceedings.mlr.press/v119/wang20i/wang20i.pdf},
  ranking    = {rank1},
  readstatus = {read},
  url        = {http://proceedings.mlr.press/v119/wang20i.html},
}

@InProceedings{Zhang2020,
  author     = {Zhang, Sai Qian and Zhang, Qi and Lin, Jieyu},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Succinct and Robust Multi-Agent Communication With Temporal Message Control},
  year       = {2020},
  editor     = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages      = {17271--17282},
  publisher  = {Curran Associates, Inc.},
  volume     = {33},
  abstract   = {TMC},
  comment    = {TMC},
  file       = {:Zhang2020 - Succinct and Robust Multi Agent Communication with Temporal Message Control.pdf:PDF},
  groups     = {Communication},
  priority   = {prio2},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2020/file/c82b013313066e0702d58dc70db033ca-Paper.pdf},
}

@Article{Nowak2000,
  author    = {Martin A. Nowak and Joshua B. Plotkin and Vincent A. A. Jansen},
  journal   = {Nature},
  title     = {The evolution of syntactic communication},
  year      = {2000},
  month     = {mar},
  number    = {6777},
  pages     = {495--498},
  volume    = {404},
  doi       = {10.1038/35006635},
  file      = {:nowak2000.pdf:PDF},
  groups    = {Language, Language origins and evolution},
  publisher = {Springer Science and Business Media {LLC}},
  ranking   = {rank3},
}

@Article{Mezghani2021,
  author        = {Lina Mezghani and Sainbayar Sukhbaatar and Thibaut Lavril and Oleksandr Maksymets and Dhruv Batra and Piotr Bojanowski and Karteek Alahari},
  title         = {Memory-Augmented Reinforcement Learning for Image-Goal Navigation},
  year          = {2021},
  month         = jan,
  abstract      = {In this work, we present a memory-augmented approach for image-goal navigation. Our key hypothesis is that, while episodic reinforcement learning may be a convenient framework for tackling this task, embodied agents, once deployed, do not simply cease to exist after an episode has ended. They persist and so should their memories. Our approach leverages a cross-episode memory to learn to navigate. First, we train a state-embedding network in a self-supervised fashion, and then use it to embed previously-visited states into the agent's memory. Our navigation policy takes advantage of the information stored in the memory via an attention mechanism. We validate our approach through extensive evaluations, and show that our model establishes a new state of the art on the challenging Gibson dataset. We obtain this competitive performance from RGB input alone, without access to additional information such as position or depth.},
  archiveprefix = {arXiv},
  eprint        = {2101.05181},
  file          = {:Mezghani2021 - Memory Augmented Reinforcement Learning for Image Goal Navigation.pdf:PDF},
  groups        = {Memory},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@InProceedings{tian-etal-2019-learning,
  author    = {Tian, Zhiliang and Bi, Wei and Li, Xiaopeng and Zhang, Nevin L.},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  title     = {Learning to Abstract for Memory-augmented Conversational Response Generation},
  year      = {2019},
  address   = {Florence, Italy},
  month     = jul,
  pages     = {3816--3825},
  publisher = {Association for Computational Linguistics},
  abstract  = {Neural generative models for open-domain chit-chat conversations have become an active area of research in recent years. A critical issue with most existing generative models is that the generated responses lack informativeness and diversity. A few researchers attempt to leverage the results of retrieval models to strengthen the generative models, but these models are limited by the quality of the retrieval results. In this work, we propose a memory-augmented generative model, which learns to abstract from the training corpus and saves the useful information to the memory to assist the response generation. Our model clusters query-response samples, extracts characteristics of each cluster, and learns to utilize these characteristics for response generation. Experimental results show that our model outperforms other competitive baselines.},
  doi       = {10.18653/v1/P19-1371},
  file      = {:tian-etal-2019-learning - Learning to Abstract for Memory Augmented Conversational Response Generation.pdf:PDF},
  groups    = {Memory},
  url       = {https://aclanthology.org/P19-1371},
}

@InProceedings{He2017,
  author     = {He He and Anusha Balakrishnan and Mihail Eric and Percy Liang},
  booktitle  = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  title      = {Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings},
  year       = {2017},
  note       = {Funding Information: This work is supported by DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF-15-1-0462. Mike Kayser worked on an early version of the project while he was at Stanford. We also thank members of the Stanford NLP group for insightful discussions. Publisher Copyright: {\textcopyright} 2017 Association for Computational Linguistics.; 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017 ; Conference date: 30-07-2017 Through 04-08-2017},
  pages      = {1766--1776},
  publisher  = {Association for Computational Linguistics (ACL)},
  series     = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
  abstract   = {We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.},
  doi        = {10.18653/v1/P17-1162},
  file       = {:He2017 - Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings.pdf:PDF},
  groups     = {Memory},
  language   = {English (US)},
  ranking    = {rank1},
  readstatus = {skimmed},
}

@InProceedings{Khandelwal2020Generalization,
  author    = {Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
  booktitle = {International Conference on Learning Representations},
  title     = {Generalization through Memorization: Nearest Neighbor Language Models},
  year      = {2020},
  file      = {:Khandelwal2020Generalization - Generalization through Memorization_ Nearest Neighbor Language Models.pdf:PDF},
  groups    = {Memory},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=HklBjCEKvH},
}

@InProceedings{parisotto2018,
  author     = {Emilio Parisotto and Ruslan Salakhutdinov},
  booktitle  = {International Conference on Learning Representations},
  title      = {Neural Map: Structured Memory for Deep Reinforcement Learning},
  year       = {2018},
  file       = {:parisotto2018 - Neural Map_ Structured Memory for Deep Reinforcement Learning.pdf:PDF},
  groups     = {Memory},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=Bk9zbyZCZ},
}

@InProceedings{Oh2016_Minecraft,
  author     = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
  booktitle  = {Proceedings of The 33rd International Conference on Machine Learning},
  title      = {Control of Memory, Active Perception, and Action in Minecraft},
  year       = {2016},
  address    = {New York, New York, USA},
  editor     = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  month      = {20--22 Jun},
  pages      = {2790--2799},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {48},
  abstract   = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
  comment    = {MQN, RMQN, FRMQN},
  file       = {:Oh2016 - Control of Memory, Active Perception, and Action in Minecraft.pdf:PDF},
  groups     = {Memory, Environments},
  pdf        = {http://proceedings.mlr.press/v48/oh16.pdf},
  ranking    = {rank3},
  readstatus = {skimmed},
  url        = {http://proceedings.mlr.press/v48/oh16.html},
}

@InProceedings{Zhu2020,
  author    = {Guangxiang Zhu and Zichuan Lin and Guangwen Yang and Chongjie Zhang},
  booktitle = {International Conference on Learning Representations},
  title     = {Episodic Reinforcement Learning with Associative Memory},
  year      = {2020},
  file      = {:Zhu2020 - Episodic Reinforcement Learning with Associative Memory.pdf:PDF},
  groups    = {Memory},
  url       = {https://openreview.net/forum?id=HkxjqxBYDB},
}

@Article{Hu2021b,
  author        = {Hao Hu and Jianing Ye and Guangxiang Zhu and Zhizhou Ren and Chongjie Zhang},
  title         = {Generalizable Episodic Memory for Deep Reinforcement Learning},
  year          = {2021},
  month         = mar,
  abstract      = {Episodic memory-based methods can rapidly latch onto past successful strategies by a non-parametric memory and improve sample efficiency of traditional reinforcement learning. However, little effort is put into the continuous domain, where a state is never visited twice, and previous episodic methods fail to efficiently aggregate experience across trajectories. To address this problem, we propose Generalizable Episodic Memory (GEM), which effectively organizes the state-action values of episodic memory in a generalizable manner and supports implicit planning on memorized trajectories. GEM utilizes a double estimator to reduce the overestimation bias induced by value propagation in the planning process. Empirical evaluation shows that our method significantly outperforms existing trajectory-based methods on various MuJoCo continuous control tasks. To further show the general applicability, we evaluate our method on Atari games with discrete action space, which also shows a significant improvement over baseline algorithms.},
  archiveprefix = {arXiv},
  eprint        = {2103.06469},
  file          = {:Hu2021b - Generalizable Episodic Memory for Deep Reinforcement Learning.pdf:PDF},
  groups        = {Memory},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@Article{Wayne2018,
  author        = {Greg Wayne and Chia-Chun Hung and David Amos and Mehdi Mirza and Arun Ahuja and Agnieszka Grabska-Barwinska and Jack Rae and Piotr Mirowski and Joel Z. Leibo and Adam Santoro and Mevlana Gemici and Malcolm Reynolds and Tim Harley and Josh Abramson and Shakir Mohamed and Danilo Rezende and David Saxton and Adam Cain and Chloe Hillier and David Silver and Koray Kavukcuoglu and Matt Botvinick and Demis Hassabis and Timothy Lillicrap},
  title         = {Unsupervised Predictive Memory in a Goal-Directed Agent},
  year          = {2018},
  month         = mar,
  abstract      = {Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.},
  archiveprefix = {arXiv},
  eprint        = {1803.10760},
  file          = {:Wayne2018 - Unsupervised Predictive Memory in a Goal Directed Agent.pdf:PDF},
  groups        = {Memory},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
  readstatus    = {skimmed},
}

@InProceedings{khan2018memory,
  author    = {Arbaaz Khan and Clark Zhang and Nikolay Atanasov and Konstantinos Karydis and Vijay Kumar and Daniel D. Lee},
  booktitle = {International Conference on Learning Representations},
  title     = {Memory Augmented Control Networks},
  year      = {2018},
  file      = {:khan2018memory - Memory Augmented Control Networks.pdf:PDF},
  groups    = {Memory},
  url       = {https://openreview.net/forum?id=HyfHgI6aW},
}

@InProceedings{NEURIPS2019_e6d8545d,
  author    = {Lee, Su Young and Sungik, Choi and Chung, Sae-Young},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  file      = {:NEURIPS2019_e6d8545d - Sample Efficient Deep Reinforcement Learning Via Episodic Backward Update.pdf:PDF},
  groups    = {Memory},
  url       = {https://proceedings.neurips.cc/paper/2019/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf},
}

@InProceedings{10.5555/3304889.3304998,
  author    = {Lin, Zichuan and Zhao, Tianqi and Yang, Guangwen and Zhang, Lintao},
  booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  title     = {Episodic Memory Deep Q-Networks},
  year      = {2018},
  address   = {Stockholm, Sweden},
  pages     = {2433–2439},
  publisher = {AAAI Press},
  series    = {IJCAI'18},
  abstract  = {Reinforcement learning (RL) algorithms have made huge progress in recent years by
leveraging the power of deep neural networks (DNN). Despite the success, deep RL algorithms
are known to be sample inefficient, often requiring many rounds of interaction with
the environments to obtain satisfactory performance. Recently, episodic memory based
RL has attracted attention due to its ability to latch on good actions quickly. In
this paper, we present a simple yet effective biologically inspired RL algorithm called
Episodic Memory Deep Q-Networks (EMDQN), which leverages episodic memory to supervise
an agent during training. Experiments show that our proposed method can lead to better
sample efficiency and is more likely to find good policies. It only requires 1/5 of
the interactions of DQN to achieve many state-of-the-art performances on Atari games,
significantly outperforming regular DQN and other episodic memory based RL algorithms.},
  file      = {:10.5555_3304889.3304998 - Episodic Memory Deep Q Networks.pdf:PDF},
  groups    = {Memory},
  isbn      = {9780999241127},
  numpages  = {7},
}

@Article{Nair2021,
  author        = {Suraj Nair and Eric Mitchell and Kevin Chen and Brian Ichter and Silvio Savarese and Chelsea Finn},
  journal       = {Proceedings of the 5th Conference on Robot Learning},
  title         = {Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation},
  year          = {2022},
  month         = sep,
  abstract      = {We study the problem of learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. In order to accomplish this, humans need easy and effective ways of specifying tasks to the robot. Goal images are one popular form of task specification, as they are already grounded in the robot's observation space. However, goal images also have a number of drawbacks: they are inconvenient for humans to provide, they can over-specify the desired behavior leading to a sparse reward signal, or under-specify task information in the case of non-goal reaching tasks. Natural language provides a convenient and flexible alternative for task specification, but comes with the challenge of grounding language in the robot's observation space. To scalably learn this grounding we propose to leverage offline robot datasets (including highly sub-optimal, autonomously collected data) with crowd-sourced natural language labels. With this data, we learn a simple classifier which predicts if a change in state completes a language instruction. This provides a language-conditioned reward function that can then be used for offline multi-task RL. In our experiments, we find that on language-conditioned manipulation tasks our approach outperforms both goal-image specifications and language conditioned imitation techniques by more than 25%, and is able to perform visuomotor tasks from natural language, such as "open the right drawer" and "move the stapler", on a Franka Emika Panda robot.},
  archiveprefix = {arXiv},
  eprint        = {2109.01115},
  file          = {:Nair2021 - Learning Language Conditioned Robot Behavior from Offline Data and Crowd Sourced Annotation.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.RO, cs.AI, cs.LG},
  primaryclass  = {cs.RO},
  ranking       = {rank2},
}

@Article{Sorokin2021,
  author        = {Maks Sorokin and Jie Tan and C. Karen Liu and Sehoon Ha},
  title         = {Learning to Navigate Sidewalks in Outdoor Environments},
  year          = {2021},
  month         = sep,
  abstract      = {Outdoor navigation on sidewalks in urban environments is the key technology behind important human assistive applications, such as last-mile delivery or neighborhood patrol. This paper aims to develop a quadruped robot that follows a route plan generated by public map services, while remaining on sidewalks and avoiding collisions with obstacles and pedestrians. We devise a two-staged learning framework, which first trains a teacher agent in an abstract world with privileged ground-truth information, and then applies Behavior Cloning to teach the skills to a student agent who only has access to realistic sensors. The main research effort of this paper focuses on overcoming challenges when deploying the student policy on a quadruped robot in the real world. We propose methodologies for designing sensing modalities, network architectures, and training procedures to enable zero-shot policy transfer to unstructured and dynamic real outdoor environments. We evaluate our learning framework on a quadrupedal robot navigating sidewalks in the city of Atlanta, USA. Using the learned navigation policy and its onboard sensors, the robot is able to walk 3.2 kilometers with a limited number of human interventions.},
  archiveprefix = {arXiv},
  eprint        = {2109.05603},
  file          = {:Sorokin2021 - Learning to Navigate Sidewalks in Outdoor Environments.pdf:PDF},
  groups        = {Navigation},
  keywords      = {cs.RO},
  primaryclass  = {cs.RO},
}

@Article{Ecoffet2021,
  author        = {Paul Ecoffet and Nicolas Fontbonne and Jean-Baptiste André and Nicolas Bredeche},
  title         = {Policy Search with Rare Significant Events: Choosing the Right Partner to Cooperate with},
  year          = {2021},
  month         = mar,
  abstract      = {This paper focuses on a class of reinforcement learning problems where significant events are rare and limited to a single positive reward per episode. A typical example is that of an agent who has to choose a partner to cooperate with, while a large number of partners are simply not interested in cooperating, regardless of what the agent has to offer. We address this problem in a continuous state and action space with two different kinds of search methods: a gradient policy search method and a direct policy search method using an evolution strategy. We show that when significant events are rare, gradient information is also scarce, making it difficult for policy gradient search methods to find an optimal policy, with or without a deep neural architecture. On the other hand, we show that direct policy search methods are invariant to the rarity of significant events, which is yet another confirmation of the unique role evolutionary algorithms has to play as a reinforcement learning method.},
  archiveprefix = {arXiv},
  eprint        = {2103.06846},
  file          = {:Ecoffet2021 - Policy Search with Rare Significant Events_ Choosing the Right Partner to Cooperate with.pdf:PDF},
  groups        = {Evolution Strategy},
  keywords      = {cs.LG, cs.AI, cs.NE, I.2.6; I.2},
  primaryclass  = {cs.LG},
}

@Article{Salimans2017,
  author        = {Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
  title         = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
  year          = {2017},
  month         = mar,
  abstract      = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archiveprefix = {arXiv},
  eprint        = {1703.03864},
  file          = {:Salimans2017 - Evolution Strategies As a Scalable Alternative to Reinforcement Learning.pdf:PDF},
  groups        = {Evolution Strategy},
  keywords      = {stat.ML, cs.AI, cs.LG, cs.NE},
  primaryclass  = {stat.ML},
  ranking       = {rank5},
}

@Article{Such2017,
  author        = {Felipe Petroski Such and Vashisht Madhavan and Edoardo Conti and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  title         = {Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
  year          = {2017},
  month         = dec,
  abstract      = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.\ DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in ${\raise.17ex\hbox{$\scriptstyle\sim$}}$4 hours on one desktop or ${\raise.17ex\hbox{$\scriptstyle\sim$}}$1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
  archiveprefix = {arXiv},
  eprint        = {1712.06567},
  file          = {:Such2017 - Deep Neuroevolution_ Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning.pdf:PDF},
  groups        = {Evolution Strategy},
  keywords      = {cs.NE, cs.LG},
  primaryclass  = {cs.NE},
  ranking       = {rank4},
}

@InProceedings{Lin2021_GroundMAC,
  author     = {Toru Lin and Minyoung Huh and Chris Stauffer and Ser-Nam Lim and Phillip Isola},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Learning to Ground Multi-Agent Communication with Autoencoders},
  year       = {2021},
  abstract   = {Communication requires having a common language, a lingua franca, between agents. This language could emerge via a consensus process, but it may require many generations of trial and error. Alternatively, the lingua franca can be given by the environment, where agents ground their language in representations of the observed world. We demonstrate a simple way to ground language in learned representations, which facilitates decentralized multi-agent communication and coordination. We find that a standard representation learning algorithm -- autoencoding -- is sufficient for arriving at a grounded common language. When agents broadcast these representations, they learn to understand and respond to each other's utterances and achieve surprisingly strong task performance across a variety of multi-agent communication environments.},
  file       = {:Lin2021 - Learning to Ground Multi Agent Communication with Autoencoders.pdf:PDF},
  groups     = {Discrete language, Communication},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/80fee67c8a4c4989bf8a580b4bbb0cd2-Abstract.html},
}

@Misc{exa2021,
  author    = {Mendonca, Russell and Rybkin, Oleh and Daniilidis, Kostas and Hafner, Danijar and Pathak, Deepak},
  title     = {Discovering and Achieving Goals via World Models},
  year      = {2021},
  booktitle = {NeurIPS},
  file      = {:exa2021 - Discovering and Achieving Goals Via World Models.pdf:PDF},
  groups    = {Model-based exploration},
}

@InProceedings{pmlr-v70-finn17a,
  author    = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {06--11 Aug},
  pages     = {1126--1135},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  file      = {:pmlr-v70-finn17a - Model Agnostic Meta Learning for Fast Adaptation of Deep Networks.pdf:PDF},
  groups    = {Meta-Learning},
  pdf       = {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  ranking   = {rank5},
  url       = {https://proceedings.mlr.press/v70/finn17a.html},
}

@Article{Yu2021_MAPPO,
  author        = {Chao Yu and Akash Velu and Eugene Vinitsky and Yu Wang and Alexandre Bayen and Yi Wu},
  title         = {The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games},
  year          = {2021},
  month         = mar,
  abstract      = {Proximal Policy Optimization (PPO) is a popular on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due the belief that on-policy methods are significantly less sample efficient than their off-policy counterparts in multi-agent problems. In this work, we investigate Multi-Agent PPO (MAPPO), a variant of PPO which is specialized for multi-agent settings. Using a 1-GPU desktop, we show that MAPPO achieves surprisingly strong performance in three popular multi-agent testbeds: the particle-world environments, the Starcraft multi-agent challenge, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. In the majority of environments, we find that compared to off-policy baselines, MAPPO achieves strong results while exhibiting comparable sample efficiency. Finally, through ablation studies, we present the implementation and algorithmic factors which are most influential to MAPPO's practical performance.},
  archiveprefix = {arXiv},
  comment       = {MAPPO},
  eprint        = {2103.01955},
  file          = {:- The Surprising Effectiveness of PPO in Cooperative, Multi Agent Games.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.LG, cs.AI, cs.MA},
  primaryclass  = {cs.LG},
  priority      = {prio2},
  ranking       = {rank3},
}

@Article{Harnad1990_SymbolGrounding,
  author     = {Stevan Harnad},
  journal    = {Physica D: Nonlinear Phenomena},
  title      = {The symbol grounding problem},
  year       = {1990},
  issn       = {0167-2789},
  number     = {1},
  pages      = {335-346},
  volume     = {42},
  abstract   = {There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the “symbol grounding problem”: How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) iconic representations, which are analogs of the proximal sensory projections of distal objects and events, and (2) categorical representations, which are learned and innate feature detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) symbolic representations, grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g. “An X is a Y that is Z”). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic “module,” however; the symbolic functions would emerge as an intrinsically “dedicated” symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded.},
  doi        = {https://doi.org/10.1016/0167-2789(90)90087-6},
  file       = {:HARNAD1990335 - The Symbol Grounding Problem.pdf:PDF},
  groups     = {Language, Artificial Intelligence, Symbol grounding},
  ranking    = {rank5},
  readstatus = {read},
  url        = {https://www.sciencedirect.com/science/article/pii/0167278990900876},
}

@Article{jbp_/content/journals/10.1075/eoc.4.1.03ste,
  author    = {Steels, Luc and Kaplan, Frédéric},
  journal   = {Evolution of Communication},
  title     = {AIBO’s first words: The social learning of language and meaning},
  year      = {2000},
  issn      = {1387-5337},
  number    = {1},
  pages     = {3-32},
  volume    = {4},
  abstract  = {This paper explores the hypothesis that language communication in its very first stage is bootstrapped in a social learning process under the strong influence of culture. A concrete framework for social learning has been developed based on the notion of a language game. Autonomous robots have been programmed to behave according to this framework. We show experiments that demonstrate why there has to be a causal role of language on category acquisition; partly by showing that it leads effectively to the bootstrapping of communication and partly by showing that other forms of learning do not generate categories usable in communication or make information assumptions which cannot be satisfied.},
  doi       = {https://doi.org/10.1075/eoc.4.1.03ste},
  file      = {:jbp__content_journals_10.1075_eoc.4.1.03ste - AIBO’s First Words_ the Social Learning of Language and Meaning.pdf:PDF},
  groups    = {Language, Language origins and evolution},
  publisher = {John Benjamins},
  ranking   = {rank3},
  type      = {Journal Article},
  url       = {https://www.jbe-platform.com/content/journals/10.1075/eoc.4.1.03ste},
}

@Article{Xie2020,
  author        = {Annie Xie and Dylan P. Losey and Ryan Tolsma and Chelsea Finn and Dorsa Sadigh},
  title         = {Learning Latent Representations to Influence Multi-Agent Interaction},
  year          = {2020},
  month         = nov,
  abstract      = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
  archiveprefix = {arXiv},
  eprint        = {2011.06619},
  file          = {:Xie2020 - Learning Latent Representations to Influence Multi Agent Interaction.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.RO, cs.AI, cs.LG},
  primaryclass  = {cs.RO},
}

@Article{Udagawa2019,
  author    = {Takuma Udagawa and Akiko Aizawa},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {A Natural Language Corpus of Common Grounding under Continuous and Partially-Observable Context},
  year      = {2019},
  month     = {jul},
  pages     = {7120--7127},
  volume    = {33},
  doi       = {10.1609/aaai.v33i01.33017120},
  file      = {:Udagawa2019 - A Natural Language Corpus of Common Grounding under Continuous and Partially Observable Context.pdf:PDF},
  groups    = {Language},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@Article{Chen2021,
  author    = {Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
  title     = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
  year      = {2021},
  month     = jun,
  abstract  = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  booktitle = {Advances in Neural Information Processing Systems},
  file      = {:Chen2021 - Decision Transformer_ Reinforcement Learning Via Sequence Modeling.pdf:PDF},
  groups    = {Offline RL, Transformers in RL},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/7f489f642a0ddb10272b5c31057f0663-Abstract.html},
}

@InProceedings{pmlr-v119-parisotto20a,
  author    = {Parisotto, Emilio and Song, Francis and Rae, Jack and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant and Jaderberg, Max and Kaufman, Rapha{\"e}l Lopez and Clark, Aidan and Noury, Seb and Botvinick, Matthew and Heess, Nicolas and Hadsell, Raia},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Stabilizing Transformers for Reinforcement Learning},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {7487--7498},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP). Harnessing the transformer’s ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical.},
  file      = {:pmlr-v119-parisotto20a - Stabilizing Transformers for Reinforcement Learning.pdf:PDF},
  groups    = {RL, Transformers in RL},
  pdf       = {http://proceedings.mlr.press/v119/parisotto20a/parisotto20a.pdf},
  ranking   = {rank3},
  url       = {https://proceedings.mlr.press/v119/parisotto20a.html},
}

@Article{Colas2020a,
  author        = {Cédric Colas and Ahmed Akakzia and Pierre-Yves Oudeyer and Mohamed Chetouani and Olivier Sigaud},
  title         = {Language-Conditioned Goal Generation: a New Approach to Language Grounding for RL},
  year          = {2020},
  month         = jun,
  abstract      = {In the real world, linguistic agents are also embodied agents: they perceive and act in the physical world. The notion of Language Grounding questions the interactions between language and embodiment: how do learning agents connect or ground linguistic representations to the physical world ? This question has recently been approached by the Reinforcement Learning community under the framework of instruction-following agents. In these agents, behavioral policies or reward functions are conditioned on the embedding of an instruction expressed in natural language. This paper proposes another approach: using language to condition goal generators. Given any goal-conditioned policy, one could train a language-conditioned goal generator to generate language-agnostic goals for the agent. This method allows to decouple sensorimotor learning from language acquisition and enable agents to demonstrate a diversity of behaviors for any given instruction. We propose a particular instantiation of this approach and demonstrate its benefits.},
  archiveprefix = {arXiv},
  eprint        = {2006.07043},
  file          = {:Colas2020a - Language Conditioned Goal Generation_ a New Approach to Language Grounding for RL.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.LG, cs.AI, cs.CL, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Veeriah2021,
  author    = {Vivek Veeriah and Tom Zahavy and Matteo Hessel and Zhongwen Xu and Junhyuk Oh and Iurii Kemaev and Hado van Hasselt and David Silver and Satinder Singh},
  title     = {Discovery of Options via Meta-Learned Subgoals},
  year      = {2021},
  month     = feb,
  abstract  = {Temporal abstractions in the form of options have been shown to help reinforcement learning (RL) agents learn faster. However, despite prior work on this topic, the problem of discovering options through interaction with an environment remains a challenge. In this paper, we introduce a novel meta-gradient approach for discovering useful options in multi-task RL environments. Our approach is based on a manager-worker decomposition of the RL agent, in which a manager maximises rewards from the environment by learning a task-dependent policy over both a set of task-independent discovered-options and primitive actions. The option-reward and termination functions that define a subgoal for each option are parameterised as neural networks and trained via meta-gradients to maximise their usefulness. Empirical analysis on gridworld and DeepMind Lab tasks show that: (1) our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, (2) the discovered options are frequently used by the agent while learning to solve the training tasks, and (3) that the discovered options help a randomly initialised manager learn faster in completely new tasks.},
  booktitle = {Advances in Neural Information Processing Systems 35 (NeurIPS 2021)},
  file      = {:Veeriah2021 - Discovery of Options Via Meta Learned Subgoals.pdf:PDF},
  groups    = {Macro-actions},
}

@InProceedings{bahdanau2018learning,
  author     = {Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette},
  booktitle  = {International Conference on Learning Representations},
  title      = {Learning to Understand Goal Specifications by Modelling Reward},
  year       = {2019},
  file       = {:bahdanau2018learning - Learning to Understand Goal Specifications by Modelling Reward.pdf:PDF},
  groups     = {Language-Augmented RL},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=H1xsSjC9Ym},
}

@Article{Narasimhan2018,
  author     = {Karthik Narasimhan and Regina Barzilay and Tommi Jaakkola},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {Grounding Language for Transfer in Deep Reinforcement Learning},
  year       = {2018},
  month      = {dec},
  pages      = {849--874},
  volume     = {63},
  doi        = {10.1613/jair.1.11263},
  file       = {:Narasimhan2018 - Grounding Language for Transfer in Deep Reinforcement Learning (3).pdf:PDF},
  groups     = {Language-Augmented RL},
  publisher  = {{AI} Access Foundation},
  readstatus = {read},
}

@Article{HermannHGWFSSCJ17,
  author     = {Karl Moritz Hermann and Felix Hill and Simon Green and Fumin Wang and Ryan Faulkner and Hubert Soyer and David Szepesvari and Wojciech Marian Czarnecki and Max Jaderberg and Denis Teplyashin and Marcus Wainwright and Chris Apps and Demis Hassabis and Phil Blunsom},
  title      = {Grounded Language Learning in a Simulated 3D World},
  year       = {2017},
  cdate      = {1483228800000},
  file       = {:HermannHGWFSSCJ17 - Grounded Language Learning in a Simulated 3D World.pdf:PDF},
  groups     = {Language-Augmented RL},
  publtype   = {informal},
  ranking    = {rank2},
  readstatus = {read},
  url        = {http://arxiv.org/abs/1706.06551},
}

@InProceedings{Goyal2019,
  author       = {Goyal, P. and Niekum, S. and Mooney, R.J.},
  title        = {Using Natural Language for Reward Shaping in Reinforcement Learning},
  year         = {2019},
  abstractnote = {Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma’s Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60 % more often on average, compared to learning without language.},
  file         = {:osti_10100557 - Using Natural Language for Reward Shaping in Reinforcement Learning.pdf:PDF},
  groups       = {Language-Augmented RL},
  journal      = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  place        = {Country unknown/Code not available},
  ranking      = {rank1},
  url          = {https://par.nsf.gov/biblio/10100557},
}

@Article{Hospedales2020,
  author        = {Timothy Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey},
  title         = {Meta-Learning in Neural Networks: A Survey},
  year          = {2020},
  month         = apr,
  abstract      = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  eprint        = {2004.05439},
  file          = {:Hospedales2020 - Meta Learning in Neural Networks_ a Survey.pdf:PDF},
  groups        = {Meta-Learning},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
}

@InProceedings{10.1145/3356464.3357707,
  author    = {Zhou, Ming and Chen, Yong and Wen, Ying and Yang, Yaodong and Su, Yufeng and Zhang, Weinan and Zhang, Dell and Wang, Jun},
  booktitle = {Proceedings of the First International Conference on Distributed Artificial Intelligence},
  title     = {Factorized Q-Learning for Large-Scale Multi-Agent Systems},
  year      = {2019},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {DAI '19},
  abstract  = {Deep Q-learning has achieved significant success in single-agent decision making tasks. However, it is challenging to extend Q-learning to large-scale multi-agent scenarios, due to the explosion of action space resulting from the complex dynamics between the environment and the agents. In this paper, we propose to make the computation of multi-agent Q-learning tractable by treating the Q-function (w.r.t. state and joint-action) as a high-order high-dimensional tensor and then approximate it with factorized pairwise interactions. Furthermore, we utilize a composite deep neural network architecture for computing the factorized Q-function, share the model parameters among all the agents within the same group, and estimate the agents' optimal joint actions through a coordinate descent type algorithm. All these simplifications greatly reduce the model complexity and accelerate the learning process. Extensive experiments on two different multi-agent problems demonstrate the performance gain of our proposed approach in comparison with strong baselines, particularly when there are a large number of agents.},
  articleno = {7},
  doi       = {10.1145/3356464.3357707},
  groups    = {Centralised Training and Execution},
  isbn      = {9781450376563},
  keywords  = {multi-agent reinforcement learning, large-scale multi-agent systems},
  location  = {Beijing, China},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3356464.3357707},
}

@InProceedings{Gupta2021_DynamicPop,
  author     = {Abhinav Gupta and Marc Lanctot and Angeliki Lazaridou},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Dynamic population-based meta-learning for multi-agent communication with natural language},
  year       = {2021},
  editor     = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  file       = {:Gupta2021a - Dynamic Population Based Meta Learning for Multi Agent Communication with Natural Language.pdf:PDF},
  groups     = {Natural language, Discrete language, Speaker-Listener},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2021/hash/8caa38721906c1a0bb95c80fab33a893-Abstract.html},
}

@InProceedings{Liu2021,
  author     = {Dianbo Liu and Alex Lamb and Kenji Kawaguchi and Anirudh Goyal and Chen Sun and Michael Curtis Mozer and Yoshua Bengio},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Discrete-Valued Neural Communication},
  year       = {2021},
  editor     = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  comment    = {Discretization of values sent between different modules of a neural architecture.},
  file       = {:Liu2021 - Discrete Valued Neural Communication.pdf:PDF},
  groups     = {Machine Learning},
  ranking    = {rank1},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=YSYXmOzlrou},
}

@InProceedings{Tucker2021_DiscreteEC,
  author     = {Mycal Tucker and Huao Li and Siddharth Agrawal and Dana Hughes and Katia P. Sycara and Michael Lewis and Julie Shah},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Emergent Discrete Communication in Semantic Spaces},
  year       = {2021},
  editor     = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  file       = {:Tucker2021 - Emergent Discrete Communication in Semantic Spaces.pdf:PDF},
  groups     = {Discrete language},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2021/hash/5812f92450ccaf17275500841c70924a-Abstract.html},
}

@InProceedings{Chevalier2018,
  author    = {Maxime Chevalier-Boisvert and Dzmitry Bahdanau and Salem Lahlou and Lucas Willems and Chitwan Saharia and Thien Huu Nguyen and Yoshua Bengio},
  booktitle = {International Conference on Learning Representations},
  title     = {Baby{AI}: First Steps Towards Grounded Language Learning With a Human In the Loop},
  year      = {2019},
  file      = {:Chevalier2018 - BabyAI_ First Steps Towards Grounded Language Learning with a Human in the Loop.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio1},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=rJeXCo0cYX},
}

@InProceedings{WangLM16,
  author    = {Sida I. Wang and Percy Liang and Christopher D. Manning},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  title     = {Learning Language Games through Interaction},
  year      = {2016},
  cdate     = {1451606400000},
  file      = {:WangLM16 - Learning Language Games through Interaction.pdf:PDF},
  groups    = {Language-Augmented RL},
  url       = {http://aclweb.org/anthology/P/P16/P16-1224.pdf},
}

@InProceedings{pmlr-v78-florensa17a,
  author    = {Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
  title     = {Reverse Curriculum Generation for Reinforcement Learning},
  year      = {2017},
  editor    = {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
  month     = {13--15 Nov},
  pages     = {482--495},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {78},
  abstract  = {Many relevant tasks require an agent to reach a certain state, or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved.  The robot is trained in “reverse", gradually learning to reach the goal from a set of starting positions increasingly far from the goal. Our method automatically generates a curriculum of starting positions that adapts to the agent’s performance, leading to efficient training on goal-oriented tasks.  We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.},
  file      = {:pmlr-v78-florensa17a - Reverse Curriculum Generation for Reinforcement Learning.pdf:PDF},
  groups    = {Curriculum Learning},
  pdf       = {http://proceedings.mlr.press/v78/florensa17a/florensa17a.pdf},
  url       = {https://proceedings.mlr.press/v78/florensa17a.html},
}

@InProceedings{Mei2016,
  author     = {Hongyuan Mei and Mohit Bansal and Matthew Walter},
  title      = {Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences},
  year       = {2016},
  abstract   = {We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence "regions" salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or task-specific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.},
  conference = {AAAI Conference on Artificial Intelligence},
  file       = {:Mei2016 - Listen, Attend, and Walk_ Neural Mapping of Navigational Instructions to Action Sequences.pdf:PDF},
  groups     = {Language-Augmented RL},
  keywords   = {direction following; natural language processing; natural language semantics},
  priority   = {prio2},
  ranking    = {rank2},
  url        = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12522/12021},
}

@Article{Chaplot_MysoreSathyendra_Pasumarthi_Rajagopal_Salakhutdinov_2018,
  author       = {Chaplot, Devendra Singh and Mysore Sathyendra, Kanthashree and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Gated-Attention Architectures for Task-Oriented Language Grounding},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states. &lt;/p&gt;},
  file         = {:Chaplot_MysoreSathyendra_Pasumarthi_Rajagopal_Salakhutdinov_2018 - Gated Attention Architectures for Task Oriented Language Grounding.pdf:PDF},
  groups       = {Language-Augmented RL},
  priority     = {prio2},
  ranking      = {rank2},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11832},
}

@Article{Janner2018,
  author    = {Michael Janner and Karthik Narasimhan and Regina Barzilay},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {Representation Learning for Grounded Spatial Reasoning},
  year      = {2018},
  month     = {dec},
  pages     = {49--61},
  volume    = {6},
  doi       = {10.1162/tacl_a_00004},
  file      = {:Janner2018 - Representation Learning for Grounded Spatial Reasoning.pdf:PDF},
  groups    = {Language-Augmented RL},
  publisher = {{MIT} Press - Journals},
}

@InProceedings{Chen_2019_CVPR,
  author    = {Chen, Howard and Suhr, Alane and Misra, Dipendra and Snavely, Noah and Artzi, Yoav},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments},
  year      = {2019},
  month     = {June},
  file      = {:Chen_2019_CVPR - TOUCHDOWN_ Natural Language Navigation and Spatial Reasoning in Visual Street Environments.pdf:PDF},
  groups    = {Language-Augmented RL},
  ranking   = {rank3},
}

@InProceedings{misra-etal-2017-mapping,
  author    = {Misra, Dipendra and Langford, John and Artzi, Yoav},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  title     = {Mapping Instructions and Visual Observations to Actions with Reinforcement Learning},
  year      = {2017},
  address   = {Copenhagen, Denmark},
  month     = sep,
  pages     = {1004--1015},
  publisher = {Association for Computational Linguistics},
  abstract  = {We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent{'}s exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.},
  doi       = {10.18653/v1/D17-1106},
  file      = {:misra-etal-2017-mapping - Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio2},
  ranking   = {rank2},
  url       = {https://aclanthology.org/D17-1106},
}

@InProceedings{Anderson_2018_CVPR,
  author     = {Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and Sünderhauf, Niko and Reid, Ian and Gould, Stephen and van den Hengel, Anton},
  booktitle  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments},
  year       = {2018},
  month      = {June},
  file       = {:Anderson_2018_CVPR - Vision and Language Navigation_ Interpreting Visually Grounded Navigation Instructions in Real Environments.pdf:PDF},
  groups     = {Visual-and-Language Navigation},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html},
}

@InProceedings{Wang_2019_CVPR,
  author    = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation},
  year      = {2019},
  month     = {June},
  file      = {:Wang_2019_CVPR - Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation.pdf:PDF},
  groups    = {Language-Augmented RL},
}

@InProceedings{NEURIPS2018_85fc37b1,
  author    = {Khadka, Shauharda and Tumer, Kagan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Evolution-Guided Policy Gradient in Reinforcement Learning},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {31},
  file      = {:NEURIPS2018_85fc37b1 - Evolution Guided Policy Gradient in Reinforcement Learning.pdf:PDF},
  groups    = {Evolutionary RL},
  ranking   = {rank3},
  url       = {https://proceedings.neurips.cc/paper/2018/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf},
}

@InProceedings{pmlr-v70-oh17a,
  author    = {Junhyuk Oh and Satinder Singh and Honglak Lee and Pushmeet Kohli},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {06--11 Aug},
  pages     = {2661--2670},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.},
  file      = {:pmlr-v70-oh17a - Zero Shot Task Generalization with Multi Task Deep Reinforcement Learning.pdf:PDF},
  groups    = {Language-Augmented RL},
  pdf       = {http://proceedings.mlr.press/v70/oh17a/oh17a.pdf},
  priority  = {prio2},
  ranking   = {rank2},
  url       = {https://proceedings.mlr.press/v70/oh17a.html},
}

@InProceedings{Gordon2018,
  author     = {Gordon, Daniel and Kembhavi, Aniruddha and Rastegari, Mohammad and Redmon, Joseph and Fox, Dieter and Farhadi, Ali},
  booktitle  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {IQA: Visual Question Answering in Interactive Environments},
  year       = {2018},
  month      = {June},
  file       = {:Gordon2018 - IQA_ Visual Question Answering in Interactive Environments.pdf:PDF},
  groups     = {Language-Augmented RL},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://openaccess.thecvf.com/content_cvpr_2018/html/Gordon_IQA_Visual_Question_CVPR_2018_paper.html},
}

@InProceedings{Andreas2016,
  author     = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {Neural Module Networks},
  year       = {2016},
  month      = {June},
  file       = {:Andreas2016 - Neural Module Networks.pdf:PDF},
  groups     = {Language},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@InProceedings{Shu2018_Hierarchical,
  author     = {Tianmin Shu and Caiming Xiong and Richard Socher},
  booktitle  = {International Conference on Learning Representations},
  title      = {Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},
  year       = {2018},
  file       = {:shu2018hierarchical - Hierarchical and Interpretable Skill Acquisition in Multi Task Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL, Language-Augmented RL},
  ranking    = {rank1},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=SJJQVZW0b},
}

@InProceedings{NIPS2016_c4492cbe,
  author    = {Vezhnevets, Alexander and Mnih, Volodymyr and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Agapiou, John and kavukcuoglu, koray},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Strategic Attentive Writer for Learning Macro-Actions},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  file      = {:NIPS2016_c4492cbe - Strategic Attentive Writer for Learning Macro Actions.pdf:PDF},
  groups    = {Macro-actions},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper/2016/file/c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf},
}

@InProceedings{NEURIPS2019_7967cc8e,
  author     = {Hu, Hengyuan and Yarats, Denis and Gong, Qucheng and Tian, Yuandong and Lewis, Mike},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Hierarchical Decision Making by Generating and Following Natural Language Instructions},
  year       = {2019},
  editor     = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {32},
  file       = {:NEURIPS2019_7967cc8e - Hierarchical Decision Making by Generating and Following Natural Language Instructions (1).pdf:PDF},
  groups     = {Language-Augmented RL},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2019/hash/7967cc8e3ab559e68cc944c44b1cf3e8-Abstract.html},
}

@InProceedings{Ndousse2021,
  author    = {Ndousse, Kamal K and Eck, Douglas and Levine, Sergey and Jaques, Natasha},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Emergent Social Learning via Multi-agent Reinforcement Learning},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {7991--8004},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start.},
  file      = {:Ndousse2021 - Emergent Social Learning Via Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Social Learning},
  pdf       = {http://proceedings.mlr.press/v139/ndousse21a/ndousse21a.pdf},
  url       = {https://proceedings.mlr.press/v139/ndousse21a.html},
}

@InProceedings{yu2018,
  author    = {Haonan Yu and Haichao Zhang and Wei Xu},
  booktitle = {International Conference on Learning Representations},
  title     = {Interactive Grounded Language Acquisition and Generalization in a 2D World},
  year      = {2018},
  file      = {:yu2018 - Interactive Grounded Language Acquisition and Generalization in a 2D World.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio2},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=H1UOm4gA-},
}

@InProceedings{Puig_2018_CVPR,
  author    = {Puig, Xavier and Ra, Kevin and Boben, Marko and Li, Jiaman and Wang, Tingwu and Fidler, Sanja and Torralba, Antonio},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {VirtualHome: Simulating Household Activities via Programs},
  year      = {2018},
  month     = {June},
  file      = {:Puig_2018_CVPR - VirtualHome_ Simulating Household Activities Via Programs.pdf:PDF},
  groups    = {Macro-actions},
  ranking   = {rank2},
}

@InProceedings{Gupta2017,
  author    = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Cognitive Mapping and Planning for Visual Navigation},
  year      = {2017},
  month     = {July},
  file      = {:Gupta2017 - Cognitive Mapping and Planning for Visual Navigation.pdf:PDF},
  groups    = {Navigation},
  ranking   = {rank3},
}

@InProceedings{Ng1999_Shaping,
  author    = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
  booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
  title     = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
  year      = {1999},
  address   = {San Francisco, CA, USA},
  pages     = {278–287},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series    = {ICML '99},
  file      = {:10.5555_645528.657613 - Policy Invariance under Reward Transformations_ Theory and Application to Reward Shaping.pdf:PDF},
  groups    = {RL, Reward Shaping},
  isbn      = {1558606122},
  numpages  = {10},
  ranking   = {rank4},
}

@Article{Chen_Mooney_2011,
  author       = {Chen, David and Mooney, Raymond},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Learning to Interpret Natural Language Navigation Instructions from Observations},
  year         = {2011},
  month        = {Aug.},
  number       = {1},
  pages        = {859-865},
  volume       = {25},
  abstractnote = {&lt;p&gt; The ability to understand natural-language instructions is critical to building intelligent agents that interact with humans. We present a system that learns to transform natural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation instructions for these environments is used for training and testing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the system is able to automatically learnto correctly interpret a reasonable fraction of the complex instructions in this corpus. &lt;/p&gt;},
  file         = {:Chen_Mooney_2011 - Learning to Interpret Natural Language Navigation Instructions from Observations.pdf:PDF},
  groups       = {Visual-and-Language Navigation},
  ranking      = {rank2},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/7974},
}

@InProceedings{6696569,
  author    = {Guadarrama, Sergio and Riano, Lorenzo and Golland, Dave and Go¨hring, Daniel and Jia, Yangqing and Klein, Dan and Abbeel, Pieter and Darrell, Trevor},
  booktitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title     = {Grounding spatial relations for human-robot interaction},
  year      = {2013},
  pages     = {1640-1647},
  doi       = {10.1109/IROS.2013.6696569},
  groups    = {Visual-and-Language Navigation},
  ranking   = {rank1},
}

@Article{Tellex_Kollar_Dickerson_Walter_Banerjee_Teller_Roy_2011,
  author       = {Tellex, Stefanie and Kollar, Thomas and Dickerson, Steven and Walter, Matthew and Banerjee, Ashis and Teller, Seth and Roy, Nicholas},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation},
  year         = {2011},
  month        = {Aug.},
  number       = {1},
  pages        = {1507-1514},
  volume       = {25},
  abstractnote = {&lt;p&gt; This paper describes a new model for understanding natural language commands given to autonomous systems that perform navigation and mobile manipulation in semi-structured environments. Previous approaches have used models with fixed structure to infer the likelihood of a sequence of actions given the environment and the command. In contrast, our framework, called Generalized Grounding Graphs, dynamically instantiates a probabilistic graphical model for a particular natural language command according to the command’s hierarchical and compositional semantic structure. Our system performs inference in the model to successfully find and execute plans corresponding to natural language commands such as &quot;Put the tire pallet on the truck.&quot; The model is trained using a corpus of commands collected using crowdsourcing. We pair each command with robot actions and use the corpus to learn the parameters of the model. We evaluate the robot’s performance by inferring plans from natural language commands, executing each plan in a realistic robot simulator, and asking users to evaluate the system’s performance. We demonstrate that our system can successfully follow many natural language commands from the corpus. &lt;/p&gt;},
  file         = {:Tellex_Kollar_Dickerson_Walter_Banerjee_Teller_Roy_2011 - Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation.pdf:PDF},
  groups       = {Visual-and-Language Navigation},
  ranking      = {rank3},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/7979},
}

@InProceedings{Wierstra2007_POMDPs,
  author    = {Wierstra, Daan and Foerster, Alexander and Peters, Jan and Schmidhuber, J{\"u}rgen},
  booktitle = {Artificial Neural Networks -- ICANN 2007},
  title     = {Solving Deep Memory POMDPs with Recurrent Policy Gradients},
  year      = {2007},
  address   = {Berlin, Heidelberg},
  editor    = {de S{\'a}, Joaquim Marques and Alexandre, Lu{\'i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
  pages     = {697--706},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper presents Recurrent Policy Gradients, a model-free reinforcement learning (RL) method creating limited-memory sto-chastic policies for partially observable Markov decision problems (POMDPs) that require long-term memories of past observations. The approach involves approximating a policy gradient for a Recurrent Neural Network (RNN) by backpropagating return-weighted characteristic eligibilities through time. Using a ``Long Short-Term Memory'' architecture, we are able to outperform other RL methods on two important benchmark tasks. Furthermore, we show promising results on a complex car driving simulation task.},
  file      = {:10.1007_978-3-540-74690-4_71 - Solving Deep Memory POMDPs with Recurrent Policy Gradients.pdf:PDF},
  groups    = {POMDP},
  isbn      = {978-3-540-74690-4},
  ranking   = {rank2},
  url       = {https://link.springer.com/chapter/10.1007/978-3-540-74690-4_71},
}

@InProceedings{NIPS2016_9dcb88e0,
  author    = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Hierarchical Question-Image Co-Attention for Visual Question Answering},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  file      = {:NIPS2016_9dcb88e0 - Hierarchical Question Image Co Attention for Visual Question Answering.pdf:PDF},
  groups    = {Visual-Language Learning},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper/2016/file/9dcb88e0137649590b755372b040afad-Paper.pdf},
}

@InProceedings{Yang_2016_CVPR,
  author    = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Stacked Attention Networks for Image Question Answering},
  year      = {2016},
  month     = {June},
  file      = {:Yang_2016_CVPR - Stacked Attention Networks for Image Question Answering.pdf:PDF},
  groups    = {Visual-Language Learning},
  ranking   = {rank4},
}

@InProceedings{Anderson_2018_CVPR,
  author    = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  year      = {2018},
  month     = {June},
  file      = {:Anderson_2018_CVPR - Bottom up and Top down Attention for Image Captioning and Visual Question Answering.pdf:PDF},
  groups    = {Visual-Language Learning},
  ranking   = {rank5},
}

@Article{6788810,
  author  = {Smith, Linda and Gasser, Michael},
  journal = {Artificial Life},
  title   = {The Development of Embodied Cognition: Six Lessons from Babies},
  year    = {2005},
  number  = {1-2},
  pages   = {13-29},
  volume  = {11},
  doi     = {10.1162/1064546053278973},
  file    = {:6788810 - The Development of Embodied Cognition_ Six Lessons from Babies.pdf:PDF},
  groups  = {Developmental psychology},
  ranking = {rank3},
}

@Book{piaget1977development,
  author    = {Piaget, Jean},
  publisher = {Viking},
  title     = {The development of thought: Equilibration of cognitive structures.(Trans A. Rosin).},
  year      = {1977},
  groups    = {Developmental psychology},
  ranking   = {rank5},
}

@InProceedings{fu2018from,
  author    = {Justin Fu and Anoop Korattikara and Sergey Levine and Sergio Guadarrama},
  booktitle = {International Conference on Learning Representations},
  title     = {From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following},
  year      = {2019},
  file      = {:fu2018from - From Language to Goals_ Inverse Reinforcement Learning for Vision Based Instruction Following.pdf:PDF},
  groups    = {Language-Augmented RL},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=r1lq1hRqYQ},
}

@Article{lynch2021language,
  author        = {Corey Lynch and Pierre Sermanet},
  journal       = {Robotics: Science and Systems},
  title         = {Language Conditioned Imitation Learning over Unstructured Data},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2005.07648},
  file          = {:lynch2021language - Language Conditioned Imitation Learning Over Unstructured Data (1).pdf:PDF},
  groups        = {Language-Augmented RL},
  ranking       = {rank2},
  readstatus    = {read},
}

@Article{Najar2020,
  author     = {Anis Najar and Olivier Sigaud and Mohamed Chetouani},
  journal    = {Autonomous Agents and Multi-Agent Systems},
  title      = {Interactively shaping robot behaviour with unlabeled human instructions},
  year       = {2020},
  month      = {may},
  number     = {2},
  volume     = {34},
  doi        = {10.1007/s10458-020-09459-6},
  file       = {:Najar2020 - Interactively Shaping Robot Behaviour with Unlabeled Human Instructions.pdf:PDF},
  groups     = {Language-Augmented RL},
  publisher  = {Springer Science and Business Media {LLC}},
  readstatus = {read},
}

@InProceedings{janner2021offline,
  author    = {Michael Janner and Qiyang Li and Sergey Levine},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Offline Reinforcement Learning as One Big Sequence Modeling Problem},
  year      = {2021},
  editor    = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  groups    = {Offline RL},
  ranking   = {rank3},
  url       = {https://openreview.net/forum?id=wgeK563QgSw},
}

@InProceedings{NIPS2016_f442d33f,
  author    = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  file      = {:NIPS2016_f442d33f - Hierarchical Deep Reinforcement Learning_ Integrating Temporal Abstraction and Intrinsic Motivation.pdf:PDF},
  groups    = {Hierarchical RL},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper/2016/file/f442d33fa06832082290ad8544a8da27-Paper.pdf},
}

@Article{Tessler_Givony_Zahavy_Mankowitz_Mannor_2017,
  author       = {Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel and Mannor, Shie},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {A Deep Hierarchical Approach to Lifelong Learning in Minecraft},
  year         = {2017},
  month        = {Feb.},
  number       = {1},
  volume       = {31},
  abstractnote = {&lt;p&gt; We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill distillation enables the H-DRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft. &lt;/p&gt;},
  file         = {:Tessler_Givony_Zahavy_Mankowitz_Mannor_2017 - A Deep Hierarchical Approach to Lifelong Learning in Minecraft.pdf:PDF},
  groups       = {Hierarchical RL},
  ranking      = {rank3},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/10744},
}

@InProceedings{levy2018hierarchical,
  author    = {Andrew Levy and Robert Platt and Kate Saenko},
  booktitle = {International Conference on Learning Representations},
  title     = {Hierarchical Reinforcement Learning with Hindsight},
  year      = {2019},
  file      = {:levy2018hierarchical - Hierarchical Reinforcement Learning with Hindsight.pdf:PDF},
  groups    = {Hierarchical RL},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=ryzECoAcY7},
}

@InProceedings{NEURIPS2018_e6384711,
  author    = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Data-Efficient Hierarchical Reinforcement Learning},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {31},
  file      = {:NEURIPS2018_e6384711 - Data Efficient Hierarchical Reinforcement Learning.pdf:PDF},
  groups    = {Hierarchical RL},
  ranking   = {rank3},
  url       = {https://proceedings.neurips.cc/paper/2018/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
}

@InProceedings{frans2018meta,
  author    = {Kevin Frans and Jonathan Ho and Xi Chen and Pieter Abbeel and John Schulman},
  booktitle = {International Conference on Learning Representations},
  title     = {Meta Learning Shared Hierarchies},
  year      = {2018},
  file      = {:frans2018meta - Meta Learning Shared Hierarchies.pdf:PDF},
  groups    = {Hierarchical RL, Meta-Learning},
  ranking   = {rank3},
  url       = {https://openreview.net/forum?id=SyX0IeWAW},
}

@Article{Bacon_Harb_Precup_2017,
  author       = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {The Option-Critic Architecture},
  year         = {2017},
  month        = {Feb.},
  number       = {1},
  volume       = {31},
  abstractnote = {&lt;p&gt; Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging.We tackle this problem in the framework of options [Sutton,Precup and Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework. &lt;/p&gt;},
  file         = {:Bacon_Harb_Precup_2017 - The Option Critic Architecture.pdf:PDF},
  groups       = {Macro-actions},
  ranking      = {rank4},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/10916},
}

@InProceedings{NEURIPS2018_6a81681a,
  author     = {Fried, Daniel and Hu, Ronghang and Cirik, Volkan and Rohrbach, Anna and Andreas, Jacob and Morency, Louis-Philippe and Berg-Kirkpatrick, Taylor and Saenko, Kate and Klein, Dan and Darrell, Trevor},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Speaker-Follower Models for Vision-and-Language Navigation},
  year       = {2018},
  editor     = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {31},
  file       = {:NEURIPS2018_6a81681a - Speaker Follower Models for Vision and Language Navigation.pdf:PDF},
  groups     = {Language-Augmented RL},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2018/file/6a81681a7af700c6385d36577ebec359-Paper.pdf},
}

@InProceedings{co-reyes2018metalearning,
  author     = {John D Co-Reyes and Abhishek Gupta and Suvansh Sanjeev and Nick Altieri and John DeNero and Pieter Abbeel and Sergey Levine},
  booktitle  = {International Conference on Learning Representations},
  title      = {Guiding Policies with Language via Meta-Learning},
  year       = {2019},
  file       = {:co-reyes2018metalearning - Meta Learning Language Guided Policy Learning.pdf:PDF},
  groups     = {Meta-Learning, Language-Augmented RL},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=HkgSEnA5KQ},
}

@InProceedings{Johnson_2017_CVPR,
  author    = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  year      = {2017},
  month     = {July},
  file      = {:Johnson_2017_CVPR - CLEVR_ a Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning.pdf:PDF},
  groups    = {Question Answering, Language},
  ranking   = {rank5},
  url       = {https://openaccess.thecvf.com/content_cvpr_2017/html/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.html},
}

@InProceedings{pmlr-v100-nachum20a,
  author    = {Nachum, Ofir and Ahn, Michael and Ponte, Hugo and Gu, Shixiang (Shane) and Kumar, Vikash},
  booktitle = {Proceedings of the Conference on Robot Learning},
  title     = {Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real},
  year      = {2020},
  editor    = {Kaelbling, Leslie Pack and Kragic, Danica and Sugiura, Komei},
  month     = {30 Oct--01 Nov},
  pages     = {110--121},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  abstract  = {Manipulation and locomotion are closely related problems that are often studied in isolation. In this work, we study the problem of coordinating multiple mobile agents to exhibit manipulation behaviors using a reinforcement learning (RL) approach. Our method hinges on the use of hierarchical sim2real – a simulated environment is used to learn low-level goal-reaching skills, which are then used as the action space for a high-level RL controller, also trained in simulation. The full hierarchical policy is then transferred to the real world in a zero-shot fashion. The application of domain randomization during training enables the learned behaviors to generalize to real-world settings, while the use of hierarchy provides a modular paradigm for learning and transferring increasingly complex behaviors. We evaluate our method on a number of real-world tasks, including coordinated object manipulation in a multi-agent setting.},
  file      = {:pmlr-v100-nachum20a - Multi Agent Manipulation Via Locomotion Using Hierarchical Sim2Real.pdf:PDF},
  groups    = {Sim to Real, Hierarchical RL, Multi-robot},
  pdf       = {http://proceedings.mlr.press/v100/nachum20a/nachum20a.pdf},
  priority  = {prio2},
  ranking   = {rank1},
  url       = {https://proceedings.mlr.press/v100/nachum20a.html},
}

@InProceedings{Zhao2020,
  author    = {Wenshuai Zhao and Jorge Pena Queralta and Tomi Westerlund},
  booktitle = {2020 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
  title     = {Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey},
  year      = {2020},
  month     = {dec},
  publisher = {{IEEE}},
  doi       = {10.1109/ssci47803.2020.9308468},
  file      = {:Zhao2020 - Sim to Real Transfer in Deep Reinforcement Learning for Robotics_ a Survey.pdf:PDF},
  groups    = {Sim to Real},
  ranking   = {rank2},
}

@InProceedings{pmlr-v37-schaul15,
  author    = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  title     = {Universal Value Function Approximators},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Bach, Francis and Blei, David},
  month     = {07--09 Jul},
  pages     = {1312--1320},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  abstract  = {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
  comment   = {DQN with different possible goals
the agent takes as input the current state and goal, the value function depends also on this goal},
  file      = {:pmlr-v37-schaul15 - Universal Value Function Approximators.pdf:PDF},
  groups    = {RL, Value based, Goal-conditioned RL},
  pdf       = {http://proceedings.mlr.press/v37/schaul15.pdf},
  ranking   = {rank4},
  url       = {https://proceedings.mlr.press/v37/schaul15.html},
}

@Article{Dafoe2020,
  author        = {Allan Dafoe and Edward Hughes and Yoram Bachrach and Tantum Collins and Kevin R. McKee and Joel Z. Leibo and Kate Larson and Thore Graepel},
  title         = {Open Problems in Cooperative AI},
  year          = {2020},
  month         = dec,
  abstract      = {Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.},
  archiveprefix = {arXiv},
  eprint        = {2012.08630},
  file          = {:Dafoe2020 - Open Problems in Cooperative AI.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.AI, cs.MA},
  primaryclass  = {cs.AI},
  ranking       = {rank2},
}

@InProceedings{NEURIPS2020_ca3a9be7,
  author    = {Zheng, Han and Wei, Pengfei and Jiang, Jing and Long, Guodong and Lu, Qinghua and Zhang, Chengqi},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Cooperative Heterogeneous Deep Reinforcement Learning},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {17455--17465},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  abstract  = {Numerous deep reinforcement learning agents have been proposed, and each of them has its strengths and flaws. In this work, we present a Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a policy by integrating the advantages of heterogeneous agents. Specifically, we propose a cooperative learning framework that classifies heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efficient, to guide the learning of local agents so that local agents can benefit from sample-efficient agents and simultaneously maintain their advantages, e.g., stability. Global agents also benefit from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines.},
  file      = {:NEURIPS2020_ca3a9be7 - Cooperative Heterogeneous Deep Reinforcement Learning.pdf:PDF},
  groups    = {Heterogeneous MADRL},
  url       = {https://proceedings.neurips.cc/paper/2020/file/ca3a9be77f7e88708afb20c8cdf44b60-Paper.pdf},
}

@InProceedings{Zhong2020RTFM,
  author    = {Victor Zhong and Tim Rockt\"{a}schel and Edward Grefenstette},
  booktitle = {International Conference on Learning Representations},
  title     = {RTFM: Generalising to New Environment Dynamics via Reading},
  year      = {2020},
  file      = {:Zhong2020RTFM - RTFM_ Generalising to New Environment Dynamics Via Reading.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio3},
  url       = {https://arxiv.org/abs/1910.08210},
}

@InProceedings{pmlr-v15-ross11a,
  author    = {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  title     = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  year      = {2011},
  address   = {Fort Lauderdale, FL, USA},
  editor    = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  month     = {11--13 Apr},
  pages     = {627--635},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {15},
  abstract  = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
  comment   = {DAgger},
  file      = {:pmlr-v15-ross11a - A Reduction of Imitation Learning and Structured Prediction to No Regret Online Learning.pdf:PDF},
  groups    = {Imitation},
  pdf       = {http://proceedings.mlr.press/v15/ross11a/ross11a.pdf},
  ranking   = {rank5},
  url       = {https://proceedings.mlr.press/v15/ross11a.html},
}

@Article{Zelikman2022,
  author        = {Eric Zelikman and Yuhuai Wu and Noah D. Goodman},
  title         = {STaR: Bootstrapping Reasoning With Reasoning},
  year          = {2022},
  month         = mar,
  abstract      = {Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
  archiveprefix = {arXiv},
  eprint        = {2203.14465},
  file          = {:Zelikman2022 - STaR_ Bootstrapping Reasoning with Reasoning.pdf:PDF},
  groups        = {Question Answering},
  keywords      = {cs.LG, cs.AI, cs.CL},
  primaryclass  = {cs.LG},
}

@Article{Mu2022,
  author     = {Jesse Mu and Victor Zhong and Roberta Raileanu and Minqi Jiang and Noah Goodman and Tim Rocktäschel and Edward Grefenstette},
  title      = {Improving Intrinsic Exploration with Language Abstractions},
  year       = {2022},
  abstract   = {Reinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.},
  booktitle  = {Advances in Neural Information Processing Systems},
  comment    = {L-AMIGO L-NovelD},
  editor     = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  file       = {:Mu2022 - Improving Intrinsic Exploration with Language Abstractions.pdf:PDF},
  groups     = {Language-Augmented RL},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=ALIYCycCsTy},
}

@InProceedings{campero2021learning,
  author    = {Andres Campero and Roberta Raileanu and Heinrich Kuttler and Joshua B. Tenenbaum and Tim Rockt{\"a}schel and Edward Grefenstette},
  booktitle = {International Conference on Learning Representations},
  title     = {Learning with AMIGo: Adversarially Motivated Intrinsic Goals},
  year      = {2021},
  file      = {:campero2021learning - Learning with AMIG_o_ Adversarially Motivated Intrinsic Goals.pdf:PDF},
  groups    = {Adversary Guidance, Intrinsic goals},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=ETBc_MIMgoX},
}

@InProceedings{Zhang2021_NovelD,
  author     = {Zhang, Tianjun and Xu, Huazhe and Wang, Xiaolong and Wu, Yi and Keutzer, Kurt and Gonzalez, Joseph E and Tian, Yuandong},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {NovelD: A Simple yet Effective Exploration Criterion},
  year       = {2021},
  editor     = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages      = {25217--25230},
  publisher  = {Curran Associates, Inc.},
  volume     = {34},
  file       = {:NEURIPS2021_d428d070 - NovelD_ a Simple yet Effective Exploration Criterion.pdf:PDF},
  groups     = {Curiosity, Intrinsic goals},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2021/file/d428d070622e0f4363fceae11f4a3576-Paper.pdf},
}

@InProceedings{NEURIPS2021_f6f15441,
  author     = {Mirchandani, Suvir and Karamcheti, Siddharth and Sadigh, Dorsa},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {ELLA: Exploration through Learned Language Abstraction},
  year       = {2021},
  editor     = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages      = {29529--29540},
  publisher  = {Curran Associates, Inc.},
  volume     = {34},
  file       = {:NEURIPS2021_f6f15441 - ELLA_ Exploration through Learned Language Abstraction.pdf:PDF},
  groups     = {Language-Augmented RL},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2021/file/f6f154417c4665861583f9b9c4afafa2-Paper.pdf},
}

@InProceedings{Lazaridou2020_MACNatLang,
  author     = {Lazaridou, Angeliki and Potapenko, Anna and Tieleman, Olivier},
  booktitle  = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title      = {Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning},
  year       = {2020},
  address    = {Online},
  month      = jul,
  pages      = {7663--7674},
  publisher  = {Association for Computational Linguistics},
  abstract   = {We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.},
  doi        = {10.18653/v1/2020.acl-main.685},
  file       = {:lazaridou-etal-2020-multi - Multi Agent Communication Meets Natural Language_ Synergies between Functional and Structural Language Learning.pdf:PDF},
  groups     = {Natural language, Discrete language},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://aclanthology.org/2020.acl-main.685},
}

@Article{Donati2021,
  author        = {Alice Martin Donati and Guillaume Quispe and Charles Ollion and Sylvain Le Corff and Florian Strub and Olivier Pietquin},
  title         = {Learning Natural Language Generation from Scratch},
  year          = {2021},
  month         = sep,
  abstract      = {This paper introduces TRUncated ReinForcement Learning for Language (TrufLL), an original ap-proach to train conditional language models from scratch by only using reinforcement learning (RL). AsRL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary spaceusing a generic language model. TrufLL thus enables to train a language agent by solely interacting withits environment without any task-specific prior knowledge; it is only guided with a task-agnostic languagemodel. Interestingly, this approach avoids the dependency to labelled datasets and inherently reduces pre-trained policy flaws such as language or exposure biases. We evaluate TrufLL on two visual questiongeneration tasks, for which we report positive results over performance and language metrics, which wethen corroborate with a human evaluation. To our knowledge, it is the first approach that successfullylearns a language generation policy (almost) from scratch.},
  archiveprefix = {arXiv},
  eprint        = {2109.09371},
  file          = {:Donati2021 - Learning Natural Language Generation from Scratch.pdf:PDF},
  groups        = {Language},
  keywords      = {cs.AI, cs.CL, cs.NE, stat.ML},
  primaryclass  = {cs.AI},
}

@InProceedings{Uchendu2022_Jump,
  author    = {Uchendu, Ikechukwu and Xiao, Ted and Lu, Yao and Zhu, Banghua and Yan, Mengyuan and Simon, Jos\'{e}phine and Bennice, Matthew and Fu, Chuyuan and Ma, Cong and Jiao, Jiantao and Levine, Sergey and Hausman, Karol},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {Jump-Start Reinforcement Learning},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {34556--34583},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent’s behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks that present exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that it is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.},
  file      = {:Uchendu2022 - Jump Start Reinforcement Learning.pdf:PDF},
  groups    = {Curriculum Learning},
  pdf       = {https://proceedings.mlr.press/v202/uchendu23a/uchendu23a.pdf},
  ranking   = {rank2},
  url       = {https://proceedings.mlr.press/v202/uchendu23a.html},
}

@InProceedings{Ammanabrolu2020Graph,
  author    = {Prithviraj Ammanabrolu and Matthew Hausknecht},
  booktitle = {International Conference on Learning Representations},
  title     = {Graph Constrained Reinforcement Learning for Natural Language Action Spaces},
  year      = {2020},
  file      = {:Ammanabrolu2020Graph - Graph Constrained Reinforcement Learning for Natural Language Action Spaces.pdf:PDF},
  groups    = {Text-based environments},
  url       = {https://openreview.net/forum?id=B1x6w0EtwH},
}

@InProceedings{NEURIPS2020_1fc30b9d,
  author    = {Adhikari, Ashutosh and Yuan, Xingdi and C\^{o}t\'{e}, Marc-Alexandre and Zelinka, Mikul\'{a}\v{s} and Rondeau, Marc-Antoine and Laroche, Romain and Poupart, Pascal and Tang, Jian and Trischler, Adam and Hamilton, Will},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning Dynamic Belief Graphs to Generalize on Text-Based Games},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {3045--3057},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  file      = {:NEURIPS2020_1fc30b9d - Learning Dynamic Belief Graphs to Generalize on Text Based Games.pdf:PDF},
  groups    = {Text-based environments},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1fc30b9d4319760b04fab735fbfed9a9-Paper.pdf},
}

@InProceedings{Yao2020,
  author    = {Shunyu Yao and Rohan Rao and Matthew Hausknecht and Karthik Narasimhan},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  title     = {Keep CALM and explore: Language models for action generation in text-based games},
  year      = {2020},
  note      = {Funding Information: Gracious thanks to Jacqeline Ashwell for running ClubFloyd and agreeing to our use of the collected transcripts. We thank Danqi Chen, Jimmy Yang, Jens Tuyls, and other colleagues from Princeton NLP group for proofreading and discussion. We also thank reviewers for constructive feedbacks. This research was partially funded by the Center for Statistics and Machine Learning at Princeton University through support from Microsoft. Publisher Copyright: {\textcopyright} 2020 Association for Computational Linguistics.; 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020 ; Conference date: 16-11-2020 Through 20-11-2020},
  pages     = {8736--8754},
  publisher = {Association for Computational Linguistics (ACL)},
  series    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  abstract  = {Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark (Hausknecht et al., 2019a), on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions.},
  file      = {:Yao2020 - Keep CALM and Explore_ Language Models for Action Generation in Text Based Games.pdf:PDF},
  groups    = {Text-based environments},
  language  = {English (US)},
}

@InProceedings{Lu2020,
  author     = {Lu, Yuchen and Singhal, Soumye and Strub, Florian and Courville, Aaron and Pietquin, Olivier},
  booktitle  = {Proceedings of the 37th International Conference on Machine Learning},
  title      = {Countering Language Drift with Seeded Iterated Learning},
  year       = {2020},
  editor     = {III, Hal Daumé and Singh, Aarti},
  month      = {13--18 Jul},
  pages      = {6437--6447},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {119},
  abstract   = {Pretraining on human corpus and then finetuning in a simulator has become a standard pipeline for training a goal-oriented dialogue agent. Nevertheless, as soon as the agents are finetuned to maximize task completion, they suffer from the so-called language drift phenomenon: they slowly lose syntactic and semantic properties of language as they only focus on solving the task. In this paper, we propose a generic approach to counter language drift called Seeded iterated learning (SIL). We periodically refine a pretrained student agent by imitating data sampled from a newly generated teacher agent. At each time step, the teacher is created by copying the student agent, before being finetuned to maximize task completion. SIL does not require external syntactic constraint nor semantic knowledge, making it a valuable task-agnostic finetuning protocol. We evaluate SIL in a toy-setting Lewis Game, and then scale it up to the translation game with natural language. In both settings, SIL helps counter language drift as well as it improves the task completion compared to baselines.},
  file       = {:Lu2020 - Countering Language Drift with Seeded Iterated Learning.pdf:PDF},
  groups     = {Dialogue agents, Natural language},
  pdf        = {http://proceedings.mlr.press/v119/lu20c/lu20c.pdf},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v119/lu20c.html},
}

@InProceedings{hanjie21a,
  author    = {Hanjie, Austin W. and Zhong, Victor Y and Narasimhan, Karthik},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {4051--4062},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {We investigate the use of natural language to drive the generalization of control policies and introduce the new multi-task environment Messenger with free-form text manuals describing the environment dynamics. Unlike previous work, Messenger does not assume prior knowledge connecting text and state observations {—} the control policy must simultaneously ground the game manual to entity symbols and dynamics in the environment. We develop a new model, EMMA (Entity Mapper with Multi-modal Attention) which uses an entity-conditioned attention module that allows for selective focus over relevant descriptions in the manual for each entity in the environment. EMMA is end-to-end differentiable and learns a latent grounding of entities and dynamics from text to observations using only environment rewards. EMMA achieves successful zero-shot generalization to unseen games with new dynamics, obtaining a 40% higher win rate compared to multiple baselines. However, win rate on the hardest stage of Messenger remains low (10%), demonstrating the need for additional work in this direction.},
  file      = {:hanjie21a - Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning.pdf:PDF},
  groups    = {Language-Augmented RL},
  pdf       = {http://proceedings.mlr.press/v139/hanjie21a/hanjie21a.pdf},
  priority  = {prio1},
  url       = {https://proceedings.mlr.press/v139/hanjie21a.html},
}

@InProceedings{NEURIPS2021_2b38c2df,
  author    = {Raileanu, Roberta and Goldstein, Maxwell and Yarats, Denis and Kostrikov, Ilya and Fergus, Rob},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Automatic Data Augmentation for Generalization in Reinforcement Learning},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {5402--5415},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:NEURIPS2021_2b38c2df - Automatic Data Augmentation for Generalization in Reinforcement Learning.pdf:PDF},
  groups    = {RL},
  url       = {https://proceedings.neurips.cc/paper/2021/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf},
}

@Article{Tam2022,
  author        = {Allison C. Tam and Neil C. Rabinowitz and Andrew K. Lampinen and Nicholas A. Roy and Stephanie C. Y. Chan and DJ Strouse and Jane X. Wang and Andrea Banino and Felix Hill},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {Semantic Exploration from Language Abstractions and Pretrained Representations},
  year          = {2022},
  month         = apr,
  abstract      = {Continuous first-person 3D environments pose unique exploration challenges to reinforcement learning (RL) agents because of their high-dimensional state and action spaces. These challenges can be ameliorated by using semantically meaningful state abstractions to define novelty for exploration. We propose that learned representations shaped by natural language provide exactly this form of abstraction. In particular, we show that vision-language representations, when pretrained on image captioning datasets sampled from the internet, can drive meaningful, task-relevant exploration and improve performance on 3D simulated environments. We also characterize why and how language provides useful abstractions for exploration by comparing the impacts of using representations from a pretrained model, a language oracle, and several ablations. We demonstrate the benefits of our approach in two very different task domains -- one that stresses the identification and manipulation of everyday objects, and one that requires navigational exploration in an expansive world -- as well as two popular deep RL algorithms: Impala and R2D2. Our results suggest that using language-shaped representations could improve exploration for various algorithms and agents in challenging environments.},
  archiveprefix = {arXiv},
  eprint        = {2204.05080},
  file          = {:Tam2022 - Semantic Exploration from Language Abstractions and Pretrained Representations.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}





@Article{doi_10.1111/j.1467-9280.2007.02028.x,
  author   = {Gary Lupyan and David H. Rakison and James L. McClelland},
  journal  = {Psychological Science},
  title    = {Language is not Just for Talking: Redundant Labels Facilitate Learning of Novel Categories},
  year     = {2007},
  note     = {PMID: 18031415},
  number   = {12},
  pages    = {1077-1083},
  volume   = {18},
  abstract = {In addition to having communicative functions, verbal labels may play a role in shaping concepts. Two experiments assessed whether the presence of labels affected category formation. Subjects learned to categorize “aliens” as those to be approached or those to be avoided. After accuracy feedback on each response was provided, a nonsense label was either presented or not. Providing nonsense category labels facilitated category learning even though the labels were redundant and all subjects had equivalent experience with supervised categorization of the stimuli. A follow-up study investigated differences between learning verbal and nonverbal associations and showed that learning a nonverbal association did not facilitate categorization. The findings show that labels make category distinctions more concrete and bear directly on the language-and-thought debate.},
  doi      = {10.1111/j.1467-9280.2007.02028.x},
  eprint   = {https://doi.org/10.1111/j.1467-9280.2007.02028.x},
  file     = {:doi_10.1111_j.1467-9280.2007.02028.x - Language Is Not Just for Talking_ Redundant Labels Facilitate Learning of Novel Categories.pdf:PDF},
  groups   = {Language},
  ranking  = {rank3},
  url      = {https://doi.org/10.1111/j.1467-9280.2007.02028.x},
}

@InCollection{LUPYAN2012255,
  author     = {Gary Lupyan},
  booktitle  = {The Psychology of Learning and Motivation},
  publisher  = {Academic Press},
  title      = {Chapter Seven - What Do Words Do? Toward a Theory of Language-Augmented Thought},
  year       = {2012},
  editor     = {Brian H. Ross},
  pages      = {255-297},
  series     = {Psychology of Learning and Motivation},
  volume     = {57},
  abstract   = {Much of human communication involves language—a system of communication qualitatively different from those used by other animals. In this chapter, I focus on a fundamental property of language: referring to objects with labels (e.g., using the word “chair” to refer to a chair). What consequences does such labeling have on cognitive and perceptual processes? I review evidence indicating that verbal labels do not simply point or refer to nonlinguistic concepts, but rather actively modulate object representations that are brought on-line during “nonverbal” tasks. Using words to refer to concrete objects affects the learning of new categories, memory for and reasoning about familiar object categories, and even basic visual processing. Object representations activated by verbal means appear to be different, and specifically, more categorical, than ostensibly the same object representations activated by nonverbal means. A connectionist model of “language augmented thought” provides a computational account of how labels may augment cognitive and perceptual processing.},
  doi        = {https://doi.org/10.1016/B978-0-12-394293-7.00007-8},
  file       = {:LUPYAN2012255 - Chapter Seven What Do Words Do_ toward a Theory of Language Augmented Thought.pdf:PDF},
  groups     = {Language},
  issn       = {0079-7421},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://www.sciencedirect.com/science/article/pii/B9780123942937000078},
}

@InBook{LogicandConversation,
  author     = {H. P. Grice},
  pages      = {41 - 58},
  publisher  = {Brill},
  title      = {Logic and Conversation},
  year       = {1975},
  address    = {Leiden, The Netherlands},
  isbn       = {9789004368811},
  doi        = {https://doi.org/10.1163/9789004368811_003},
  file       = {:LogicandConversation - Logic and Conversation.pdf:PDF},
  groups     = {Language},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://brill.com/view/book/edcoll/9789004368811/BP000003.xml},
}

@Article{Schwartz2019,
  author        = {Erez Schwartz and Guy Tennenholtz and Chen Tessler and Shie Mannor},
  title         = {Language is Power: Representing States Using Natural Language in Reinforcement Learning},
  year          = {2019},
  month         = oct,
  abstract      = {Recent advances in reinforcement learning have shown its potential to tackle complex real-life tasks. However, as the dimensionality of the task increases, reinforcement learning methods tend to struggle. To overcome this, we explore methods for representing the semantic information embedded in the state. While previous methods focused on information in its raw form (e.g., raw visual input), we propose to represent the state using natural language. Language can represent complex scenarios and concepts, making it a favorable candidate for representation. Empirical evidence, within the domain of ViZDoom, suggests that natural language based agents are more robust, converge faster and perform better than vision based agents, showing the benefit of using natural language representations for reinforcement learning.},
  archiveprefix = {arXiv},
  eprint        = {1910.02789},
  file          = {:Schwartz2019 - Language Is Power_ Representing States Using Natural Language in Reinforcement Learning.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.CL, cs.AI, cs.LG},
  primaryclass  = {cs.CL},
  readstatus    = {read},
}

@InProceedings{Badia2020_NGU,
  author     = {Adrià Puigdomènech Badia and Pablo Sprechmann and Alex Vitvitskyi and Daniel Guo and Bilal Piot and Steven Kapturowski and Olivier Tieleman and Martin Arjovsky and Alexander Pritzel and Andrew Bolt and Charles Blundell},
  booktitle  = {8th International Conference on Learning Representations, {ICLR} 2020},
  title      = {Never Give Up: Learning Directed Exploration Strategies},
  year       = {2020},
  file       = {:Badia2020Never - Never Give Up_ Learning Directed Exploration Strategies.pdf:PDF},
  groups     = {Curiosity},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=Sye57xStvB},
}

@InProceedings{Raileanu2020_RIDE,
  author     = {Roberta Raileanu and Tim Rocktäschel},
  booktitle  = {9th International Conference on Learning Representations},
  title      = {RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments},
  year       = {2020},
  file       = {:Raileanu2020RIDE_ - RIDE_ Rewarding Impact Driven Exploration for Procedurally Generated Environments.pdf:PDF},
  groups     = {Curiosity},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=rkg-TJBFPB},
}

@InProceedings{jacob-etal-2021-multitasking,
  author     = {Jacob, Athul Paul and Lewis, Mike and Andreas, Jacob},
  booktitle  = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title      = {Multitasking Inhibits Semantic Drift},
  year       = {2021},
  address    = {Online},
  month      = jun,
  pages      = {5351--5366},
  publisher  = {Association for Computational Linguistics},
  abstract   = {When intelligent agents communicate to accomplish shared goals, how do these goals shape the agents{'} language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to low-level actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying task-oriented language use. But previous work has found that LLP training is prone to semantic drift (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem: we prove that multitask training eliminates semantic drift in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency.},
  doi        = {10.18653/v1/2021.naacl-main.421},
  file       = {:jacob-etal-2021-multitasking - Multitasking Inhibits Semantic Drift.pdf:PDF},
  groups     = {Language-Augmented RL},
  readstatus = {skimmed},
  url        = {https://aclanthology.org/2021.naacl-main.421},
}

@InProceedings{Lee2019_CounteringLDrift,
  author     = {Lee, Jason and Cho, Kyunghyun and Kiela, Douwe},
  booktitle  = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title      = {Countering Language Drift via Visual Grounding},
  year       = {2019},
  address    = {Hong Kong, China},
  month      = nov,
  pages      = {4385--4395},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.},
  doi        = {10.18653/v1/D19-1447},
  file       = {:lee-etal-2019-countering - Countering Language Drift Via Visual Grounding.pdf:PDF},
  groups     = {Language-Augmented RL, Natural language},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://aclanthology.org/D19-1447},
}

@Article{Wen2022,
  author        = {Muning Wen and Jakub Grudzien Kuba and Runji Lin and Weinan Zhang and Ying Wen and Jun Wang and Yaodong Yang},
  title         = {Multi-Agent Reinforcement Learning is a Sequence Modeling Problem},
  year          = {2022},
  month         = may,
  abstract      = {Large sequence model (SM) such as GPT series and BERT has displayed outstanding performance and generalization capabilities on vision, language, and recently reinforcement learning tasks. A natural follow-up question is how to abstract multi-agent decision making into an SM problem and benefit from the prosperous development of SMs. In this paper, we introduce a novel architecture named Multi-Agent Transformer (MAT) that effectively casts cooperative multi-agent reinforcement learning (MARL) into SM problems wherein the task is to map agents' observation sequence to agents' optimal action sequence. Our goal is to build the bridge between MARL and SMs so that the modeling power of modern sequence models can be unleashed for MARL. Central to our MAT is an encoder-decoder architecture which leverages the multi-agent advantage decomposition theorem to transform the joint policy search problem into a sequential decision making process; this renders only linear time complexity for multi-agent problems and, most importantly, endows MAT with monotonic performance improvement guarantee. Unlike prior arts such as Decision Transformer fit only pre-collected offline data, MAT is trained by online trials and errors from the environment in an on-policy fashion. To validate MAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo, Dexterous Hands Manipulation, and Google Research Football benchmarks. Results demonstrate that MAT achieves superior performance and data efficiency compared to strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that MAT is an excellent few-short learner on unseen tasks regardless of changes in the number of agents. See our project page at https://sites.google.com/view/multi-agent-transformer.},
  archiveprefix = {arXiv},
  eprint        = {2205.14953},
  file          = {:Wen2022 - Multi Agent Reinforcement Learning Is a Sequence Modeling Problem.pdf:PDF},
  groups        = {Transformers in RL, Centralised Training and Execution},
  keywords      = {cs.MA, cs.LG},
  primaryclass  = {cs.MA},
  readstatus    = {skimmed},
}

@Article{Henderson_Islam_Bachman_Pineau_Precup_Meger_2018,
  author       = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Deep Reinforcement Learning That Matters},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted. &lt;/p&gt;},
  doi          = {10.1609/aaai.v32i1.11694},
  file         = {:Henderson_Islam_Bachman_Pineau_Precup_Meger_2018 - Deep Reinforcement Learning That Matters.pdf:PDF},
  groups       = {RL, Implementation tricks in RL},
  ranking      = {rank4},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11694},
}

@Article{Khetarpal2020,
  author        = {Khimya Khetarpal and Matthew Riemer and Irina Rish and Doina Precup},
  title         = {Towards Continual Reinforcement Learning: A Review and Perspectives},
  year          = {2020},
  month         = dec,
  abstract      = {In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations and mathematically characterize the non-stationary dynamics of each setting. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.},
  archiveprefix = {arXiv},
  eprint        = {2012.13490},
  file          = {:http\://arxiv.org/pdf/2012.13490v1:PDF},
  groups        = {Continual RL},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@Article{Perez_Strub_deVries_Dumoulin_Courville_2018,
  author       = {Perez, Ethan and Strub, Florian and de Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {FiLM: Visual Reasoning with a General Conditioning Layer},
  year         = {2018},
  month        = {Apr.},
  number       = {1},
  volume       = {32},
  abstractnote = {&lt;p&gt; We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot. &lt;/p&gt;},
  doi          = {10.1609/aaai.v32i1.11671},
  file         = {:Perez_Strub_deVries_Dumoulin_Courville_2018 - FiLM_ Visual Reasoning with a General Conditioning Layer.pdf:PDF},
  groups       = {Question Answering, Visual-Language Learning},
  ranking      = {rank4},
  readstatus   = {skimmed},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11671},
}

@Article{Waytowich2019,
  author        = {Nicholas Waytowich and Sean L. Barton and Vernon Lawhern and Garrett Warnell},
  title         = {A Narration-based Reward Shaping Approach using Grounded Natural Language Commands},
  year          = {2019},
  month         = oct,
  abstract      = {While deep reinforcement learning techniques have led to agents that are successfully able to learn to perform a number of tasks that had been previously unlearnable, these techniques are still susceptible to the longstanding problem of reward sparsity. This is especially true for tasks such as training an agent to play StarCraft II, a real-time strategy game where reward is only given at the end of a game which is usually very long. While this problem can be addressed through reward shaping, such approaches typically require a human expert with specialized knowledge. Inspired by the vision of enabling reward shaping through the more-accessible paradigm of natural-language narration, we develop a technique that can provide the benefits of reward shaping using natural language commands. Our narration-guided RL agent projects sequences of natural-language commands into the same high-dimensional representation space as corresponding goal states. We show that we can get improved performance with our method compared to traditional reward-shaping approaches. Additionally, we demonstrate the ability of our method to generalize to unseen natural-language commands.},
  archiveprefix = {arXiv},
  eprint        = {1911.00497},
  file          = {:Waytowich2019 - A Narration Based Reward Shaping Approach Using Grounded Natural Language Commands.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.AI, cs.CL, cs.LG},
  primaryclass  = {cs.AI},
}

@InProceedings{andreas-2020-good,
  author    = {Andreas, Jacob},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title     = {Good-Enough Compositional Data Augmentation},
  year      = {2020},
  address   = {Online},
  month     = jul,
  pages     = {7556--7566},
  publisher = {Association for Computational Linguistics},
  abstract  = {We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87{\%} on diagnostic tasks from the SCAN dataset and 16{\%} on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1{\%} on small corpora in several languages.},
  doi       = {10.18653/v1/2020.acl-main.676},
  file      = {:andreas-2020-good - Good Enough Compositional Data Augmentation.pdf:PDF},
  groups    = {Language},
  ranking   = {rank3},
  url       = {https://aclanthology.org/2020.acl-main.676},
}

@InProceedings{qian2022evaluating,
  author    = {Qian, Peizhu and Unhelkar, Vaibhav},
  booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  title     = {Evaluating the Role of Interactivity on Improving Transparency in Autonomous Agents},
  year      = {2022},
  pages     = {1083--1091},
  file      = {:qian2022evaluating - Evaluating the Role of Interactivity on Improving Transparency in Autonomous Agents.pdf:PDF},
  groups    = {Interactivity},
}

@InProceedings{pmlr-v139-radford21a,
  author     = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle  = {Proceedings of the 38th International Conference on Machine Learning},
  title      = {Learning Transferable Visual Models From Natural Language Supervision},
  year       = {2021},
  editor     = {Meila, Marina and Zhang, Tong},
  month      = {18--24 Jul},
  pages      = {8748--8763},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {139},
  abstract   = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  comment    = {CLIP},
  file       = {:pmlr-v139-radford21a - Learning Transferable Visual Models from Natural Language Supervision.pdf:PDF},
  groups     = {Visual-Language Learning},
  pdf        = {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  ranking    = {rank5},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v139/radford21a.html},
}

@InProceedings{Desai_2021_CVPR,
  author    = {Desai, Karan and Johnson, Justin},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {VirTex: Learning Visual Representations From Textual Annotations},
  year      = {2021},
  month     = {June},
  pages     = {11162-11173},
  file      = {:Desai_2021_CVPR - VirTex_ Learning Visual Representations from Textual Annotations.pdf:PDF},
  groups    = {Visual-Language Learning},
  ranking   = {rank3},
  url       = {https://openaccess.thecvf.com/content/CVPR2021/html/Desai_VirTex_Learning_Visual_Representations_From_Textual_Annotations_CVPR_2021_paper.html},
}

@Article{Zhang2020a,
  author        = {Yuhao Zhang and Hang Jiang and Yasuhide Miura and Christopher D. Manning and Curtis P. Langlotz},
  title         = {Contrastive Learning of Medical Visual Representations from Paired Images and Text},
  year          = {2020},
  month         = oct,
  abstract      = {Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
  archiveprefix = {arXiv},
  comment       = {ConVIRT},
  eprint        = {2010.00747},
  file          = {:Zhang2020a - Contrastive Learning of Medical Visual Representations from Paired Images and Text.pdf:PDF},
  groups        = {Visual-Language Learning, Contrastive Learning},
  keywords      = {cs.CV, cs.CL, cs.LG},
  primaryclass  = {cs.CV},
  ranking       = {rank3},
}

@Article{Caccia2022,
  author        = {Massimo Caccia and Jonas Mueller and Taesup Kim and Laurent Charlin and Rasool Fakoor},
  title         = {Task-Agnostic Continual Reinforcement Learning: In Praise of a Simple Baseline},
  year          = {2022},
  month         = may,
  abstract      = {We study task-agnostic continual reinforcement learning (TACRL) in which standard RL challenges are compounded with partial observability stemming from task agnosticism, as well as additional difficulties of continual learning (CL), i.e., learning on a non-stationary sequence of tasks. Here we compare TACRL methods with their soft upper bounds prescribed by previous literature: multi-task learning (MTL) methods which do not have to deal with non-stationary data distributions, as well as task-aware methods, which are allowed to operate under full observability. We consider a previously unexplored and straightforward baseline for TACRL, replay-based recurrent RL (3RL), in which we augment an RL algorithm with recurrent mechanisms to address partial observability and experience replay mechanisms to address catastrophic forgetting in CL. Studying empirical performance in a sequence of RL tasks, we find surprising occurrences of 3RL matching and overcoming the MTL and task-aware soft upper bounds. We lay out hypotheses that could explain this inflection point of continual and task-agnostic learning research. Our hypotheses are empirically tested in continuous control tasks via a large-scale study of the popular multi-task and continual learning benchmark Meta-World. By analyzing different training statistics including gradient conflict, we find evidence that 3RL's outperformance stems from its ability to quickly infer how new tasks relate with the previous ones, enabling forward transfer.},
  archiveprefix = {arXiv},
  eprint        = {2205.14495},
  file          = {:Caccia2022 - Task Agnostic Continual Reinforcement Learning_ in Praise of a Simple Baseline.pdf:PDF},
  groups        = {Continual RL},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@InProceedings{Zhou_2022_CVPR,
  author    = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Conditional Prompt Learning for Vision-Language Models},
  year      = {2022},
  month     = {June},
  pages     = {16816-16825},
  file      = {:Zhou_2022_CVPR - Conditional Prompt Learning for Vision Language Models.pdf:PDF},
  groups    = {Visual-Language Learning},
  ranking   = {rank2},
  url       = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html},
}

@Article{Eysenbach2022,
  author        = {Benjamin Eysenbach and Tianjun Zhang and Ruslan Salakhutdinov and Sergey Levine},
  title         = {Contrastive Learning as Goal-Conditioned Reinforcement Learning},
  year          = {2022},
  month         = jun,
  abstract      = {In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.},
  archiveprefix = {arXiv},
  eprint        = {2206.07568},
  file          = {:Eysenbach2022 - Contrastive Learning As Goal Conditioned Reinforcement Learning.pdf:PDF},
  groups        = {Contrastive Learning, Goal-conditioned RL},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
}

@InProceedings{10.1007/978-3-030-58621-8_45,
  author    = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle = {Computer Vision -- ECCV 2020},
  title     = {Contrastive Multiview Coding},
  year      = {2020},
  address   = {Cham},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  pages     = {776--794},
  publisher = {Springer International Publishing},
  abstract  = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ``dog'' can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Code is available at: http://github.com/HobbitLong/CMC/.},
  file      = {:10.1007_978-3-030-58621-8_45 - Contrastive Multiview Coding.pdf:PDF},
  groups    = {Contrastive Learning},
  isbn      = {978-3-030-58621-8},
  ranking   = {rank5},
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-58621-8_45},
}

@InProceedings{pmlr-v119-laskin20a,
  author    = {Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {{CURL}: Contrastive Unsupervised Representations for Reinforcement Learning},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {5639--5650},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://www.github.com/MishaLaskin/curl.},
  file      = {:pmlr-v119-laskin20a - CURL_ Contrastive Unsupervised Representations for Reinforcement Learning.pdf:PDF},
  groups    = {Contrastive Learning},
  pdf       = {http://proceedings.mlr.press/v119/laskin20a/laskin20a.pdf},
  ranking   = {rank3},
  url       = {https://proceedings.mlr.press/v119/laskin20a.html},
}

@Article{Hafner2022_Hierarchical,
  author        = {Danijar Hafner and Kuang-Huei Lee and Ian Fischer and Pieter Abbeel},
  title         = {Deep Hierarchical Planning from Pixels},
  year          = {2022},
  month         = jun,
  abstract      = {Intelligent agents need to select long sequences of actions to solve complex tasks. While humans easily break down tasks into subgoals and reach them through millions of muscle commands, current artificial intelligence is limited to tasks with horizons of a few hundred decisions, despite large compute budgets. Research on hierarchical reinforcement learning aims to overcome this limitation but has proven to be challenging, current methods rely on manually specified goal spaces or subtasks, and no general solution exists. We introduce Director, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. The high-level policy maximizes task and exploration rewards by selecting latent goals and the low-level policy learns to achieve the goals. Despite operating in latent space, the decisions are interpretable because the world model can decode goals into images for visualization. Director outperforms exploration methods on tasks with sparse rewards, including 3D maze traversal with a quadruped robot from an egocentric camera and proprioception, without access to the global position or top-down view that was used by prior work. Director also learns successful behaviors across a wide range of environments, including visual control, Atari games, and DMLab levels.},
  archiveprefix = {arXiv},
  eprint        = {2206.04114},
  file          = {:Hafner2022 - Deep Hierarchical Planning from Pixels.pdf:PDF},
  groups        = {Hierarchical RL},
  keywords      = {cs.AI, cs.LG, cs.RO, stat.ML},
  primaryclass  = {cs.AI},
  ranking       = {rank1},
}

@Article{Wan2022,
  author        = {Lipeng Wan and Zeyang Liu and Xingyu Chen and Han Wang and Xuguang Lan},
  journal       = {Proceedings of the 39th International Conference on Machine Learning},
  title         = {Greedy-based Value Representation for Optimal Coordination in Multi-agent Reinforcement Learning},
  year          = {2022},
  month         = dec,
  abstract      = {Due to the representation limitation of the joint Q value function, multi-agent reinforcement learning methods with linear value decomposition (LVD) or monotonic value decomposition (MVD) suffer from relative overgeneralization. As a result, they can not ensure optimal consistency (i.e., the correspondence between individual greedy actions and the maximal true Q value). In this paper, we derive the expression of the joint Q value function of LVD and MVD. According to the expression, we draw a transition diagram, where each self-transition node (STN) is a possible convergence. To ensure optimal consistency, the optimal node is required to be the unique STN. Therefore, we propose the greedy-based value representation (GVR), which turns the optimal node into an STN via inferior target shaping and further eliminates the non-optimal STNs via superior experience replay. In addition, GVR achieves an adaptive trade-off between optimality and stability. Our method outperforms state-of-the-art baselines in experiments on various benchmarks. Theoretical proofs and empirical results on matrix games demonstrate that GVR ensures optimal consistency under sufficient exploration.},
  archiveprefix = {arXiv},
  eprint        = {2112.04454},
  file          = {:Wan2021 - Greedy Based Value Representation for Optimal Coordination in Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Multi-agent RL},
  keywords      = {cs.MA},
  primaryclass  = {cs.MA},
}

@Article{Li2022,
  author        = {Pengyi Li and Hongyao Tang and Tianpei Yang and Xiaotian Hao and Tong Sang and Yan Zheng and Jianye Hao and Matthew E. Taylor and Wenyuan Tao and Zhen Wang},
  journal       = {Proceedings of the 39th International Conference on Machine Learning},
  title         = {PMIC: Improving Multi-Agent Reinforcement Learning with Progressive Mutual Information Collaboration},
  year          = {2022},
  month         = mar,
  abstract      = {Learning to collaborate is critical in Multi-Agent Reinforcement Learning (MARL). Previous works promote collaboration by maximizing the correlation of agents' behaviors, which is typically characterized by Mutual Information (MI) in different forms. However, we reveal sub-optimal collaborative behaviors also emerge with strong correlations, and simply maximizing the MI can, surprisingly, hinder the learning towards better collaboration. To address this issue, we propose a novel MARL framework, called Progressive Mutual Information Collaboration (PMIC), for more effective MI-driven collaboration. PMIC uses a new collaboration criterion measured by the MI between global states and joint actions. Based on this criterion, the key idea of PMIC is maximizing the MI associated with superior collaborative behaviors and minimizing the MI associated with inferior ones. The two MI objectives play complementary roles by facilitating better collaborations while avoiding falling into sub-optimal ones. Experiments on a wide range of MARL benchmarks show the superior performance of PMIC compared with other algorithms.},
  archiveprefix = {arXiv},
  eprint        = {2203.08553},
  file          = {:Li2022 - PMIC_ Improving Multi Agent Reinforcement Learning with Progressive Mutual Information Collaboration.pdf:PDF},
  groups        = {Multi-agent RL, Intrinsic rewards in MARL},
  keywords      = {cs.MA, cs.AI},
  primaryclass  = {cs.MA},
  priority      = {prio2},
}

@InProceedings{AFK2022,
  author    = {Iou-Jen Liu and Xingdi Yuan and Marc-Alexandre C\^{o}t\'{e} and Pierre-Yves Oudeyer and Alexander G. Schwing},
  booktitle = {International Conference on Machine Learning},
  title     = {Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language},
  year      = {2022},
  file      = {:AFK2022 - Asking for Knowledge_ Training RL Agents to Query External Knowledge Using Language.pdf:PDF},
  groups    = {Language-Augmented RL},
}

@InProceedings{mahmoudieh22a,
  author    = {Mahmoudieh, Parsa and Pathak, Deepak and Darrell, Trevor},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  title     = {Zero-Shot Reward Specification via Grounded Natural Language},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  month     = {17--23 Jul},
  pages     = {14743--14752},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  abstract  = {Reward signals in reinforcement learning are expensive to design and often require access to the true state which is not available in the real world. Common alternatives are usually demonstrations or goal images which can be labor-intensive to collect. On the other hand, text descriptions provide a general, natural, and low-effort way of communicating the desired task. However, prior works in learning text-conditioned policies still rely on rewards that are defined using either true state or labeled expert demonstrations. We use recent developments in building large-scale visuolanguage models like CLIP to devise a framework that generates the task reward signal just from goal text description and raw pixel observations which is then used to learn the task policy. We evaluate the proposed framework on control and robotic manipulation tasks. Finally, we distill the individual task policies into a single goal text conditioned policy that can generalize in a zero-shot manner to new tasks with unseen objects and unseen goal text descriptions.},
  file      = {:mahmoudieh22a - Zero Shot Reward Specification Via Grounded Natural Language.pdf:PDF},
  groups    = {Language-Augmented RL},
  pdf       = {https://proceedings.mlr.press/v162/mahmoudieh22a/mahmoudieh22a.pdf},
  url       = {https://proceedings.mlr.press/v162/mahmoudieh22a.html},
}

@Article{Huang2022,
  author        = {Wenlong Huang and Pieter Abbeel and Deepak Pathak and Igor Mordatch},
  journal       = {International Conference on Machine Learning},
  title         = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  year          = {2022},
  month         = jan,
  abstract      = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
  archiveprefix = {arXiv},
  eprint        = {2201.07207},
  file          = {:Huang2022 - Language Models As Zero Shot Planners_ Extracting Actionable Knowledge for Embodied Agents.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.LG, cs.AI, cs.CL, cs.CV, cs.RO},
  primaryclass  = {cs.LG},
  ranking       = {rank3},
  readstatus    = {skimmed},
}

@Article{Carta2022,
  author        = {Thomas Carta and Sylvain Lamprier and Pierre-Yves Oudeyer and Olivier Sigaud},
  title         = {EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL},
  year          = {2022},
  month         = jun,
  abstract      = {Reinforcement learning (RL) in long horizon and sparse reward tasks is notoriously difficult and requires a lot of training steps. A standard solution to speed up the process is to leverage additional reward signals, shaping it to better guide the learning process. In the context of language-conditioned RL, the abstraction and generalisation properties of the language input provide opportunities for more efficient ways of shaping the reward. In this paper, we leverage this idea and propose an automated reward shaping method where the agent extracts auxiliary objectives from the general language goal. These auxiliary objectives use a question generation (QG) and question answering (QA) system: they consist of questions leading the agent to try to reconstruct partial information about the global goal using its own trajectory. When it succeeds, it receives an intrinsic reward proportional to its confidence in its answer. This incentivizes the agent to generate trajectories which unambiguously explain various aspects of the general language goal. Our experimental study shows that this approach, which does not require engineer intervention to design the auxiliary objectives, improves sample efficiency by effectively directing exploration.},
  archiveprefix = {arXiv},
  eprint        = {2206.09674},
  file          = {:Carta2022 - EAGER_ Asking and Answering Questions for Automatic Reward Shaping in Language Guided RL.pdf:PDF},
  groups        = {LA Reward shaping},
  keywords      = {cs.CL, cs.AI, cs.LG},
  primaryclass  = {cs.CL},
  priority      = {prio2},
}

@InProceedings{radulescu2022name,
  author    = {Radulescu, Angela and Vong, Wai Keen and Gureckis, Todd M},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  title     = {Name that state: How language affects human reinforcement learning},
  year      = {2022},
  number    = {44},
  volume    = {44},
  file      = {:radulescu2022name - Name That State_ How Language Affects Human Reinforcement Learning.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio3},
}

@Article{Micheli2022,
  author        = {Vincent Micheli and Eloi Alonso and François Fleuret},
  title         = {Transformers are Sample Efficient World Models},
  year          = {2022},
  month         = sep,
  abstract      = {Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games. Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our codebase at https://github.com/eloialonso/iris.},
  archiveprefix = {arXiv},
  eprint        = {2209.00588},
  file          = {:Micheli2022 - Transformers Are Sample Efficient World Models.pdf:PDF},
  groups        = {Transformers in RL},
  keywords      = {cs.LG, cs.AI, cs.CV},
  primaryclass  = {cs.LG},
}

@Article{Li2022a,
  author        = {Shuang Li and Xavier Puig and Chris Paxton and Yilun Du and Clinton Wang and Linxi Fan and Tao Chen and De-An Huang and Ekin Akyürek and Anima Anandkumar and Jacob Andreas and Igor Mordatch and Antonio Torralba and Yuke Zhu},
  title         = {Pre-Trained Language Models for Interactive Decision-Making},
  year          = {2022},
  month         = feb,
  abstract      = {Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6% in the VirtualHome environment. We then examine how our framework may be used in environments without pre-collected expert data. To do this, we integrate an active data gathering procedure into pre-trained LMs. The agent iteratively learns by interacting with the environment, relabeling the language goal of past 'failed' experiences, and updating the policy in a self-supervised loop. The active data gathering procedure also enables effective combinatorial generalization, outperforming the best baseline by 25.1%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and favorable weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans.},
  archiveprefix = {arXiv},
  eprint        = {2202.01771},
  file          = {:Li2022a - Pre Trained Language Models for Interactive Decision Making.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.LG, cs.CL},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hill2020a,
  author        = {Felix Hill and Sona Mokra and Nathaniel Wong and Tim Harley},
  title         = {Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text},
  year          = {2020},
  month         = may,
  abstract      = {Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.},
  archiveprefix = {arXiv},
  eprint        = {2005.09382},
  file          = {:Hill2020a - Human Instruction Following with Deep Reinforcement Learning Via Transfer Learning from Text.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
}

@InProceedings{10.1007/978-3-030-58539-6_16,
  author    = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
  booktitle = {Computer Vision -- ECCV 2020},
  title     = {Improving Vision-and-Language Navigation with Image-Text Pairs from the Web},
  year      = {2020},
  address   = {Cham},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  pages     = {259--274},
  publisher = {Springer International Publishing},
  abstract  = {Following a navigation instruction such as `Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground referenced scene elements referenced (e.g. `stairs') to visual content in the environment (pixels corresponding to `stairs'). We ask the following question -- can we leverage abundant `disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (`...stop at the brown sofa') and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN -- outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful -- with their combination resulting in further gains.},
  groups    = {Visual-Language Learning},
  isbn      = {978-3-030-58539-6},
  ranking   = {rank2},
}

@InProceedings{sharma-etal-2022-skill,
  author    = {Sharma, Pratyusha and Torralba, Antonio and Andreas, Jacob},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Skill Induction and Planning with Latent Language},
  year      = {2022},
  address   = {Dublin, Ireland},
  month     = may,
  pages     = {1713--1726},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present a framework for learning hierarchical policies from demonstrations, using sparse natural language annotations to guide the discovery of reusable skills for autonomous decision-making. We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions, and these descriptions generate sequences of low-level actions. We describe how to train this model using primarily unannotated demonstrations by parsing demonstrations into sequences of named high-level sub-tasks, using only a small number of seed annotations to ground language in action. In trained models, natural language commands index a combinatorial library of skills; agents can use these skills to plan by generating high-level instruction sequences tailored to novel goals. We evaluate this approach in the ALFRED household simulation environment, providing natural language annotations for only 10{\%} of demonstrations. It achieves performance comparable state-of-the-art models on ALFRED success rate, outperforming several recent methods with access to ground-truth plans during training and evaluation.},
  doi       = {10.18653/v1/2022.acl-long.120},
  file      = {:sharma-etal-2022-skill - Skill Induction and Planning with Latent Language.pdf:PDF},
  groups    = {Language-Augmented RL},
  url       = {https://aclanthology.org/2022.acl-long.120},
}

@InProceedings{zheng2021episodic,
  author     = {Lulu Zheng and Jiarui Chen and Jianhao Wang and Jiamin He and Yujing Hu and Yingfeng Chen and Changjie Fan and Yang Gao and Chongjie Zhang},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Episodic Multi-agent Reinforcement Learning with Curiosity-driven Exploration},
  year       = {2021},
  editor     = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  file       = {:zheng2021episodic - Episodic Multi Agent Reinforcement Learning with Curiosity Driven Exploration.pdf:PDF},
  groups     = {Intrinsic rewards in MARL},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=YDGJ5YExiw6},
}

@Article{Iqbal2019_MultiExplore,
  author        = {Shariq Iqbal and Fei Sha},
  title         = {Coordinated Exploration via Intrinsic Rewards for Multi-Agent Reinforcement Learning},
  year          = {2019},
  month         = may,
  abstract      = {Solving tasks with sparse rewards is one of the most important challenges in reinforcement learning. In the single-agent setting, this challenge is addressed by introducing intrinsic rewards that motivate agents to explore unseen regions of their state spaces; however, applying these techniques naively to the multi-agent setting results in agents exploring independently, without any coordination among themselves. Exploration in cooperative multi-agent settings can be accelerated and improved if agents coordinate their exploration. In this paper we introduce a framework for designing intrinsic rewards which consider what other agents have explored such that the agents can coordinate. Then, we develop an approach for learning how to dynamically select between several exploration modalities to maximize extrinsic rewards. Concretely, we formulate the approach as a hierarchical policy where a high-level controller selects among sets of policies trained on diverse intrinsic rewards and the low-level controllers learn the action policies of all agents under these specific rewards. We demonstrate the effectiveness of the proposed approach in cooperative domains with sparse rewards where state-of-the-art methods fail and challenging multi-stage tasks that necessitate changing modes of coordination.},
  archiveprefix = {arXiv},
  comment       = {MASAC
Multi-Explore},
  eprint        = {1905.12127},
  file          = {:Iqbal2019a - Coordinated Exploration Via Intrinsic Rewards for Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Intrinsic rewards in MARL},
  keywords      = {cs.LG, cs.AI, cs.MA, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
  url           = {https://openreview.net/forum?id=rkltE0VKwH},
}

@InProceedings{mguni2022ligs,
  author     = {David Henry Mguni and Taher Jafferjee and Jianhong Wang and Nicolas Perez-Nieves and Oliver Slumbers and Feifei Tong and Yang Li and Jiangcheng Zhu and Yaodong Yang and Jun Wang},
  booktitle  = {International Conference on Learning Representations},
  title      = {{LIGS}: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning},
  year       = {2022},
  file       = {:mguni2022ligs - LIGS_ Learnable Intrinsic Reward Generation Selection for Multi Agent Learning.pdf:PDF},
  groups     = {Intrinsic rewards in MARL},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=CpTuR2ECuW},
}





@Article{doi_10.1177/17298814211044946,
  author   = {Haolin Wu and Hui Li and Jianwei Zhang and Zhuang Wang and Jianeng Zhang},
  journal  = {International Journal of Advanced Robotic Systems},
  title    = {Generating individual intrinsic reward for cooperative multiagent reinforcement learning},
  year     = {2021},
  number   = {5},
  pages    = {17298814211044946},
  volume   = {18},
  abstract = {Multiagent reinforcement learning holds considerable promise to deal with cooperative multiagent tasks. Unfortunately, the only global reward shared by all agents in the cooperative tasks may lead to the lazy agent problem. To cope with such a problem, we propose a generating individual intrinsic reward algorithm, which introduces an intrinsic reward encoder to generate an individual intrinsic reward for each agent and utilizes the hypernetworks as the decoder to help to estimate the individual action values of the decomposition methods based on the generated individual intrinsic reward. Experimental results in the StarCraft II micromanagement benchmark prove that the proposed algorithm can increase learning efficiency and improve policy performance.},
  doi      = {10.1177/17298814211044946},
  eprint   = {https://doi.org/10.1177/17298814211044946},
  file     = {:Wu2021.pdf:PDF},
  groups   = {Intrinsic rewards in MARL},
  url      = {https://doi.org/10.1177/17298814211044946},
}

@Article{MIROLLI2011298,
  author   = {Marco Mirolli and Domenico Parisi},
  journal  = {New Ideas in Psychology},
  title    = {Towards a Vygotskyan cognitive robotics: The role of language as a cognitive tool},
  year     = {2011},
  issn     = {0732-118X},
  note     = {Special Issue: Cognitive Robotics and Reevaluation of Piaget Concept of Egocentrism},
  number   = {3},
  pages    = {298-311},
  volume   = {29},
  abstract = {Cognitive Robotics can be defined as the study of cognitive phenomena by their modeling in physical artifacts such as robots. This is a very lively and fascinating field which has already given fundamental contributions to our understanding of natural cognition. Nonetheless, robotics has to date addressed mainly very basic, low-level cognitive phenomena like sensory-motor coordination, perception, and navigation, and it is not clear how the current approach might scale up to explain high-level human cognition. In this paper we argue that a promising way to do that is to merge current ideas and methods of ‘embodied cognition’ with the Russian tradition of theoretical psychology which views language not only as a communication system but also as a cognitive tool, that is by developing a Vygotskyan cognitive robotics. We substantiate this idea by discussing several domains in which language can improve basic cognitive abilities and permit the development of high-level cognition: learning, categorization, abstraction, memory, voluntary control, and mental life.},
  doi      = {https://doi.org/10.1016/j.newideapsych.2009.07.001},
  file     = {:MIROLLI2011298 - Towards a Vygotskyan Cognitive Robotics_ the Role of Language As a Cognitive Tool.pdf:PDF},
  groups   = {Language-Augmented RL},
  keywords = {Robotics, Cognition, Language, Vygotsky, Categorization, Learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0732118X09000348},
}

@InProceedings{chen2021ask,
  author    = {Valerie Chen and Abhinav Gupta and Kenneth Marino},
  booktitle = {International Conference on Learning Representations},
  title     = {Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning},
  year      = {2021},
  file      = {:chen2021ask - Ask Your Humans_ Using Human Instructions to Improve Generalization in Reinforcement Learning.pdf:PDF},
  groups    = {Language-Augmented RL},
  url       = {https://openreview.net/forum?id=Y87Ri-GNHYu},
}

@InProceedings{Wang2020_EITI,
  author     = {Tonghan Wang and Jianhao Wang and Yi Wu and Chongjie Zhang},
  booktitle  = {8th International Conference on Learning Representations},
  title      = {Influence-Based Multi-Agent Exploration},
  year       = {2020},
  file       = {:Wang_2020Influence-Based - Influence Based Multi Agent Exploration.pdf:PDF},
  groups     = {Intrinsic rewards in MARL},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=BJgy96EYvr},
}

@InProceedings{NEURIPS2021_f3f1fa1e,
  author    = {Wang, Jianhao and Ren, Zhizhou and Han, Beining and Ye, Jianing and Zhang, Chongjie},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Towards Understanding Cooperative Multi-Agent Q-Learning with Value Factorization},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {29142--29155},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:NEURIPS2021_f3f1fa1e - Towards Understanding Cooperative Multi Agent Q Learning with Value Factorization.pdf:PDF},
  groups    = {Value factorisation},
  priority  = {prio1},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper/2021/file/f3f1fa1e4348bfbebdeee8c80a04c3b9-Paper.pdf},
}

@InProceedings{seurin_hal-03259315,
  author      = {Seurin, Mathieu and Strub, Florian and Preux, Philippe and Pietquin, Olivier},
  booktitle   = {{Internationnal Joint Conference on Artificial Intelligence (IJCAI)}},
  title       = {{Don't Do What Doesn't Matter: Intrinsic Motivation with Action Usefulness}},
  year        = {2021},
  address     = {Montreal, Canada},
  month       = Aug,
  pages       = {2950--2956},
  file        = {:seurin_hal-03259315 - Don't Do What Doesn't Matter_ Intrinsic Motivation with Action Usefulness.pdf:PDF},
  groups      = {Intrinsic goals},
  hal_id      = {hal-03259315},
  hal_version = {v1},
  pdf         = {https://hal.archives-ouvertes.fr/hal-03259315/file/Rare_Actions_Matter_IJCAI.pdf},
  readstatus  = {read},
  url         = {https://hal.archives-ouvertes.fr/hal-03259315},
}

@InProceedings{ijcai2020p390,
  author     = {Simmons-Edler, Riley and Eisner, Ben and Yang, Daniel and Bisulco, Anthony and Mitchell, Eric and Seung, Sebastian and Lee, Daniel},
  booktitle  = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}},
  title      = {Reward Prediction Error as an Exploration Objective in Deep RL},
  year       = {2020},
  editor     = {Christian Bessiere},
  month      = {7},
  note       = {Main track},
  pages      = {2816--2823},
  publisher  = {International Joint Conferences on Artificial Intelligence Organization},
  doi        = {10.24963/ijcai.2020/390},
  file       = {:ijcai2020p390 - Reward Prediction Error As an Exploration Objective in Deep RL.pdf:PDF},
  groups     = {Intrinsic goals, Adversary Guidance},
  readstatus = {read},
  url        = {https://doi.org/10.24963/ijcai.2020/390},
}

@InProceedings{pmlr-v80-colas18a,
  author    = {Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {{GEP}-{PG}: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {1039--1048},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient-descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG . We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments.},
  file      = {:pmlr-v80-colas18a - GEP PG_ Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms.pdf:PDF},
  groups    = {Intrinsic goals},
  pdf       = {http://proceedings.mlr.press/v80/colas18a/colas18a.pdf},
  url       = {https://proceedings.mlr.press/v80/colas18a.html},
}

@Article{Steels2016,
  author    = {Luc Steels},
  journal   = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  title     = {Agent-based models for the emergence and evolution of grammar},
  year      = {2016},
  month     = {aug},
  number    = {1701},
  pages     = {20150447},
  volume    = {371},
  doi       = {10.1098/rstb.2015.0447},
  file      = {:Steels2016 - Agent Based Models for the Emergence and Evolution of Grammar.pdf:PDF},
  groups    = {Language},
  publisher = {The Royal Society},
}

@Article{Ohmer2022_MutualInfluence,
  author    = {Ohmer, Xenia AND Marino, Michael AND Franke, Michael AND König, Peter},
  journal   = {PLOS Computational Biology},
  title     = {Mutual influence between language and perception in multi-agent communication games},
  year      = {2022},
  month     = {10},
  number    = {10},
  pages     = {1-28},
  volume    = {18},
  abstract  = {Language interfaces with many other cognitive domains. This paper explores how interactions at these interfaces can be studied with deep learning methods, focusing on the relation between language emergence and visual perception. To model the emergence of language, a sender and a receiver agent are trained on a reference game. The agents are implemented as deep neural networks, with dedicated vision and language modules. Motivated by the mutual influence between language and perception in cognition, we apply systematic manipulations to the agents’ (i) visual representations, to analyze the effects on emergent communication, and (ii) communication protocols, to analyze the effects on visual representations. Our analyses show that perceptual biases shape semantic categorization and communicative content. Conversely, if the communication protocol partitions object space along certain attributes, agents learn to represent visual information about these attributes more accurately, and the representations of communication partners align. Finally, an evolutionary analysis suggests that visual representations may be shaped in part to facilitate the communication of environmentally relevant distinctions. Aside from accounting for co-adaptation effects between language and perception, our results point out ways to modulate and improve visual representation learning and emergent communication in artificial agents.},
  doi       = {10.1371/journal.pcbi.1010658},
  file      = {:journal.pcbi.1010658.pdf:PDF},
  groups    = {Discrete language},
  publisher = {Public Library of Science},
  url       = {https://doi.org/10.1371/journal.pcbi.1010658},
}

@InProceedings{Ohmer2022,
  author    = {Xenia Ohmer and Marko Duda and Elia Bruni},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  title     = {Emergence of hierarchical reference systems in multi-agent communication},
  year      = {2022},
  abstract  = {In natural language, referencing objects at different levels of specificity is a fundamental pragmatic mechanism for efficient communication in context. We develop a novel communication game, the hierarchical reference game, to study the emergence of such reference systems in artificial agents. We consider a simplified world, in which concepts are abstractions over a set of primitive attributes (e.g., color, style, shape). Depending on how many attributes are combined, concepts are more general ("circle") or more specific ("red dotted circle"). Based on the context, the agents have to communicate at different levels of this hierarchy. Our results show that the agents learn to play the game successfully and can even generalize to novel concepts. To achieve abstraction, they use implicit (omitting irrelevant information) and explicit (indicating that attributes are irrelevant) strategies. In addition, the compositional structure underlying the concept hierarchy is reflected in the emergent protocols, indicating that the need to develop hierarchical reference systems supports the emergence of compositionality.},
  file      = {:Ohmer2022 - Emergence of Hierarchical Reference Systems in Multi Agent Communication.pdf:PDF},
  groups    = {Discrete language},
  url       = {https://aclanthology.org/2022.coling-1.501/},
}

@Article{BARD2020103216,
  author   = {Nolan Bard and Jakob N. Foerster and Sarath Chandar and Neil Burch and Marc Lanctot and H. Francis Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and Hugo Larochelle and Marc G. Bellemare and Michael Bowling},
  journal  = {Artificial Intelligence},
  title    = {The Hanabi challenge: A new frontier for AI research},
  year     = {2020},
  issn     = {0004-3702},
  pages    = {103216},
  volume   = {280},
  abstract = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.},
  doi      = {https://doi.org/10.1016/j.artint.2019.103216},
  file     = {:1-s2.0-S0004370219300116-main.pdf:PDF},
  groups   = {Multi-Agent Environments},
  keywords = {Multi-agent learning, Challenge paper, Reinforcement learning, Games, Theory of mind, Communication, Imperfect information, Cooperative},
  ranking  = {rank4},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370219300116},
}

@InProceedings{Bellemare2016_PseudoCounts,
  author     = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle  = {Advances in Neural Information Processing Systems 29 (NIPS 2016)},
  title      = {Unifying Count-Based Exploration and Intrinsic Motivation},
  year       = {2016},
  editor     = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {29},
  file       = {:NIPS2016_afda3322 - Unifying Count Based Exploration and Intrinsic Motivation.pdf:PDF},
  groups     = {Visitation count},
  ranking    = {rank4},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper/2016/file/afda332245e2af431fb7b672a68b659d-Paper.pdf},
}

@InProceedings{Henaff2022_E3B,
  author     = {Mikael Henaff and Roberta Raileanu and Minqi Jiang and Tim Rockt{\"a}schel},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Exploration via Elliptical Episodic Bonuses},
  year       = {2022},
  editor     = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  pages      = {37631-37646},
  volume     = {35},
  file       = {:Henaff2022_E3B - Exploration Via Elliptical Episodic Bonuses.pdf:PDF;:appendix.pdf:PDF},
  groups     = {Curiosity},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=Xg-yZos9qJQ},
}

@Article{Andres2022,
  author        = {Alain Andres and Esther Villar-Rodriguez and Javier Del Ser},
  title         = {An Evaluation Study of Intrinsic Motivation Techniques applied to Reinforcement Learning over Hard Exploration Environments},
  year          = {2022},
  month         = may,
  abstract      = {In the last few years, the research activity around reinforcement learning tasks formulated over environments with sparse rewards has been especially notable. Among the numerous approaches proposed to deal with these hard exploration problems, intrinsic motivation mechanisms are arguably among the most studied alternatives to date. Advances reported in this area over time have tackled the exploration issue by proposing new algorithmic ideas to generate alternative mechanisms to measure the novelty. However, most efforts in this direction have overlooked the influence of different design choices and parameter settings that have also been introduced to improve the effect of the generated intrinsic bonus, forgetting the application of those choices to other intrinsic motivation techniques that may also benefit of them. Furthermore, some of those intrinsic methods are applied with different base reinforcement algorithms (e.g. PPO, IMPALA) and neural network architectures, being hard to fairly compare the provided results and the actual progress provided by each solution. The goal of this work is to stress on this crucial matter in reinforcement learning over hard exploration environments, exposing the variability and susceptibility of avant-garde intrinsic motivation techniques to diverse design factors. Ultimately, our experiments herein reported underscore the importance of a careful selection of these design aspects coupled with the exploration requirements of the environment and the task in question under the same setup, so that fair comparisons can be guaranteed.},
  archiveprefix = {arXiv},
  eprint        = {2205.11184},
  file          = {:Andres2022 - An Evaluation Study of Intrinsic Motivation Techniques Applied to Reinforcement Learning Over Hard Exploration Environments (1).pdf:PDF},
  groups        = {Intrinsic goals},
  keywords      = {cs.LG, cs.AI},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Bougie2020,
  author    = {Nicolas Bougie and Ryutaro Ichise},
  journal   = {Applied Intelligence},
  title     = {Fast and slow curiosity for high-level exploration in reinforcement learning},
  year      = {2020},
  month     = {sep},
  number    = {2},
  pages     = {1086--1107},
  volume    = {51},
  doi       = {10.1007/s10489-020-01849-3},
  file      = {:s10489-020-01849-3.pdf:PDF},
  groups    = {Curiosity},
  publisher = {Springer Science and Business Media {LLC}},
  ranking   = {rank1},
}

@InProceedings{Taiga2020On,
  author    = {Adrien Ali Taiga and William Fedus and Marlos C. Machado and Aaron Courville and Marc G. Bellemare},
  booktitle = {International Conference on Learning Representations},
  title     = {On Bonus Based Exploration Methods In The Arcade Learning Environment},
  year      = {2020},
  file      = {:Taiga2020On - On Bonus Based Exploration Methods in the Arcade Learning Environment.pdf:PDF},
  groups    = {Intrinsic goals},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=BJewlyStDr},
}

@Misc{jing2022divide,
  author     = {Xiao Jing and Zhenwei Zhu and Hongliang Li and Xin Pei and Yoshua Bengio and Tong Che and Hongyong Song},
  title      = {Divide and Explore: Multi-Agent Separate Exploration with Shared Intrinsic Motivations},
  year       = {2022},
  file       = {:jing2022divide - Divide and Explore_ Multi Agent Separate Exploration with Shared Intrinsic Motivations.pdf:PDF},
  groups     = {Intrinsic goals},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=NgmcJ66xQz_},
}

@InProceedings{pislar2022when,
  author    = {Miruna Pislar and David Szepesvari and Georg Ostrovski and Diana L Borsa and Tom Schaul},
  booktitle = {International Conference on Learning Representations},
  title     = {When should agents explore?},
  year      = {2022},
  file      = {:pislar2022when - When Should Agents Explore_.pdf:PDF},
  groups    = {Random Exploration, Exploration},
  priority  = {prio3},
  ranking   = {rank3},
  url       = {https://openreview.net/forum?id=dEwfxt14bca},
}

@InProceedings{zha2021rank,
  author    = {Daochen Zha and Wenye Ma and Lei Yuan and Xia Hu and Ji Liu},
  booktitle = {International Conference on Learning Representations},
  title     = {Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments},
  year      = {2021},
  file      = {:zha2021rank - Rank the Episodes_ a Simple Approach for Exploration in Procedurally Generated Environments.pdf:PDF},
  groups    = {Exploration},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=MtEE0CktZht},
}

@InProceedings{pmlr-v80-espeholt18a,
  author    = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {{IMPALA}: Scalable Distributed Deep-{RL} with Importance Weighted Actor-Learner Architectures},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {1407--1416},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  file      = {:pmlr-v80-espeholt18a - IMPALA_ Scalable Distributed Deep RL with Importance Weighted Actor Learner Architectures.pdf:PDF},
  groups    = {Policy based},
  pdf       = {http://proceedings.mlr.press/v80/espeholt18a/espeholt18a.pdf},
  ranking   = {rank4},
  url       = {https://proceedings.mlr.press/v80/espeholt18a.html},
}

@InProceedings{pmlr-v97-cobbe19a,
  author    = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {Quantifying Generalization in Reinforcement Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month     = {09--15 Jun},
  pages     = {1282--1289},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent’s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
  file      = {:pmlr-v97-cobbe19a - Quantifying Generalization in Reinforcement Learning.pdf:PDF},
  groups    = {RL},
  pdf       = {http://proceedings.mlr.press/v97/cobbe19a/cobbe19a.pdf},
  ranking   = {rank3},
  url       = {https://proceedings.mlr.press/v97/cobbe19a.html},
}

@Article{osti_10132613,
  author       = {Justesen, Niels and Rodriguez Torrado, Ruben and Bontrager, Philip and Khalifa, Ahmed and Togelius, Julian and Risi, Sebastian},
  journal      = {NeurIPS Workshop on Deep Reinforcement Learning},
  title        = {Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation},
  year         = {2018},
  abstractnote = {Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators’ distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.},
  file         = {:osti_10132613 - Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation.pdf:PDF},
  groups       = {RL},
  place        = {Country unknown/Code not available},
  ranking      = {rank2},
  url          = {https://par.nsf.gov/biblio/10132613},
}

@InProceedings{Ma2022_ELIGN,
  author     = {Zixian Ma and Rose E Wang and Li Fei-Fei and Michael S. Bernstein and Ranjay Krishna},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {{ELIGN}: Expectation Alignment as a Multi-Agent Intrinsic Reward},
  year       = {2022},
  editor     = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  pages      = {8304-8317},
  volume     = {35},
  file       = {:ma2022elign - ELIGN_ Expectation Alignment As a Multi Agent Intrinsic Reward.pdf:PDF},
  groups     = {Intrinsic rewards in MARL},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=uPyNR2yPoe},
}

@InProceedings{chen2021randomized,
  author    = {Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},
  booktitle = {International Conference on Learning Representations},
  title     = {Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},
  year      = {2021},
  file      = {:chen2021randomized - Randomized Ensembled Double Q Learning_ Learning Fast without a Model.pdf:PDF},
  groups    = {Value based},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=AY8zfZm0tDd},
}

@InProceedings{fan2022minedojo,
  author    = {Linxi Fan and Guanzhi Wang and Yunfan Jiang and Ajay Mandlekar and Yuncong Yang and Haoyi Zhu and Andrew Tang and De-An Huang and Yuke Zhu and Anima Anandkumar},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  title     = {MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge},
  year      = {2022},
  file      = {:fan2022minedojo - MineDojo_ Building Open Ended Embodied Agents with Internet Scale Knowledge.pdf:PDF},
  groups    = {Language-Augmented RL},
  ranking   = {rank2},
  url       = {https://openreview.net/forum?id=rc8o_j8I8PX},
}

@InProceedings{NIPS2015_e0040614,
  author    = {Mohamed, Shakir and Jimenez Rezende, Danilo},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
  year      = {2015},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {28},
  file      = {:NIPS2015_e0040614 - Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning.pdf:PDF},
  groups    = {Intrinsic goals},
  ranking   = {rank3},
  url       = {https://proceedings.neurips.cc/paper/2015/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},
}

@InProceedings{dimakopoulou18a,
  author    = {Dimakopoulou, Maria and Van Roy, Benjamin},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Coordinated Exploration in Concurrent Reinforcement Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {1271--1279},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {We consider a team of reinforcement learning agents that concurrently learn to operate in a common environment. We identify three properties - adaptivity, commitment, and diversity - which are necessary for efficient coordinated exploration and demonstrate that straightforward extensions to single-agent optimistic and posterior sampling approaches fail to satisfy them. As an alternative, we propose seed sampling, which extends posterior sampling in a manner that meets these requirements. Simulation results investigate how per-agent regret decreases as the number of agents grows, establishing substantial advantages of seed sampling over alternative exploration schemes.},
  file      = {:dimakopoulou18a - Coordinated Exploration in Concurrent Reinforcement Learning.pdf:PDF},
  groups    = {Intrinsic rewards in MARL},
  pdf       = {http://proceedings.mlr.press/v80/dimakopoulou18a/dimakopoulou18a.pdf},
  url       = {https://proceedings.mlr.press/v80/dimakopoulou18a.html},
}

@InProceedings{yang20i,
  author    = {Yang, Yaodong and Wen, Ying and Wang, Jun and Chen, Liheng and Shao, Kun and Mguni, David and Zhang, Weinan},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {Multi-Agent Determinantal Q-Learning},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {10757--10766},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {Centralized training with decentralized execution has become an important paradigm in multi-agent learning. Though practical, current methods rely on restrictive assumptions to decompose the centralized value function across agents for execution. In this paper, we eliminate this restriction by proposing multi-agent determinantal Q-learning. Our method is established on Q-DPP, a novel extension of determinantal point process (DPP) to multi-agent setting. Q-DPP promotes agents to acquire diverse behavioral models; this allows a natural factorization of the joint Q-functions with no need for \emph{a priori} structural constraints on the value function or special network architectures. We demonstrate that Q-DPP generalizes major solutions including VDN, QMIX, and QTRAN on decentralizable cooperative tasks. To efficiently draw samples from Q-DPP, we develop a linear-time sampler with theoretical approximation guarantee. Our sampler also benefits exploration by coordinating agents to cover orthogonal directions in the state space during training. We evaluate our algorithm on multiple cooperative benchmarks; its effectiveness has been demonstrated when compared with the state-of-the-art.},
  file      = {:yang20i - Multi Agent Determinantal Q Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v119/yang20i/yang20i.pdf},
  ranking   = {rank1},
  url       = {https://proceedings.mlr.press/v119/yang20i.html},
}

@Article{Wang_2020,
  author    = {Jianhong Wang and Yuan Zhang and Tae-Kyun Kim and Yunjie Gu},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Shapley Q-Value: A Local Reward Approach to Solve Global Reward Games},
  year      = {2020},
  month     = {apr},
  number    = {05},
  pages     = {7285--7292},
  volume    = {34},
  doi       = {10.1609/aaai.v34i05.6220},
  groups    = {Multi-agent RL},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@Article{Boehmer2019,
  author        = {Wendelin Böhmer and Tabish Rashid and Shimon Whiteson},
  title         = {Exploration with Unreliable Intrinsic Reward in Multi-Agent Reinforcement Learning},
  year          = {2019},
  month         = jun,
  abstract      = {This paper investigates the use of intrinsic reward to guide exploration in multi-agent reinforcement learning. We discuss the challenges in applying intrinsic reward to multiple collaborative agents and demonstrate how unreliable reward can prevent decentralized agents from learning the optimal policy. We address this problem with a novel framework, Independent Centrally-assisted Q-learning (ICQL), in which decentralized agents share control and an experience replay buffer with a centralized agent. Only the centralized agent is intrinsically rewarded, but the decentralized agents still benefit from improved exploration, without the distraction of unreliable incentives.},
  archiveprefix = {arXiv},
  eprint        = {1906.02138},
  file          = {:Boehmer2019 - Exploration with Unreliable Intrinsic Reward in Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Intrinsic rewards in MARL},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@PhdThesis{schafer2019curiosity,
  author     = {Schafer, Lukas},
  school     = {Master’s thesis, The University of Edinburgh},
  title      = {Curiosity in multi-agent reinforcement learning},
  year       = {2019},
  file       = {:schaefer_msc_thesis.pdf:PDF},
  groups     = {Intrinsic rewards in MARL},
  readstatus = {skimmed},
  url        = {https://www.researchgate.net/profile/Lukas-Schaefer-4/publication/337991678_Curiosity_in_Multi-Agent_Reinforcement_Learning/links/5df92a6c299bf10bc3634a1a/Curiosity-in-Multi-Agent-Reinforcement-Learning.pdf},
}

@InProceedings{Chitnis2020Intrinsic,
  author    = {Rohan Chitnis and Shubham Tulsiani and Saurabh Gupta and Abhinav Gupta},
  booktitle = {International Conference on Learning Representations},
  title     = {Intrinsic Motivation for Encouraging Synergistic Behavior},
  year      = {2020},
  file      = {:Chitnis2020Intrinsic - Intrinsic Motivation for Encouraging Synergistic Behavior.pdf:PDF},
  groups    = {Intrinsic rewards in MARL},
  url       = {https://openreview.net/forum?id=SJleNCNtDH},
}

@InProceedings{NEURIPS2018_7fea637f,
  author    = {Hughes, Edward and Leibo, Joel Z and Phillips, Matthew and Tuyls, Karl and Due\~{n}ez-Guzman, Edgar and Garc\'{\i}a Casta\~{n}eda, Antonio and Dunning, Iain and Zhu, Tina and McKee, Kevin and Koster, Raphael and Roff, Heather and Graepel, Thore},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Inequity aversion improves cooperation in intertemporal social dilemmas},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {31},
  file      = {:NEURIPS2018_7fea637f - Inequity Aversion Improves Cooperation in Intertemporal Social Dilemmas.pdf:PDF},
  groups    = {Intrinsic rewards in MARL},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper/2018/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf},
}

@InProceedings{NEURIPS2018_1ef03ed0,
  author    = {Strouse, DJ and Kleiman-Weiner, Max and Tenenbaum, Josh and Botvinick, Matt and Schwab, David J},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning to Share and Hide Intentions using Information Regularization},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {31},
  file      = {:NEURIPS2018_1ef03ed0 - Learning to Share and Hide Intentions Using Information Regularization.pdf:PDF},
  groups    = {Intrinsic rewards in MARL},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper/2018/file/1ef03ed0cd5863c550128836b28ec3e9-Paper.pdf},
}

@Article{Oudeyer2007,
  author   = {Oudeyer, Pierre-Yves and Kaplan, Frederic},
  journal  = {Frontiers in neurorobotics},
  title    = {What is Intrinsic Motivation? A Typology of Computational Approaches},
  year     = {2007},
  issn     = {1662-5218},
  pages    = {6},
  volume   = {1},
  abstract = {Intrinsic motivation, centrally involved in spontaneous exploration and curiosity, is a crucial concept in developmental psychology. It has been argued to be a crucial mechanism for open-ended cognitive development in humans, and as such has gathered a growing interest from developmental roboticists in the recent years. The goal of this paper is threefold. First, it provides a synthesis of the different approaches of intrinsic motivation in psychology. Second, by interpreting these approaches in a computational reinforcement learning framework, we argue that they are not operational and even sometimes inconsistent. Third, we set the ground for a systematic operational study of intrinsic motivation by presenting a formal typology of possible computational approaches. This typology is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We argue that this kind of computational typology might be useful for opening new avenues for research both in psychology and developmental robotics.},
  doi      = {10.3389/neuro.12.006.2007},
  file     = {:14505970.pdf:PDF},
  groups   = {Intrinsic goals},
  ranking  = {rank3},
}

@Article{4141061,
  author  = {Oudeyer, Pierre-Yves and Kaplan, Frdric and Hafner, Verena V.},
  journal = {IEEE Transactions on Evolutionary Computation},
  title   = {Intrinsic Motivation Systems for Autonomous Mental Development},
  year    = {2007},
  number  = {2},
  pages   = {265-286},
  volume  = {11},
  doi     = {10.1109/TEVC.2006.890271},
  file    = {:4141061 - Intrinsic Motivation Systems for Autonomous Mental Development (1).pdf:PDF},
  groups  = {Intrinsic goals},
  ranking = {rank4},
}

@InProceedings{NIPS2017_3a20f62a,
  author    = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {\#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  file      = {:NIPS2017_3a20f62a - #Exploration_ a Study of Count Based Exploration for Deep Reinforcement Learning.pdf:PDF},
  groups    = {Visitation count},
  ranking   = {rank3},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf},
}

@PhdThesis{wiegand2003analysis,
  author  = {Wiegand, R Paul},
  school  = {George Mason University},
  title   = {An Analysis of Cooperative Coevolutionary Algorithms},
  year    = {2003},
  comment = {Definition of relative overgeneralization},
  groups  = {Multi-agent learning},
  ranking = {rank2},
  url     = {http://l.academicdirect.org/Horticulture/GAs/Refs/PhD_Wiegand&Jong_2003.pdf},
}

@InProceedings{Schmidhuber1991,
  author    = {Schmidhuber, J{\"u}rgen},
  booktitle = {Proceedings of the international conference on simulation of adaptive behavior: From animals to animats},
  title     = {A possibility for implementing curiosity and boredom in model-building neural controllers},
  year      = {1991},
  pages     = {222--227},
  file      = {:Schmidhuber1991 - A Possibility for Implementing Curiosity and Boredom in Model Building Neural Controllers.pdf:PDF},
  groups    = {Intrinsic goals},
  ranking   = {rank3},
}

@InProceedings{Jo2022_LECO,
  author    = {Daejin Jo and Sungwoong Kim and Daniel Wontae Nam and Taehwan Kwon and Seungeun Rho and Jongmin Kim and Donghoon Lee},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{LECO}: Learnable Episodic Count for Task-Specific Intrinsic Reward},
  year      = {2022},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  groups    = {Intrinsic goals},
  url       = {https://openreview.net/forum?id=FJ42JCNNUYT},
}

@InProceedings{Huang2022_InnerMonol,
  author     = {Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Tomas Jackson and Noah Brown and Linda Luu and Sergey Levine and Karol Hausman and brian ichter},
  booktitle  = {Proceedings of The 6th Conference on Robot Learning},
  title      = {Inner Monologue: Embodied Reasoning through Planning with Language Models},
  year       = {2023},
  editor     = {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  month      = {14--18 Dec},
  pages      = {1769--1782},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {205},
  abstract   = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
  file       = {:Huang2022_InnerMonol - Inner Monologue_ Embodied Reasoning through Planning with Language Models.pdf:PDF},
  groups     = {Language-Augmented RL},
  pdf        = {https://proceedings.mlr.press/v205/huang23c/huang23c.pdf},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v205/huang23c.html},
}

@InProceedings{Niu2021,
  author     = {Niu, Yaru and Paleja, Rohan and Gombolay, Matthew},
  booktitle  = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Multi-Agent Graph-Attention Communication and Teaming},
  year       = {2021},
  address    = {Richland, SC},
  pages      = {964–973},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '21},
  abstract   = {High-performing teams learn effective communication strategies to judiciously share information and reduce the cost of communication overhead. Within multi-agent reinforcement learning, synthesizing effective policies requires reasoning about when to communicate, whom to communicate with, and how to process messages. We propose a novel multi-agent reinforcement learning algorithm, Multi-Agent Graph-attentIon Communication (MAGIC), with a graph-attention communication protocol in which we learn 1) a Scheduler to help with the problems of when to communicate and whom to address messages to, and 2) a Message Processor using Graph Attention Networks (GATs) with dynamic graphs to deal with communication signals. The Scheduler consists of a graph attention encoder and a differentiable attention mechanism, which outputs dynamic, differentiable graphs to the Message Processor, which enables the Scheduler and Message Processor to be trained end-to-end. We evaluate our approach on a variety of cooperative tasks, including Google Research Football. Our method outperforms baselines across all domains, achieving ~10.5% increase in reward in the most challenging domain. We also show MAGIC communicates $27.4%$ more efficiently on average than baselines, is robust to stochasticity, and scales to larger state-action spaces. Finally, we demonstrate MAGIC on a physical, multi-robot testbed.},
  file       = {:Niu2021 - Multi Agent Graph Attention Communication and Teaming.pdf:PDF},
  groups     = {Communication},
  isbn       = {9781450383073},
  keywords   = {multi-agent reinforcement learning, multi-agent communication, graph-based communication},
  location   = {Virtual Event, United Kingdom},
  numpages   = {10},
  ranking    = {rank1},
  readstatus = {read},
}

@InProceedings{Wang2022_FCMNet,
  author     = {Wang, Yutong and Sartoretti, Guillaume},
  booktitle  = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
  title      = {FCMNet: Full Communication Memory Net for Team-Level Cooperation in Multi-Agent Systems},
  year       = {2022},
  address    = {Richland, SC},
  pages      = {1355–1363},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '22},
  abstract   = {Decentralized cooperation in partially-observable multi-agent systems requires effective communications among agents. To support this effort, this work focuses on the class of problems where global communications are available but may be unreliable, thus precluding differentiable communication learning methods. We introduce FCMNet, a reinforcement learning based approach that allows agents to simultaneously learn a) an effective multi-hop communications protocol and b) a common, decentralized policy that enables team-level decision-making. Specifically, our proposed method utilizes the hidden states of multiple directional recurrent neural networks as communication messages among agents. Using a simple multi-hop topology, we endow each agent with the ability to receive information sequentially encoded by every other agent at each time step, leading to improved global cooperation. We demonstrate FCMNet on a challenging set of StarCraft II micromanagement tasks with shared rewards, as well as a collaborative multi-agent pathfinding task with individual rewards. There, our comparison results show that FCMNet outperforms state-of-the-art communication-based reinforcement learning methods in all StarCraft II micromanagement tasks, and value decomposition methods in certain tasks. We further investigate the robustness of FCMNet under realistic communication disturbances, such as random message loss or binarized messages (i.e., non-differentiable communication channels), to showcase FMCNet's potential applicability to robotic tasks under a variety of real-world conditions.},
  file       = {:Wang2022 - FCMNet_ Full Communication Memory Net for Team Level Cooperation in Multi Agent Systems.pdf:PDF},
  groups     = {Communication},
  keywords   = {multi-agent reinforcement learning, differentiable communications, decentralized cooperation, communication learning},
  location   = {Virtual Event, New Zealand},
  numpages   = {9},
  priority   = {prio1},
  readstatus = {skimmed},
  url        = {https://dl.acm.org/doi/abs/10.5555/3535850.3536001},
}

@InProceedings{Tucker2020,
  author    = {Tucker, Mycal and Aksaray, Derya and Paul, Rohan and Stein, Gregory J. and Roy, Nicholas},
  booktitle = {Robotics Research},
  title     = {Learning Unknown Groundings for Natural Language Interaction with Mobile Robots},
  year      = {2020},
  address   = {Cham},
  editor    = {Amato, Nancy M. and Hager, Greg and Thomas, Shawna and Torres-Torriti, Miguel},
  pages     = {317--333},
  publisher = {Springer International Publishing},
  abstract  = {Our goal is to enable robots to understand or ``ground'' natural language instructions in the context of their perceived workspace. Contemporary models learn a probabilistic correspondence between input phrases and semantic concepts (or groundings) such as objects, regions or goals for robot motion derived from the robot's world model. Crucially, these models assume a fixed and a priori known set of object types as well as phrases and train probable correspondences offline using static language-workspace corpora. Hence, model inference fails when an input command contains unknown phrases or references to novel object types that were not seen during the training. We introduce a probabilistic model that incorporates a notion of unknown groundings and learns a correspondence between an unknown phrase and an unknown object that cannot be classified into known visual categories. Further, we extend the model to ``hypothesize'' known or unknown object groundings in case the language utterance references an object that exists beyond the robot's partial view of its workspace. When the grounding for an instruction is unknown or hypothetical, the robot performs exploratory actions to gather new observations and find the referenced objects beyond the current view. Once an unknown grounding is associated with percepts of a new object, the model is adapted and trained online using accrued visual-linguistic observations to reflect the new knowledge gained for interpreting future utterances. We evaluate the model quantitatively using a corpus from a user study and report experiments on a mobile platform in a workspace populated with objects from a standardized dataset. A video of the experimental demonstration is available at: https://youtu.be/XFLNdaUKgW0.},
  file      = {:Tucker2020 - Learning Unknown Groundings for Natural Language Interaction with Mobile Robots.pdf:PDF},
  groups    = {Language-Augmented RL},
  isbn      = {978-3-030-28619-4},
  priority  = {prio2},
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-28619-4_27},
}

@Article{Karten2023,
  author        = {Karten, Seth and Tucker, Mycal and Li, Huao and Kailas, Siva and Lewis, Michael and Sycara, Katia},
  journal       = {{IEEE} Transactions on Cognitive and Developmental Systems},
  title         = {Interpretable Learned Emergent Communication for Human-Agent Teams},
  year          = {2023},
  month         = jan,
  pages         = {1--1},
  abstract      = {Learning interpretable communication is essential for multi-agent and human-agent teams (HATs). In multi-agent reinforcement learning for partially-observable environments, agents may convey information to others via learned communication, allowing the team to complete its task. Inspired by human languages, recent works study discrete (using only a finite set of tokens) and sparse (communicating only at some time-steps) communication. However, the utility of such communication in human-agent team experiments has not yet been investigated. In this work, we analyze the efficacy of sparse-discrete methods for producing emergent communication that enables high agent-only and human-agent team performance. We develop agent-only teams that communicate sparsely via our scheme of Enforcers that sufficiently constrain communication to any budget. Our results show no loss or minimal loss of performance in benchmark environments and tasks. In human-agent teams tested in benchmark environments, where agents have been modeled using the Enforcers, we find that a prototype-based method produces meaningful discrete tokens that enable human partners to learn agent communication faster and better than a one-hot baseline. Additional HAT experiments show that an appropriate sparsity level lowers the cognitive load of humans when communicating with teams of agents and leads to superior team performance.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.1109/tcds.2023.3236599},
  eprint        = {2201.07452},
  file          = {:Karten2022 - Interpretable Learned Emergent Communication for Human Agent Teams.pdf:PDF},
  groups        = {Discrete language},
  keywords      = {Machine Learning (cs.LG), Multiagent Systems (cs.MA), Robotics (cs.RO), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {Institute of Electrical and Electronics Engineers ({IEEE})},
  readstatus    = {read},
}

@InProceedings{Tucker2022_VQVIB,
  author    = {Mycal Tucker and Roger P. Levy and Julie Shah and Noga Zaslavsky},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Trading off Utility, Informativeness, and Complexity in Emergent Communication},
  year      = {2022},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  file      = {:Tucker2022_VQBIB - Trading off Utility, Informativeness, and Complexity in Emergent Communication.pdf:PDF},
  groups    = {Discrete language},
  priority  = {prio1},
  url       = {https://openreview.net/forum?id=O5arhQvBdH},
}

@InProceedings{Karten2022,
  author    = {Karten, Seth and Sycara, Katia},
  booktitle = {Workshop IROS 2022 - Multi-agent workshop},
  title     = {Intent-Grounded Compositional Communication through Mutual Information in Multi-Agent Teams},
  year      = {2022},
  file      = {:Karten2022 - Intent Grounded Compositional Communication through Mutual Information in Multi Agent Teams.pdf:PDF},
  groups    = {Discrete language},
}

@InProceedings{chaabouni2022emergent,
  author     = {Rahma Chaabouni and Florian Strub and Florent Altch{\'e} and Eugene Tarassov and Corentin Tallec and Elnaz Davoodi and Kory Wallace Mathewson and Olivier Tieleman and Angeliki Lazaridou and Bilal Piot},
  booktitle  = {International Conference on Learning Representations},
  title      = {Emergent Communication at Scale},
  year       = {2022},
  file       = {:chaabouni2022emergent - Emergent Communication at Scale.pdf:PDF},
  groups     = {Communication},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://openreview.net/forum?id=AUGBfDIV9rL},
}

@InProceedings{yao2022linking,
  author    = {Shunyu Yao and Mo Yu and Yang Zhang and Karthik R Narasimhan and Joshua B. Tenenbaum and Chuang Gan},
  booktitle = {International Conference on Learning Representations},
  title     = {Linking Emergent and Natural Languages via Corpus Transfer},
  year      = {2022},
  file      = {:yao2022linking - Linking Emergent and Natural Languages Via Corpus Transfer.pdf:PDF},
  groups    = {Communication},
  priority  = {prio2},
  url       = {https://openreview.net/forum?id=49A1Y6tRhaq},
}

@InProceedings{Rita2022_PopHetero,
  author    = {Mathieu Rita and Florian Strub and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux},
  booktitle = {International Conference on Learning Representations},
  title     = {On the role of population heterogeneity in emergent communication},
  year      = {2022},
  file      = {:rita2022on - On the Role of Population Heterogeneity in Emergent Communication.pdf:PDF},
  groups    = {Communication},
  priority  = {prio1},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=5Qkd7-bZfI},
}

@InProceedings{Rita2020,
  author    = {Rita, Mathieu and Chaabouni, Rahma and Dupoux, Emmanuel},
  booktitle = {Proceedings of the 24th Conference on Computational Natural Language Learning},
  title     = {LazImpa: Lazy and Impatient neural agents learn to communicate efficiently},
  year      = {2020},
  address   = {Online},
  month     = nov,
  pages     = {335--343},
  publisher = {Association for Computational Linguistics},
  abstract  = {Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, {``}LazImpa{''}, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.},
  doi       = {10.18653/v1/2020.conll-1.26},
  file      = {:Rita2020 - ``LazImpa''_ Lazy and Impatient Neural Agents Learn to Communicate Efficiently.pdf:PDF},
  groups    = {Discrete language, Speaker-Listener},
  priority  = {prio2},
  url       = {https://aclanthology.org/2020.conll-1.26},
}

@InProceedings{wang2022tomc,
  author    = {Yuanfei Wang and fangwei zhong and Jing Xu and Yizhou Wang},
  booktitle = {International Conference on Learning Representations},
  title     = {ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind},
  year      = {2022},
  file      = {:wang2022tomc - ToM2C_ Target Oriented Multi Agent Communication and Cooperation with Theory of Mind.pdf:PDF},
  groups    = {Communication},
  priority  = {prio1},
  url       = {https://openreview.net/forum?id=2t7CkQXNpuq},
}

@InProceedings{Lu2022_R5,
  author    = {Shengyao Lu and Bang Liu and Keith G Mills and SHANGLING JUI and Di Niu},
  booktitle = {International Conference on Learning Representations},
  title     = {R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning},
  year      = {2022},
  file      = {:Lu2022_R5 - R5_ Rule Discovery with Reinforced and Recurrent Relational Reasoning.pdf:PDF},
  groups    = {RL},
  priority  = {prio3},
  url       = {https://openreview.net/forum?id=2eXhNpHeW6E},
}

@InProceedings{banino2022coberl,
  author    = {Andrea Banino and Adria Puigdomenech Badia and Jacob C Walker and Tim Scholtes and Jovana Mitrovic and Charles Blundell},
  booktitle = {International Conference on Learning Representations},
  title     = {Co{BERL}: Contrastive {BERT} for Reinforcement Learning},
  year      = {2022},
  file      = {:banino2022coberl - CoBERL_ Contrastive BERT for Reinforcement Learning.pdf:PDF},
  groups    = {RL, Transformers in RL},
  url       = {https://openreview.net/forum?id=sRZ3GhmegS},
}

@InProceedings{franzmeyer2022learning,
  author    = {Tim Franzmeyer and Mateusz Malinowski and Joao F. Henriques},
  booktitle = {International Conference on Learning Representations},
  title     = {Learning Altruistic Behaviours in Reinforcement Learning without External Rewards},
  year      = {2022},
  file      = {:franzmeyer2022learning - Learning Altruistic Behaviours in Reinforcement Learning without External Rewards.pdf:PDF},
  groups    = {Intrinsic rewards in MARL},
  url       = {https://openreview.net/forum?id=KxbhdyiPHE},
}

@InProceedings{rengarajan2022reinforcement,
  author    = {Desik Rengarajan and Gargi Vaidya and Akshay Sarvesh and Dileep Kalathil and Srinivas Shakkottai},
  booktitle = {International Conference on Learning Representations},
  title     = {Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration},
  year      = {2022},
  file      = {:rengarajan2022reinforcement - Reinforcement Learning with Sparse Rewards Using Guidance from Offline Demonstration.pdf:PDF},
  groups    = {RL},
  url       = {https://openreview.net/forum?id=YJ1WzgMVsMt},
}

@InProceedings{kuba2022trust,
  author    = {Jakub Grudzien Kuba and Ruiqing Chen and Muning Wen and Ying Wen and Fanglei Sun and Jun Wang and Yaodong Yang},
  booktitle = {International Conference on Learning Representations},
  title     = {Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning},
  year      = {2022},
  file      = {:kuba2022trust - Trust Region Policy Optimisation in Multi Agent Reinforcement Learning.pdf:PDF},
  groups    = {Multi-agent RL},
  url       = {https://openreview.net/forum?id=EcGGFkNTxdJ},
}

@InProceedings{liang2022reward,
  author    = {Xinran Liang and Katherine Shu and Kimin Lee and Pieter Abbeel},
  booktitle = {International Conference on Learning Representations},
  title     = {Reward Uncertainty for Exploration in Preference-based Reinforcement Learning},
  year      = {2022},
  file      = {:liang2022reward - Reward Uncertainty for Exploration in Preference Based Reinforcement Learning.pdf:PDF},
  groups    = {Intrinsic goals, RLHF},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=OWZVD-l-ZrC},
}

@InProceedings{Trauble2022the,
  author    = {Frederik Tr{\"a}uble and Andrea Dittadi and Manuel Wuthrich and Felix Widmaier and Peter Vincent Gehler and Ole Winther and Francesco Locatello and Olivier Bachem and Bernhard Sch{\"o}lkopf and Stefan Bauer},
  booktitle = {International Conference on Learning Representations},
  title     = {The Role of Pretrained Representations for the {OOD} Generalization of {RL} Agents},
  year      = {2022},
  file      = {:Trauble2022the - The Role of Pretrained Representations for the OOD Generalization of RL Agents.pdf:PDF},
  groups    = {RL},
  url       = {https://openreview.net/forum?id=8eb12UQYxrG},
}

@InProceedings{f79ef4d4dd994bceb33181d654f20d21,
  author    = {Lake, {Brenden M.} and Tal Linzen and Marco Baroni},
  booktitle = {Proceedings of the 41st Annual Meeting of the Cognitive Science Society},
  title     = {Human few-shot learning of compositional instructions},
  year      = {2019},
  note      = {Funding Information: We thank the NYU ConCats group, Michael Frank, Kristina Gulordava, Germ{\'a}n Kruszewski, Roger Levy, and Adina Williams for helpful suggestions. Publisher Copyright: {\textcopyright} Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019.All rights reserved.; 41st Annual Meeting of the Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019 ; Conference date: 24-07-2019 Through 27-07-2019},
  pages     = {611--617},
  publisher = {The Cognitive Science Society},
  series    = {Proceedings of the 41st Annual Meeting of the Cognitive Science Society: Creativity + Cognition + Computation, CogSci 2019},
  abstract  = {People learn in fast and flexible ways that have not been emulated by machines. Once a person learns a new verb “dax,” he or she can effortlessly understand how to “dax twice,” “walk and dax,” or “dax vigorously.” There have been striking recent improvements in machine learning for natural language processing, yet the best algorithms require vast amounts of experience and struggle to generalize new concepts in compositional ways. To better understand these distinctively human abilities, we study the compositional skills of people through language-like instruction learning tasks. Our results show that people can learn and use novel functional concepts from very few examples (few-shot learning), successfully applying familiar functions to novel inputs. People can also compose concepts in complex ways that go beyond the provided demonstrations. Two additional experiments examined the assumptions and inductive biases that people make when solving these tasks, revealing three biases: mutual exclusivity, one-to-one mappings, and iconic concatenation. We discuss the implications for cognitive modeling and the potential for building machines with more human-like language learning capabilities.},
  file      = {:f79ef4d4dd994bceb33181d654f20d21 - Human Few Shot Learning of Compositional Instructions.pdf:PDF},
  groups    = {Language},
  keywords  = {compositionality, concept learning, neural networks, word learning},
  language  = {English (US)},
  priority  = {prio2},
  url       = {https://nyuscholars.nyu.edu/en/publications/human-few-shot-learning-of-compositional-instructions},
}

@InProceedings{Lake2019,
  author    = {Lake, Brenden M},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Compositional generalization through meta sequence-to-sequence learning},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  file      = {:Lake2019 - Compositional Generalization through Meta Sequence to Sequence Learning.pdf:PDF},
  groups    = {Language},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper/2019/file/f4d0e2e7fc057a58f7ca4a391f01940a-Paper.pdf},
}

@InProceedings{Ruis2020,
  author    = {Ruis, Laura and Andreas, Jacob and Baroni, Marco and Bouchacourt, Diane and Lake, Brenden M},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {A Benchmark for Systematic Generalization in Grounded Language Understanding},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {19861--19872},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  file      = {:Ruis2020 - A Benchmark for Systematic Generalization in Grounded Language Understanding.pdf:PDF},
  groups    = {Language},
  priority  = {prio1},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper/2020/file/e5a90182cc81e12ab5e72d66e0b46fe3-Paper.pdf},
}

@Article{Lake2021,
  author     = {Brenden M. Lake and Gregory L. Murphy},
  journal    = {Psychological Review},
  title      = {Word meaning in minds and machines.},
  year       = {2021},
  month      = {jul},
  doi        = {10.1037/rev0000297},
  file       = {:Lake2021 - Word Meaning in Minds and Machines..pdf:PDF},
  groups     = {Language},
  publisher  = {American Psychological Association ({APA})},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{Li2022b,
  author     = {Sheng Li and Yutai Zhou and Ross Allen and Mykel J. Kochenderfer},
  booktitle  = {2022 International Conference on Robotics and Automation ({ICRA})},
  title      = {Learning Emergent Discrete Message Communication for Cooperative Reinforcement Learning},
  year       = {2022},
  month      = {may},
  publisher  = {{IEEE}},
  doi        = {10.1109/icra46639.2022.9812285},
  file       = {:Li2022b - Learning Emergent Discrete Message Communication for Cooperative Reinforcement Learning.pdf:PDF},
  groups     = {Discrete language},
  readstatus = {read},
}

@Article{Mao2020_GACML,
  author       = {Mao, Hangyu and Zhang, Zhengchao and Xiao, Zhen and Gong, Zhibo and Ni, Yan},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Learning Agent Communication under Limited Bandwidth by Message Pruning},
  year         = {2020},
  month        = {Apr.},
  number       = {04},
  pages        = {5142-5149},
  volume       = {34},
  abstractnote = {&lt;p&gt;Communication is a crucial factor for the big multi-agent world to stay organized and productive. Recently, Deep Reinforcement Learning (DRL) has been applied to learn the communication strategy and the control policy for multiple agents. However, the practical &lt;em&gt;limited bandwidth&lt;/em&gt; in multi-agent communication has been largely ignored by the existing DRL methods. Specifically, many methods keep sending messages incessantly, which consumes too much bandwidth. As a result, they are inapplicable to multi-agent systems with limited bandwidth. To handle this problem, we propose a gating mechanism to adaptively prune less beneficial messages. We evaluate the gating mechanism on several tasks. Experiments demonstrate that it can prune a lot of messages with little impact on performance. In fact, the performance may be greatly improved by pruning redundant messages. Moreover, the proposed gating mechanism is applicable to several previous methods, equipping them the ability to address bandwidth restricted settings.&lt;/p&gt;},
  doi          = {10.1609/aaai.v34i04.5957},
  file         = {:Mao2020 - Learning Agent Communication under Limited Bandwidth by Message Pruning.pdf:PDF},
  groups       = {Communication},
  priority     = {prio2},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/5957},
}

@InProceedings{Das_2017_CVPR,
  author    = {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jose M. F. and Parikh, Devi and Batra, Dhruv},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Visual Dialog},
  year      = {2017},
  month     = {July},
  file      = {:Das_2017_CVPR - Visual Dialog.pdf:PDF},
  groups    = {Visual-Language Learning},
}

@InProceedings{Agarwal2019,
  author     = {Agarwal, Akshat and Gurumurthy, Swaminathan and Sharma, Vasu and Lewis, Mike and Sycara, Katia},
  booktitle  = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  title      = {Community Regularization of Visually-Grounded Dialog},
  year       = {2019},
  address    = {Richland, SC},
  pages      = {1042–1050},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '19},
  abstract   = {The task of conducting visually grounded dialog involves learning goal-oriented cooperative dialog between autonomous agents who exchange information about a scene through several rounds of questions and answers in natural language. We posit that requiring artificial agents to adhere to the rules of human language, while also requiring them to maximize information exchange through dialog is an ill-posed problem. We observe that humans do not stray from a common language because they are social creatures who live in communities, and have to communicate with many people everyday, so it is far easier to stick to a common language even at the cost of some efficiency loss. Using this as inspiration, we propose and evaluate a multi-agent community-based dialog framework where each agent interacts with, and learns from, multiple agents, and show that this community-enforced regularization results in more relevant and coherent dialog (as judged by human evaluators) without sacrificing task performance (as judged by quantitative metrics).},
  file       = {:Agarwal2019 - Community Regularization of Visually Grounded Dialog (1).pdf:PDF},
  groups     = {Visual-Language Learning, Natural language, Dialogue agents},
  isbn       = {9781450363099},
  keywords   = {curriculum learning, visual dialog, emergent communication, multi-agent reinforcement learning},
  location   = {Montreal QC, Canada},
  numpages   = {9},
  priority   = {prio2},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.5555/3306127.3331802},
}

@InProceedings{Kadar2018,
  author    = {K{\'a}d{\'a}r, {\'A}kos and Elliott, Desmond and C{\^o}t{\'e}, Marc-Alexandre and Chrupa{\l}a, Grzegorz and Alishahi, Afra},
  booktitle = {Proceedings of the 22nd Conference on Computational Natural Language Learning},
  title     = {Lessons Learned in Multilingual Grounded Language Learning},
  year      = {2018},
  address   = {Brussels, Belgium},
  month     = oct,
  pages     = {402--412},
  publisher = {Association for Computational Linguistics},
  abstract  = {Recent work has shown how to learn better visual-semantic embeddings by leveraging image descriptions in more than one language. Here, we investigate in detail which conditions affect the performance of this type of grounded language learning model. We show that multilingual training improves over bilingual training, and that low-resource languages benefit from training with higher-resource languages. We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective.},
  doi       = {10.18653/v1/K18-1039},
  file      = {:Kadar2018 - Lessons Learned in Multilingual Grounded Language Learning.pdf:PDF},
  groups    = {Visual-Language Learning},
  url       = {https://aclanthology.org/K18-1039},
}

@InProceedings{Lu2017,
  author    = {Lu, Jiasen and Kannan, Anitha and Yang, Jianwei and Parikh, Devi and Batra, Dhruv},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  file      = {:Lu2017 - Best of Both Worlds_ Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model.pdf:PDF},
  groups    = {Dialogue agents},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper/2017/file/077e29b11be80ab57e1a2ecabb7da330-Paper.pdf},
}

@Article{Zeng2022,
  author        = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  title         = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  year          = {2022},
  month         = apr,
  abstract      = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2204.00598},
  eprint        = {2204.00598},
  file          = {:Zeng2022 - Socratic Models_ Composing Zero Shot Multimodal Reasoning with Language.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
  ranking       = {rank2},
}

@InProceedings{pmlr-v164-shridhar22a,
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 5th Conference on Robot Learning},
  title     = {CLIPort: What and Where Pathways for Robotic Manipulation},
  year      = {2022},
  editor    = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  month     = {08--11 Nov},
  pages     = {894--906},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {164},
  abstract  = {How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.},
  file      = {:pmlr-v164-shridhar22a - CLIPort_ What and Where Pathways for Robotic Manipulation.pdf:PDF},
  groups    = {Language-Augmented RL},
  pdf       = {https://proceedings.mlr.press/v164/shridhar22a/shridhar22a.pdf},
  priority  = {prio2},
  ranking   = {rank3},
  url       = {https://proceedings.mlr.press/v164/shridhar22a.html},
}

@InProceedings{Lupu2021_TrajeDi,
  author     = {Lupu, Andrei and Cui, Brandon and Hu, Hengyuan and Foerster, Jakob},
  booktitle  = {Proceedings of the 38th International Conference on Machine Learning},
  title      = {Trajectory Diversity for Zero-Shot Coordination},
  year       = {2021},
  editor     = {Meila, Marina and Zhang, Tong},
  month      = {18--24 Jul},
  pages      = {7204--7213},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {139},
  abstract   = {We study the problem of zero-shot coordination (ZSC), where agents must independently produce strategies for a collaborative game that are compatible with novel partners not seen during training. Our first contribution is to consider the need for diversity in generating such agents. Because self-play (SP) agents control their own trajectory distribution during training, each policy typically only performs well on this exact distribution. As a result, they achieve low scores in ZSC, since playing with another agent is likely to put them in situations they have not encountered during training. To address this issue, we train a common best response (BR) to a population of agents, which we regulate to be diverse. To this end, we introduce \textit{Trajectory Diversity} (TrajeDi) – a differentiable objective for generating diverse reinforcement learning policies. We derive TrajeDi as a generalization of the Jensen-Shannon divergence between policies and motivate it experimentally in two simple settings. We then focus on the collaborative card game Hanabi, demonstrating the scalability of our method and improving upon the cross-play scores of both independently trained SP agents and BRs to unregularized populations.},
  file       = {:pmlr-v139-lupu21a - Trajectory Diversity for Zero Shot Coordination.pdf:PDF},
  groups     = {Intrinsic rewards in MARL},
  pdf        = {http://proceedings.mlr.press/v139/lupu21a/lupu21a.pdf},
  ranking    = {rank1},
  readstatus = {skimmed},
  url        = {https://proceedings.mlr.press/v139/lupu21a.html},
}

@InProceedings{Strouse2021a,
  author    = {Strouse, DJ and McKee, Kevin and Botvinick, Matt and Hughes, Edward and Everett, Richard},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Collaborating with Humans without Human Data},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {14502--14515},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:Strouse2021a - Collaborating with Humans without Human Data.pdf:PDF},
  groups    = {Multi-agent RL},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper.pdf},
}

@Article{Schaefer2023,
  author        = {Schäfer, Lukas and Slumbers, Oliver and McAleer, Stephen and Du, Yali and Albrecht, Stefano V. and Mguni, David},
  title         = {Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning},
  year          = {2023},
  month         = feb,
  abstract      = {Cooperative multi-agent reinforcement learning (MARL) requires agents to explore to learn to cooperate. Existing value-based MARL algorithms commonly rely on random exploration, such as $\epsilon$-greedy, which is inefficient in discovering multi-agent cooperation. Additionally, the environment in MARL appears non-stationary to any individual agent due to the simultaneous training of other agents, leading to highly variant and thus unstable optimisation signals. In this work, we propose ensemble value functions for multi-agent exploration (EMAX), a general framework to extend any value-based MARL algorithm. EMAX trains ensembles of value functions for each agent to address the key challenges of exploration and non-stationarity: (1) The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation. (2) Average value estimates across the ensemble serve as target values. These targets exhibit lower variance compared to commonly applied target networks and we show that they lead to more stable gradients during the optimisation. We instantiate three value-based MARL algorithms with EMAX, independent DQN, VDN and QMIX, and evaluate them in 21 tasks across four environments. Using ensembles of five value functions, EMAX improves sample efficiency and final evaluation returns of these algorithms by 53%, 36%, and 498%, respectively, averaged all 21 tasks.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2302.03439},
  eprint        = {2302.03439},
  file          = {:Schaefer2023 - Ensemble Value Functions for Efficient Exploration in Multi Agent Reinforcement Learning.pdf:PDF},
  groups        = {Exploration in MARL},
  keywords      = {Multiagent Systems (cs.MA), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.MA},
  priority      = {prio1},
  publisher     = {arXiv},
}

@Article{Buesing2018,
  author        = {Buesing, Lars and Weber, Theophane and Racaniere, Sebastien and Eslami, S. M. Ali and Rezende, Danilo and Reichert, David P. and Viola, Fabio and Besse, Frederic and Gregor, Karol and Hassabis, Demis and Wierstra, Daan},
  title         = {Learning and Querying Fast Generative Models for Reinforcement Learning},
  year          = {2018},
  month         = feb,
  abstract      = {A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1802.03006},
  eprint        = {1802.03006},
  file          = {:Buesing2018 - Learning and Querying Fast Generative Models for Reinforcement Learning.pdf:PDF},
  groups        = {Latent Dynamics Model},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank1},
  readstatus    = {skimmed},
}

@InProceedings{Racaniere2017,
  author    = {Racani\`{e}re, S\'{e}bastien and Weber, Theophane and Reichert, David and Buesing, Lars and Guez, Arthur and Jimenez Rezende, Danilo and Puigdom\`{e}nech Badia, Adri\`{a} and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Imagination-Augmented Agents for Deep Reinforcement Learning},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  file      = {:Racaniere2017 - Imagination Augmented Agents for Deep Reinforcement Learning.pdf:PDF},
  groups    = {Latent Dynamics Model},
  ranking   = {rank3},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf},
}

@InProceedings{Rangwala2020,
  author     = {Rangwala, Murtaza and Williams, Ryan},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Learning Multi-Agent Communication through Structured Attentive Reasoning},
  year       = {2020},
  editor     = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages      = {10088--10098},
  publisher  = {Curran Associates, Inc.},
  volume     = {33},
  comment    = {SARNet},
  file       = {:Rangwala2020 - Learning Multi Agent Communication through Structured Attentive Reasoning.pdf:PDF},
  groups     = {Memory in MARL, Communication},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2020/file/72ab54f9b8c11fae5b923d7f854ef06a-Paper.pdf},
}

@Article{Pesce2020,
  author     = {Emanuele Pesce and Giovanni Montana},
  journal    = {Machine Learning},
  title      = {Improving coordination in small-scale multi-agent deep reinforcement learning through memory-driven communication},
  year       = {2020},
  month      = {jan},
  number     = {9-10},
  pages      = {1727--1747},
  volume     = {109},
  abstract   = {MD-MADDPG},
  comment    = {MD-MADDPG},
  doi        = {10.1007/s10994-019-05864-5},
  file       = {:pesce2020improving - Improving Coordination in Small Scale Multi Agent Deep Reinforcement Learning through Memory Driven Communication.pdf:PDF},
  groups     = {Memory in MARL, Communication},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank1},
  readstatus = {read},
}

@InProceedings{Lampinen2021a,
  author    = {Lampinen, Andrew and Chan, Stephanie and Banino, Andrea and Hill, Felix},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Towards mental time travel: a hierarchical memory for reinforcement learning agents},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {28182--28195},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:Lampinen2021a - Towards Mental Time Travel_ a Hierarchical Memory for Reinforcement Learning Agents.pdf:PDF},
  groups    = {Memory},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/ed519dacc89b2bead3f453b0b05a4a8b-Paper.pdf},
}

@InProceedings{Zhu_2020_CVPR,
  author    = {Zhu, Yi and Zhu, Fengda and Zhan, Zhaohuan and Lin, Bingqian and Jiao, Jianbin and Chang, Xiaojun and Liang, Xiaodan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Vision-Dialog Navigation by Exploring Cross-Modal Memory},
  year      = {2020},
  month     = {June},
  file      = {:Zhu_2020_CVPR - Vision Dialog Navigation by Exploring Cross Modal Memory.pdf:PDF},
  groups    = {Visual-Language Learning, Language for memory, Memory},
  priority  = {prio1},
  ranking   = {rank1},
  url       = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhu_Vision-Dialog_Navigation_by_Exploring_Cross-Modal_Memory_CVPR_2020_paper.html},
}

@InProceedings{kim2021communication,
  author    = {Woojun Kim and Jongeui Park and Youngchul Sung},
  booktitle = {International Conference on Learning Representations},
  title     = {Communication in Multi-Agent Reinforcement Learning: Intention Sharing},
  year      = {2021},
  file      = {:kim2021communication - Communication in Multi Agent Reinforcement Learning_ Intention Sharing.pdf:PDF},
  groups    = {Communication},
  priority  = {prio1},
  url       = {https://openreview.net/forum?id=qpsl2dR9twy},
}

@InProceedings{Han2023_MBC,
  author     = {Han, Shuai and Dastani, Mehdi and Wang, Shihan},
  booktitle  = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  title      = {Model-Based Sparse Communication in Multi-Agent Reinforcement Learning},
  year       = {2023},
  address    = {Richland, SC},
  pages      = {439–447},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '23},
  abstract   = {Learning to communicate efficiently is central to multi-agent reinforcement learning (MARL). Existing methods often require agents to exchange messages intensively, which abuses communication channels and leads to high communication overhead. Only a few methods target on learning sparse communication, but they allow limited information to be shared, which affects the efficiency of policy learning. In this work, we propose model-based communication (MBC), a learning framework with a decentralized communication scheduling process. The MBC framework enables multiple agents to make decisions with sparse communication. In particular, the MBC framework introduces a model-based message estimator to estimate the up-to-date global messages using past local data. A decentralized message scheduling mechanism is also proposed to determine whether a message shall be sent based on the estimation. We evaluated our method in a variety of mixed cooperative-competitive environments. The experiment results show that the MBC method shows better performance and lower channel overhead than the state-of-art baselines.},
  comment    = {MBC},
  file       = {:Han2023 - Model Based Sparse Communication in Multi Agent Reinforcement Learning.pdf:PDF},
  groups     = {Communication},
  isbn       = {9781450394321},
  keywords   = {communication learning, message scheduling, multi-agent system, multi-agent reinforcement learning},
  location   = {London, United Kingdom},
  numpages   = {9},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.5555/3545946.3598669},
}

@InProceedings{Mees2022,
  author        = {Mees, Oier and Borja-Diaz, Jessica and Burgard, Wolfram},
  booktitle     = {ICRA 2023},
  title         = {Grounding Language with Visual Affordances over Unstructured Data},
  year          = {2023},
  month         = oct,
  publisher     = {arXiv},
  abstract      = {Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2210.01911},
  eprint        = {2210.01911},
  file          = {:Mees2022 - Grounding Language with Visual Affordances Over Unstructured Data.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
}

@InProceedings{Jain2023,
  author        = {Jain, Kanishk and Chhangani, Varun and Tiwari, Amogh and Krishna, K. Madhava and Gandhi, Vineet},
  booktitle     = {ICRA 2023},
  title         = {Ground then Navigate: Language-guided Navigation in Dynamic Scenes},
  year          = {2023},
  month         = sep,
  publisher     = {arXiv},
  abstract      = {We investigate the Vision-and-Language Navigation (VLN) problem in the context of autonomous driving in outdoor settings. We solve the problem by explicitly grounding the navigable regions corresponding to the textual command. At each timestamp, the model predicts a segmentation mask corresponding to the intermediate or the final navigable region. Our work contrasts with existing efforts in VLN, which pose this task as a node selection problem, given a discrete connected graph corresponding to the environment. We do not assume the availability of such a discretised map. Our work moves towards continuity in action space, provides interpretability through visual feedback and allows VLN on commands requiring finer manoeuvres like "park between the two cars". Furthermore, we propose a novel meta-dataset CARLA-NAV to allow efficient training and validation. The dataset comprises pre-recorded training sequences and a live environment for validation and testing. We provide extensive qualitative and quantitive empirical results to validate the efficacy of the proposed approach.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2209.11972},
  eprint        = {2209.11972},
  file          = {:Jain2022 - Ground Then Navigate_ Language Guided Navigation in Dynamic Scenes.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
}

@InProceedings{Karten2023_IMGS-MAC,
  author        = {Karten, Seth and Tucker, Mycal and Kailas, Siva and Sycara, Katia},
  booktitle     = {ICRA 2023},
  title         = {Towards True Lossless Sparse Communication in Multi-Agent Systems},
  year          = {2023},
  month         = nov,
  publisher     = {arXiv},
  abstract      = {Communication enables agents to cooperate to achieve their goals. Learning when to communicate, i.e., sparse (in time) communication, and whom to message is particularly important when bandwidth is limited. Recent work in learning sparse individualized communication, however, suffers from high variance during training, where decreasing communication comes at the cost of decreased reward, particularly in cooperative tasks. We use the information bottleneck to reframe sparsity as a representation learning problem, which we show naturally enables lossless sparse communication at lower budgets than prior art. In this paper, we propose a method for true lossless sparsity in communication via Information Maximizing Gated Sparse Multi-Agent Communication (IMGS-MAC). Our model uses two individualized regularization objectives, an information maximization autoencoder and sparse communication loss, to create informative and sparse communication. We evaluate the learned communication `language' through direct causal analysis of messages in non-sparse runs to determine the range of lossless sparse budgets, which allow zero-shot sparsity, and the range of sparse budgets that will inquire a reward loss, which is minimized by our learned gating function with few-shot sparsity. To demonstrate the efficacy of our results, we experiment in cooperative multi-agent tasks where communication is essential for success. We evaluate our model with both continuous and discrete messages. We focus our analysis on a variety of ablations to show the effect of message representations, including their properties, and lossless performance of our model.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2212.00115},
  eprint        = {2212.00115},
  file          = {:Karten2023_IMGS-MAC - Towards True Lossless Sparse Communication in Multi Agent Systems.pdf:PDF;:Towards_True_Lossless_Sparse_Communication_in_Multi-Agent_Systems.pdf:PDF},
  groups        = {Communication},
  keywords      = {Machine Learning (cs.LG), Multiagent Systems (cs.MA), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Andrews2022,
  author    = {Robert W. Andrews and J. Mason Lilly and Divya Srivastava and Karen M. Feigh},
  journal   = {Theoretical Issues in Ergonomics Science},
  title     = {The role of shared mental models in human-{AI} teams: a theoretical review},
  year      = {2022},
  month     = {apr},
  number    = {2},
  pages     = {129--175},
  volume    = {24},
  doi       = {10.1080/1463922x.2022.2061080},
  file      = {:The role of shared mental models in human AI teams a theoretical review.pdf:PDF},
  groups    = {Shared Mental Models},
  publisher = {Informa {UK} Limited},
}

@Article{Cannons1993,
  author    = {Cannon-Bowers, Janis A. and Salas, Eduardo and Converse, Sharolyn},
  journal   = {Individual and group decision making: Current issues},
  title     = {Shared mental models in expert team decision making},
  year      = {1993},
  pages     = {221-246},
  comment   = {The concept of shared mental model is defined as knowledge structures held by members of a team that enable them to form accurate explanations and expectations for the task, and, in turn, coordinate their actions and adapt their behavior to demands of the task and other team members.},
  groups    = {Shared Mental Models},
  publisher = {Lawrence Erlbaum Associates, Inc.},
  ranking   = {rank4},
  url       = {https://psycnet.apa.org/record/1993-98047-012},
}

@InProceedings{10.5555/3398761.3398815,
  author    = {Gervits, Felix and Thurston, Dean and Thielstrom, Ravenna and Fong, Terry and Pham, Quinn and Scheutz, Matthias},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Toward Genuine Robot Teammates: Improving Human-Robot Team Performance Using Robot Shared Mental Models},
  year      = {2020},
  address   = {Richland, SC},
  pages     = {429–437},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '20},
  abstract  = {Effective coordination is a critical requirement for human teaming, and is increasingly needed in teams of humans and robots. Building on decades of work in the behavioral literature, we have implemented a computational framework for coordination based on Shared Mental Models (SMMs) in which robots use a distributed knowledge base to coordinate activity. We also built a novel system connecting the robotic architecture, DIARC, to the 3D simulation environment, Unity, to serve as an evaluation platform for the framework implementation, and also for more general explorations of teaming with autonomous robots. Using this platform, we ran a user study to evaluate the framework by comparing performance of teams in which the robots used SMMs with those that did not. We found that teams in which the robots used SMMs significantly outperformed those without SMMs. This represents the first empirical demonstration that SMMs can be successfully used by fully autonomous robots interacting in natural language to improve team performance, bringing robots a step closer to genuine teammates.},
  file      = {:10.5555_3398761.3398815 - Toward Genuine Robot Teammates_ Improving Human Robot Team Performance Using Robot Shared Mental Models.pdf:PDF},
  groups    = {Shared Mental Models},
  isbn      = {9781450375184},
  keywords  = {shared mental models, human-robot teaming, coordination},
  location  = {Auckland, New Zealand},
  numpages  = {9},
  ranking   = {rank1},
  url       = {https://dl.acm.org/doi/abs/10.5555/3398761.3398815},
}

@InProceedings{Guan2022,
  author    = {Guan, Cong and Chen, Feng and Yuan, Lei and Wang, Chenghe and Yin, Hao and Zhang, Zongzhang and Yu, Yang},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Efficient Multi-agent Communication via Self-supervised Information Aggregation},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {1020--1033},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Guan2022 - Efficient Multi Agent Communication Via Self Supervised Information Aggregation.pdf:PDF},
  groups    = {Communication},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/075b2875e2b671ddd74aeec0ac9f0357-Paper-Conference.pdf},
}

@InProceedings{Du2021,
  author    = {Du, Yali and Liu, Bo and Moens, Vincent and Liu, Ziqi and Ren, Zhicheng and Wang, Jun and Chen, Xu and Zhang, Haifeng},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Learning Correlated Communication Topology in Multi-Agent Reinforcement Learning},
  year      = {2021},
  address   = {Richland, SC},
  pages     = {456–464},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '21},
  abstract  = {Communication improves the efficiency and convergence of multi-agent learning. Existing study of agent communication has been limited on predefined fixed connections. While an attention mechanism exists and is useful for scheduling the communication between agents, it, however, largely ignores the dynamical nature of communication and thus the correlation between agents' connections. In this work, we adopt a normalizing flow to encode correlation between agents interactions. The dynamical communication topology is directly learned by maximizing the agent rewards. In our end-to-end formulation, the communication structure is learned by considering it as a hidden dynamical variable. We realize centralized training of critics and graph reasoning policy, and decentralized execution from local observation and message that are received through the learned dynamical communication topology. Experiments on cooperative navigation in the particle world and adaptive traffic control tasks demonstrate the effectiveness of our method.},
  file      = {:Du2021.pdf:PDF},
  groups    = {Communication},
  isbn      = {9781450383073},
  keywords  = {communication topology, multi-agent systems, reinforcement learning},
  location  = {Virtual Event, United Kingdom},
  numpages  = {9},
  url       = {https://dl.acm.org/doi/abs/10.5555/3463952.3464010},
}

@Article{Weir2022,
  author        = {Weir, Nathaniel and Yuan, Xingdi and Côté, Marc-Alexandre and Hausknecht, Matthew and Laroche, Romain and Momennejad, Ida and Van Seijen, Harm and Van Durme, Benjamin},
  title         = {One-Shot Learning from a Demonstration with Hierarchical Latent Language},
  year          = {2022},
  month         = mar,
  abstract      = {Humans have the capability, aided by the expressive compositionality of their language, to learn quickly by demonstration. They are able to describe unseen task-performing procedures and generalize their execution to other contexts. In this work, we introduce DescribeWorld, an environment designed to test this sort of generalization skill in grounded agents, where tasks are linguistically and procedurally composed of elementary concepts. The agent observes a single task demonstration in a Minecraft-like grid world, and is then asked to carry out the same task in a new map. To enable such a level of generalization, we propose a neural agent infused with hierarchical latent language--both at the level of task inference and subtask planning. Our agent first generates a textual description of the demonstrated unseen task, then leverages this description to replicate it. Through multiple evaluation scenarios and a suite of generalization tests, we find that agents that perform text-based inference are better equipped for the challenge under a random split of tasks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2203.04806},
  eprint        = {2203.04806},
  file          = {:Weir2022 - One Shot Learning from a Demonstration with Hierarchical Latent Language.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio3},
  publisher     = {arXiv},
}

@Article{Haarnoja2023,
  author        = {Haarnoja, Tuomas and Moran, Ben and Lever, Guy and Huang, Sandy H. and Tirumala, Dhruva and Wulfmeier, Markus and Humplik, Jan and Tunyasuvunakool, Saran and Siegel, Noah Y. and Hafner, Roland and Bloesch, Michael and Hartikainen, Kristian and Byravan, Arunkumar and Hasenclever, Leonard and Tassa, Yuval and Sadeghi, Fereshteh and Batchelor, Nathan and Casarini, Federico and Saliceti, Stefano and Game, Charles and Sreendra, Neil and Patel, Kushal and Gwira, Marlon and Huber, Andrea and Hurley, Nicole and Nori, Francesco and Hadsell, Raia and Heess, Nicolas},
  title         = {Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning},
  year          = {2023},
  month         = apr,
  abstract      = {We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2304.13653},
  eprint        = {2304.13653},
  file          = {:Haarnoja2023 - Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning.pdf:PDF},
  groups        = {RL in Robotics},
  keywords      = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  priority      = {prio1},
  publisher     = {arXiv},
  ranking       = {rank2},
}







@Article{Liu2022,
  author   = {Siqi Liu and Guy Lever and Zhe Wang and Josh Merel and S. M. Ali Eslami and Daniel Hennes and Wojciech M. Czarnecki and Yuval Tassa and Shayegan Omidshafiei and Abbas Abdolmaleki and Noah Y. Siegel and Leonard Hasenclever and Luke Marris and Saran Tunyasuvunakool and H. Francis Song and Markus Wulfmeier and Paul Muller and Tuomas Haarnoja and Brendan Tracey and Karl Tuyls and Thore Graepel and Nicolas Heess},
  journal  = {Science Robotics},
  title    = {From motor control to team play in simulated humanoid football},
  year     = {2022},
  number   = {69},
  pages    = {eabo0235},
  volume   = {7},
  abstract = {Learning to combine control at the level of joint torques with longer-term goal-directed behavior is a long-standing challenge for physically embodied artificial agents. Intelligent behavior in the physical world unfolds across multiple spatial and temporal scales: Although movements are ultimately executed at the level of instantaneous muscle tensions or joint torques, they must be selected to serve goals that are defined on much longer time scales and that often involve complex interactions with the environment and other agents. Recent research has demonstrated the potential of learning-based approaches applied to the respective problems of complex movement, long-term planning, and multiagent coordination. However, their integration traditionally required the design and optimization of independent subsystems and remains challenging. In this work, we tackled the integration of motor control and long-horizon decision-making in the context of simulated humanoid football, which requires agile motor control and multiagent coordination. We optimized teams of agents to play simulated football via reinforcement learning, constraining the solution space to that of plausible movements learned using human motion capture data. They were trained to maximize several environment rewards and to imitate pretrained football-specific skills if doing so led to improved performance. The result is a team of coordinated humanoid football players that exhibit complex behavior at different scales, quantified by a range of analysis and statistics, including those used in real-world sport analytics. Our work constitutes a complete demonstration of learned integrated decision-making at multiple scales in a multiagent setting. In simulated humanoid football, teams of agents seamlessly integrate motor control with team play via end-to-end learning.},
  doi      = {10.1126/scirobotics.abo0235},
  eprint   = {https://www.science.org/doi/pdf/10.1126/scirobotics.abo0235},
  file     = {:Liu2022 - From Motor Control to Team Play in Simulated Humanoid Football.pdf:PDF},
  groups   = {RL in Robotics},
  priority = {prio1},
  ranking  = {rank3},
  url      = {https://www.science.org/doi/abs/10.1126/scirobotics.abo0235},
}

@Article{matignon_laurent_lefort-piat_2012,
  author    = {Matignon, Laetitia and Laurent, Guillaume J. and Le Fort-Piat, Nadine},
  journal   = {The Knowledge Engineering Review},
  title     = {Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems},
  year      = {2012},
  number    = {1},
  pages     = {1–31},
  volume    = {27},
  doi       = {10.1017/S0269888912000057},
  file      = {:matignon_laurent_lefort-piat_2012 - Independent Reinforcement Learners in Cooperative Markov Games_ a Survey Regarding Coordination Problems.pdf:PDF},
  groups    = {Multi-agent learning, MAS Reviews, Independent Learning},
  priority  = {prio1},
  publisher = {Cambridge University Press},
}

@Article{Claus1998_CoopMARL,
  author  = {Claus, Caroline and Boutilier, Craig},
  journal = {AAAI/IAAI},
  title   = {The dynamics of reinforcement learning in cooperative multiagent systems},
  year    = {1998},
  number  = {746-752},
  pages   = {2},
  volume  = {1998},
  file    = {:Claus1998_IQL - The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf:PDF:https\://cdn.aaai.org/AAAI/1998/AAAI98-106.pdf},
  groups  = {Multi-agent RL, Joint Action Learning, Independent Learning},
  ranking = {rank4},
  url     = {https://cdn.aaai.org/AAAI/1998/AAAI98-106.pdf},
}

@InProceedings{Ouyang2022_InstructGPT,
  author     = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Training language models to follow instructions with human feedback},
  year       = {2022},
  editor     = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages      = {27730--27744},
  publisher  = {Curran Associates, Inc.},
  volume     = {35},
  comment    = {InstructGPT
ChatGPT},
  file       = {:Ouyang2022 - Training Language Models to Follow Instructions with Human Feedback.pdf:PDF;:Ouyang2022 - Training Language Models to Follow Instructions with Human Feedback (1).pdf:PDF},
  groups     = {Question Answering, Language generation with RL, RLHF, Large Language Models},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
}

@Article{Jaques2019a,
  author        = {Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  title         = {Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog},
  year          = {2019},
  month         = jun,
  abstract      = {Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. These are critical shortcomings for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment -- e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms, which are able to effectively learn offline, without exploring, from a fixed batch of human interaction data. We leverage models pre-trained on data as a strong prior, and use KL-control to penalize divergence from this prior during RL training. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. The algorithms are tested on the problem of open-domain dialog generation -- a challenging reinforcement learning problem with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we can extract multiple different reward functions post-hoc from collected human interaction data, and learn effectively from all of these. We test the real-world generalization of these systems by deploying them live to converse with humans in an open-domain setting, and demonstrate that our algorithm achieves significant improvements over prior methods in off-policy batch RL.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1907.00456},
  eprint        = {1907.00456},
  file          = {:Jaques2019a - Way off Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog.pdf:PDF},
  groups        = {Language generation with RL, RLHF},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank2},
  readstatus    = {skimmed},
}

@Article{Saleh2020,
  author     = {Abdelrhman Saleh and Natasha Jaques and Asma Ghandeharioun and Judy Shen and Rosalind Picard},
  journal    = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title      = {Hierarchical Reinforcement Learning for Open-Domain Dialog},
  year       = {2020},
  month      = {apr},
  number     = {05},
  pages      = {8741--8748},
  volume     = {34},
  doi        = {10.1609/aaai.v34i05.6400},
  file       = {:Saleh2020 - Hierarchical Reinforcement Learning for Open Domain Dialog.pdf:PDF},
  groups     = {Language generation with RL},
  publisher  = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  ranking    = {rank1},
  readstatus = {skimmed},
}

@Article{Ziegler2019,
  author        = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  title         = {Fine-Tuning Language Models from Human Preferences},
  year          = {2019},
  month         = sep,
  abstract      = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  archiveprefix = {arXiv},
  comment       = {Probably used further in InstructGPT (Ouyang et al., 2022), provides more details on the approach},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1909.08593},
  eprint        = {1909.08593},
  file          = {:Ziegler2019 - Fine Tuning Language Models from Human Preferences.pdf:PDF},
  groups        = {Language generation with RL, RLHF},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
  ranking       = {rank3},
  readstatus    = {skimmed},
}

@Article{Lin2023,
  author        = {Lin, Jessy and Du, Yuqing and Watkins, Olivia and Hafner, Danijar and Abbeel, Pieter and Klein, Dan and Dragan, Anca},
  title         = {Learning to Model the World with Language},
  year          = {2023},
  month         = jul,
  abstract      = {To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2308.01399},
  eprint        = {2308.01399},
  file          = {:Lin2023 - Learning to Model the World with Language.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio1},
  publisher     = {arXiv},
}

@Article{Brohan2023,
  author   = {Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong,Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich},
  title    = {RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
  year     = {2023},
  file     = {:Brohan2023 - RT 2_ Vision Language Action Models Transfer Web Knowledge to Robotic Control.pdf:PDF},
  groups   = {Visual-Language Learning},
  priority = {prio1},
}

@Article{Wu2023_AutoGen,
  author        = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W and Burger, Doug and Wang, Chi},
  title         = {AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  year          = {2023},
  month         = aug,
  abstract      = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2308.08155},
  eprint        = {2308.08155},
  file          = {:Wu2023 - AutoGen_ Enabling Next Gen LLM Applications Via Multi Agent Conversation.pdf:PDF},
  groups        = {Large Language Models, LLM towns, Comm with LLMs},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  ranking       = {rank3},
}

@InProceedings{Carta2023_GLAM,
  author      = {Carta, Thomas and Romac, Cl{\'e}ment and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  booktitle   = {Proceedings of Machine Learning Research},
  title       = {{Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning}},
  year        = {2023},
  number      = {3676-3713},
  publisher   = {{PMLR}},
  volume      = {202},
  file        = {:Carta2023 - Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning.pdf:PDF},
  groups      = {Language-Augmented RL, Agent-based LLMs},
  hal_id      = {hal-03970122},
  hal_version = {v3},
  pdf         = {https://hal.science/hal-03970122v3/file/extended_version.pdf},
  ranking     = {rank3},
  readstatus  = {read},
  url         = {https://hal.science/hal-03970122},
}

@Article{Ahn2022,
  author        = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
  title         = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
  year          = {2022},
  month         = apr,
  abstract      = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2204.01691},
  eprint        = {2204.01691},
  file          = {:Ahn2022 - Do As I Can, Not As I Say_ Grounding Language in Robotic Affordances.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Robotics (cs.RO), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
  ranking       = {rank4},
  readstatus    = {read},
}

@InProceedings{Rana2023_CLASP,
  author     = {Krishan Rana and Andrew Melnik and Niko Suenderhauf},
  booktitle  = {ICRA2023 Workshop on Pretraining for Robotics (PT4R)},
  title      = {Contrastive Language, Action, and State Pre-training for Robot Learning},
  year       = {2023},
  file       = {:Rana2023_CLASP - Contrastive Language, Action, and State Pre Training for Robot Learning.pdf:PDF},
  groups     = {Language-Augmented RL},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=sxKR6zhBDH},
}

@Article{Karamcheti2023,
  author        = {Karamcheti, Siddharth and Nair, Suraj and Chen, Annie S. and Kollar, Thomas and Finn, Chelsea and Sadigh, Dorsa and Liang, Percy},
  title         = {Language-Driven Representation Learning for Robotics},
  year          = {2023},
  month         = feb,
  abstract      = {Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems $\unicode{x2013}$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2302.12766},
  eprint        = {2302.12766},
  file          = {:Karamcheti2023 - Language Driven Representation Learning for Robotics.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
  readstatus    = {read},
}

@InProceedings{Driess2023_PaLME,
  author     = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  booktitle  = {Proceedings of the 40th International Conference on Machine Learning},
  title      = {PaLM-E: An Embodied Multimodal Language Model},
  year       = {2023},
  editor     = {PMLR},
  volume     = {202},
  abstract   = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  doi        = {10.48550/ARXIV.2303.03378},
  eprint     = {2303.03378},
  file       = {:Driess2023 - PaLM E_ an Embodied Multimodal Language Model.pdf:PDF},
  groups     = {Large Language Models, Visual-and-Language Navigation, Agent-based LLMs},
  keywords   = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences},
  ranking    = {rank4},
  readstatus = {read},
}

@InProceedings{Dosovitskiy2021_VisionTransformer,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {International Conference on Learning Representations},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year      = {2021},
  file      = {:Dosovitskiy2021 - An Image Is Worth 16x16 Words_ Transformers for Image Recognition at Scale.pdf:PDF},
  groups    = {Computer Vision},
  ranking   = {rank5},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
}

@Article{Kalashnikov2021,
  author        = {Kalashnikov, Dmitry and Varley, Jacob and Chebotar, Yevgen and Swanson, Benjamin and Jonschkowski, Rico and Finn, Chelsea and Levine, Sergey and Hausman, Karol},
  title         = {MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale},
  year          = {2021},
  month         = apr,
  abstract      = {General-purpose robotic systems must master a large repertoire of diverse skills to be useful in a range of daily tasks. While reinforcement learning provides a powerful framework for acquiring individual behaviors, the time needed to acquire each skill makes the prospect of a generalist robot trained with RL daunting. In this paper, we study how a large-scale collective robotic learning system can acquire a repertoire of behaviors simultaneously, sharing exploration, experience, and representations across tasks. In this framework new tasks can be continuously instantiated from previously learned tasks improving overall performance and capabilities of the system. To instantiate this system, we develop a scalable and intuitive framework for specifying new tasks through user-provided examples of desired outcomes, devise a multi-robot collective learning system for data collection that simultaneously collects experience for multiple tasks, and develop a scalable and generalizable multi-task deep reinforcement learning method, which we call MT-Opt. We demonstrate how MT-Opt can learn a wide range of skills, including semantic picking (i.e., picking an object from a particular category), placing into various fixtures (e.g., placing a food item onto a plate), covering, aligning, and rearranging. We train and evaluate our system on a set of 12 real-world tasks with data collected from 7 robots, and demonstrate the performance of our system both in terms of its ability to generalize to structurally similar new tasks, and acquire distinct new tasks more quickly by leveraging past experience. We recommend viewing the videos at https://karolhausman.github.io/mt-opt/},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2104.08212},
  eprint        = {2104.08212},
  file          = {:Kalashnikov2021 - MT Opt_ Continuous Multi Task Robotic Reinforcement Learning at Scale.pdf:PDF},
  groups        = {RL in Robotics},
  keywords      = {Robotics (cs.RO), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
  ranking       = {rank3},
}





@Article{Tellex2020,
  author   = {Tellex, Stefanie and Gopalan, Nakul and Kress-Gazit, Hadas and Matuszek, Cynthia},
  journal  = {Annual Review of Control, Robotics, and Autonomous Systems},
  title    = {Robots That Use Language},
  year     = {2020},
  number   = {1},
  pages    = {25-55},
  volume   = {3},
  abstract = {This article surveys the use of natural language in robotics from a robotics point of view. To use human language, robots must map words to aspects of the physical world, mediated by the robot's sensors and actuators. This problem differs from other natural language processing domains due to the need to ground the language to noisy percepts and physical actions. Here, we describe central aspects of language use by robots, including understanding natural language requests, using language to drive learning about the physical world, and engaging in collaborative dialogue with a human partner. We describe common approaches, roughly divided into learning methods, logic-based methods, and methods that focus on questions of human–robot interaction. Finally, we describe several application domains for language-using robots.},
  doi      = {10.1146/annurev-control-101119-071628},
  eprint   = {https://doi.org/10.1146/annurev-control-101119-071628},
  file     = {:Tellex2020 - Robots That Use Language.pdf:PDF},
  groups   = {Robotics},
  priority = {prio1},
  ranking  = {rank2},
  url      = {https://doi.org/10.1146/annurev-control-101119-071628},
}

@InProceedings{Corbiere2019,
  author    = {Corbi\`{e}re, Charles and THOME, Nicolas and Bar-Hen, Avner and Cord, Matthieu and P\'{e}rez, Patrick},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Addressing Failure Prediction by Learning Model Confidence},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  file      = {:Corbiere2019 - Addressing Failure Prediction by Learning Model Confidence.pdf:PDF},
  groups    = {Confidence},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/757f843a169cc678064d9530d12a1881-Paper.pdf},
}

@Article{Huang2023,
  author        = {Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and Ichter, Brian},
  title         = {Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents},
  year          = {2023},
  month         = mar,
  abstract      = {Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models. The project's website can be found at grounded-decoding.github.io.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2303.00855},
  eprint        = {2303.00855},
  file          = {:Huang2023 - Grounded Decoding_ Guiding Text Generation with Grounded Models for Embodied Agents.pdf:PDF},
  groups        = {Language-Augmented RL},
  keywords      = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
  ranking       = {rank2},
}

@Article{Montague2012,
  author    = {Montague, P. Read and Dolan, Raymond J. and Friston, Karl J. and Dayan, Peter},
  journal   = {Trends in Cognitive Sciences},
  title     = {Computational psychiatry},
  year      = {2012},
  issn      = {1364-6613},
  month     = jan,
  number    = {1},
  pages     = {72--80},
  volume    = {16},
  doi       = {10.1016/j.tics.2011.11.018},
  groups    = {RL in Biology and Neuroscience},
  publisher = {Elsevier BV},
  ranking   = {rank3},
}

@Article{Schultz1997,
  author    = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  journal   = {Science},
  title     = {A Neural Substrate of Prediction and Reward},
  year      = {1997},
  issn      = {1095-9203},
  month     = mar,
  number    = {5306},
  pages     = {1593--1599},
  volume    = {275},
  doi       = {10.1126/science.275.5306.1593},
  groups    = {RL in Biology and Neuroscience},
  publisher = {American Association for the Advancement of Science (AAAS)},
  ranking   = {rank5},
}

@Article{Friston2010_FreeEnergy,
  author    = {Friston, Karl},
  journal   = {Nature Reviews Neuroscience},
  title     = {The free-energy principle: a unified brain theory?},
  year      = {2010},
  issn      = {1471-0048},
  month     = jan,
  number    = {2},
  pages     = {127--138},
  volume    = {11},
  doi       = {10.1038/nrn2787},
  groups    = {RL in Biology and Neuroscience},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank5},
}

@InBook{Rescorla1972_Pavlovian,
  author  = {Rescorla, RA and Wagner, Allan},
  pages   = {64-99},
  title   = {A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement},
  year    = {1972},
  month   = {01},
  volume  = {Vol. 2},
  file    = {:RescorlaWagner.1972.pdf:PDF},
  groups  = {RL in Biology and Neuroscience},
  journal = {Classical Conditioning II: Current Research and Theory},
  ranking = {rank5},
}

@Article{Brembs2010_FreeWill,
  author    = {Brembs, Björn},
  journal   = {Proceedings of the Royal Society B: Biological Sciences},
  title     = {Towards a scientific concept of free will as a biological trait: spontaneous actions and decision-making in invertebrates},
  year      = {2010},
  issn      = {1471-2954},
  month     = dec,
  number    = {1707},
  pages     = {930--939},
  volume    = {278},
  doi       = {10.1098/rspb.2010.2325},
  file      = {:brembs-2010-towards-a-scientific-concept-of-free-will-as-a-biological-trait-spontaneous-actions-and-decision-making-in.pdf:PDF},
  groups    = {RL in Biology and Neuroscience},
  publisher = {The Royal Society},
  ranking   = {rank3},
}

@Article{Gardner1984_Chimpanzee,
  author    = {Gardner, R. Allen and Gardner, Beatrix T.},
  journal   = {Journal of Comparative Psychology},
  title     = {A vocabulary test for chimpanzees (Pan troglodytes).},
  year      = {1984},
  issn      = {0735-7036},
  number    = {4},
  pages     = {381--404},
  volume    = {98},
  doi       = {10.1037/0735-7036.98.4.381},
  file      = {:gardner1984.pdf:PDF},
  groups    = {RL in Biology and Neuroscience},
  publisher = {American Psychological Association (APA)},
  ranking   = {rank3},
}

@Article{Moerland2023,
  author    = {Moerland, Thomas M. and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M.},
  journal   = {Foundations and Trends® in Machine Learning},
  title     = {Model-based Reinforcement Learning: A Survey},
  year      = {2023},
  issn      = {1935-8245},
  number    = {1},
  pages     = {1--118},
  volume    = {16},
  doi       = {10.1561/2200000086},
  file      = {:Moerland2023 - Model Based Reinforcement Learning_ a Survey.pdf:PDF},
  groups    = {Model based},
  publisher = {Now Publishers},
  ranking   = {rank4},
}

@Article{Hafner2023,
  author        = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  title         = {Mastering Diverse Domains through World Models},
  year          = {2023},
  month         = jan,
  abstract      = {General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2301.04104},
  eprint        = {2301.04104},
  file          = {:Hafner2023 - Mastering Diverse Domains through World Models.pdf:PDF},
  groups        = {Model based},
  keywords      = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  ranking       = {rank3},
}

@Article{Watkins1992,
  author    = {Watkins, Christopher J. C. H. and Dayan, Peter},
  journal   = {Machine Learning},
  title     = {Q-learning},
  year      = {1992},
  issn      = {1573-0565},
  month     = may,
  number    = {3–4},
  pages     = {279--292},
  volume    = {8},
  doi       = {10.1007/bf00992698},
  file      = {:Watkins1992 - Q Learning.pdf:PDF},
  groups    = {RL},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank5},
}

@Book{Sutton2018_RL,
  author     = {Richard S. Sutton and Andrew G. Barto},
  publisher  = {The MIT Press},
  title      = {Reinforcement Learning: An Introduction, second edition},
  year       = {2018},
  file       = {:Sutton2018_RL - Reinforcement Learning_ an Introduction, Second Edition.pdf:PDF},
  groups     = {RL},
  ranking    = {rank5},
  readstatus = {read},
  url        = {http://incompleteideas.net/sutton/book/RLbook2018.pdf},
}

@PhdThesis{Watkins1989_RL,
  author         = {Watkins, Christopher J. C. H.},
  title          = {Learning from Delayed Rewards},
  year           = {1989},
  comment-doctoq = {Q-learning Model-based RL},
  groups         = {RL},
  journal        = {PhD thesis, Cambridge University, Cambridge, England},
  ranking        = {rank5},
  url            = {https://cir.nii.ac.jp/crid/1573105974761345280},
}

@Article{Minsky1961,
  author    = {Minsky, Marvin},
  journal   = {Proceedings of the IRE},
  title     = {Steps toward Artificial Intelligence},
  year      = {1961},
  issn      = {0096-8390},
  month     = jan,
  number    = {1},
  pages     = {8--30},
  volume    = {49},
  doi       = {10.1109/jrproc.1961.287775},
  file      = {:Minsky1961 - Steps toward Artificial Intelligence.pdf:PDF},
  groups    = {RL, Artificial Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank5},
}

@Article{Andreae1969,
  author    = {Andreae, J.H. and Cashin, P.M.},
  journal   = {International Journal of Man-Machine Studies},
  title     = {A learning machine with monologue},
  year      = {1969},
  issn      = {0020-7373},
  month     = jan,
  number    = {1},
  pages     = {1--20},
  volume    = {1},
  comment   = {STeLLA, inner monologue},
  doi       = {10.1016/s0020-7373(69)80008-8},
  groups    = {RL},
  publisher = {Elsevier BV},
}

@Unpublished{Andreae2017,
  author = {John H. Andreae},
  title  = {Working memory for the associative learning of language},
  year   = {2017},
  file   = {:Andreae2017 - Working Memory for the Associative Learning of Language.pdf:PDF},
  groups = {RL},
  url    = {http://www.incompleteideas.net/papers/Andreae-2017b.pdf},
}

@Article{Silver2021_RewardEnough,
  author    = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  journal   = {Artificial Intelligence},
  title     = {Reward is enough},
  year      = {2021},
  issn      = {0004-3702},
  month     = oct,
  pages     = {103535},
  volume    = {299},
  doi       = {10.1016/j.artint.2021.103535},
  file      = {:Silver2021 - Reward Is Enough.pdf:PDF},
  groups    = {RL, Artificial Intelligence},
  publisher = {Elsevier BV},
  ranking   = {rank4},
}

@InProceedings{Vahdat2020,
  author    = {Vahdat, Arash and Kautz, Jan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {NVAE: A Deep Hierarchical Variational Autoencoder},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {19667--19679},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  file      = {:Vahdat2020 - NVAE_ a Deep Hierarchical Variational Autoencoder.pdf:PDF},
  groups    = {Computer Vision, Auto-Encoding},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e3b21256183cf7c2c7a66be163579d37-Paper.pdf},
}

@Article{Andrychowicz2020a,
  author        = {Andrychowicz, Marcin and Raichuk, Anton and Stańczyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, Léonard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and Gelly, Sylvain and Bachem, Olivier},
  title         = {What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study},
  year          = {2020},
  month         = jun,
  abstract      = {In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.},
  archiveprefix = {arXiv},
  comment       = {PPO},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2006.05990},
  eprint        = {2006.05990},
  file          = {:https___doi.org_10.48550_arxiv.2006.05990 - What Matters in on Policy Reinforcement Learning_ a Large Scale Empirical Study.pdf:PDF},
  groups        = {Policy based, Implementation tricks in RL},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank2},
}

@InProceedings{Mitchell2021_Harder,
  author     = {Mitchell, Melanie},
  booktitle  = {Proceedings of the Genetic and Evolutionary Computation Conference},
  title      = {Why AI is harder than we think},
  year       = {2021},
  month      = jun,
  publisher  = {ACM},
  series     = {GECCO ’21},
  collection = {GECCO ’21},
  doi        = {10.1145/3449639.3465421},
  file       = {:Mitchell2021 - Why AI Is Harder Than We Think.pdf:PDF},
  groups     = {Artificial Intelligence},
  ranking    = {rank2},
}

@Article{Samuel1959_Checkers,
  author    = {Samuel, A. L.},
  journal   = {IBM Journal of Research and Development},
  title     = {Some Studies in Machine Learning Using the Game of Checkers},
  year      = {1959},
  issn      = {0018-8646},
  month     = jul,
  number    = {3},
  pages     = {210--229},
  volume    = {3},
  comment   = {Checkers with temporal difference learning},
  doi       = {10.1147/rd.33.0210},
  file      = {:Samuel1959 - Some Studies in Machine Learning Using the Game of Checkers.pdf:PDF;:Samuel1959 - Some Studies in Machine Learning Using the Game of Checkers (1).pdf:PDF},
  groups    = {RL},
  publisher = {IBM},
  ranking   = {rank5},
}

@Article{Barto1983_ActorCritic,
  author    = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics},
  title     = {Neuronlike adaptive elements that can solve difficult learning control problems},
  year      = {1983},
  issn      = {2168-2909},
  month     = sep,
  number    = {5},
  pages     = {834--846},
  volume    = {SMC-13},
  comment   = {first Actor-Critic architecture},
  doi       = {10.1109/tsmc.1983.6313077},
  file      = {:Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems Barto1983.pdf:PDF},
  groups    = {RL, Actor Critic},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank5},
}

@Article{Sutton1988_TD,
  author    = {Sutton, Richard S.},
  journal   = {Machine Learning},
  title     = {Learning to predict by the methods of temporal differences},
  year      = {1988},
  issn      = {1573-0565},
  month     = aug,
  number    = {1},
  pages     = {9--44},
  volume    = {3},
  comment   = {TD(lambda)},
  doi       = {10.1007/bf00115009},
  file      = {:Sutton1988 - Learning to Predict by the Methods of Temporal Differences.pdf:PDF},
  groups    = {RL},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank5},
}

@Article{Tesauro1994,
  author    = {Tesauro, Gerald},
  journal   = {Neural Computation},
  title     = {TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play},
  year      = {1994},
  issn      = {1530-888X},
  month     = mar,
  number    = {2},
  pages     = {215--219},
  volume    = {6},
  doi       = {10.1162/neco.1994.6.2.215},
  file      = {:Tesauro1994 - TD Gammon, a Self Teaching Backgammon Program, Achieves Master Level Play.pdf:PDF},
  groups    = {RL},
  publisher = {MIT Press - Journals},
  ranking   = {rank4},
}

@Article{Michie1968_Boxes,
  author    = {Michie, Donald and Chambers, Roger A},
  journal   = {Machine intelligence},
  title     = {BOXES: An experiment in adaptive control},
  year      = {1968},
  number    = {2},
  pages     = {137--152},
  volume    = {2},
  file      = {:Michie1968_Boxes - BOXES_ an Experiment in Adaptive Control.pdf:PDF},
  groups    = {RL},
  publisher = {Citeseer},
  ranking   = {rank3},
  url       = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=2f027193fb703d0af58ec382bd1438daff9417d7},
}

@Article{Bellemare2013_Atari,
  author    = {Bellemare, M. G. and Naddaf, Y. and Veness, J. and Bowling, M.},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
  year      = {2013},
  issn      = {1076-9757},
  month     = jun,
  pages     = {253--279},
  volume    = {47},
  doi       = {10.1613/jair.3912},
  file      = {:10819-Article Text-20168-1-10-20180216.pdf:PDF},
  groups    = {RL, Environments},
  publisher = {AI Access Foundation},
  ranking   = {rank5},
}

@Article{Guss2019_MineRL,
  author        = {Guss, William H. and Codel, Cayden and Hofmann, Katja and Houghton, Brandon and Kuno, Noboru and Milani, Stephanie and Mohanty, Sharada and Liebana, Diego Perez and Salakhutdinov, Ruslan and Topin, Nicholay and Veloso, Manuela and Wang, Phillip},
  title         = {The MineRL 2019 Competition on Sample Efficient Reinforcement Learning using Human Priors},
  year          = {2019},
  month         = apr,
  abstract      = {Though deep reinforcement learning has led to breakthroughs in many difficult domains, these successes have required an ever-increasing number of samples. As state-of-the-art reinforcement learning (RL) systems require an exponentially increasing number of samples, their development is restricted to a continually shrinking segment of the AI community. Likewise, many of these systems cannot be applied to real-world problems, where environment samples are expensive. Resolution of these limitations requires new, sample-efficient methods. To facilitate research in this direction, we introduce the MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors. The primary goal of the competition is to foster the development of algorithms which can efficiently leverage human demonstrations to drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments. To that end, we introduce: (1) the Minecraft ObtainDiamond task, a sequential decision making environment requiring long-term planning, hierarchical control, and efficient exploration methods; and (2) the MineRL-v0 dataset, a large-scale collection of over 60 million state-action pairs of human demonstrations that can be resimulated into embodied trajectories with arbitrary modifications to game state and visuals. Participants will compete to develop systems which solve the ObtainDiamond task with a limited number of samples from the environment simulator, Malmo. The competition is structured into two rounds in which competitors are provided several paired versions of the dataset and environment with different game textures. At the end of each round, competitors will submit containerized versions of their learning algorithms and they will then be trained/evaluated from scratch on a hold-out dataset-environment pair for a total of 4-days on a prespecified hardware platform.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.1904.10079},
  eprint        = {1904.10079},
  file          = {:Guss2019 - The MineRL 2019 Competition on Sample Efficient Reinforcement Learning Using Human Priors.pdf:PDF},
  groups        = {Environments},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank1},
}

@InProceedings{Guss2019_MineRLDataset,
  author    = {Guss, William H. and Houghton, Brandon and Topin, Nicholay and Wang, Phillip and Codel, Cayden and Veloso, Manuela and Salakhutdinov, Ruslan},
  booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  title     = {MineRL: a large-scale dataset of minecraft demonstrations},
  year      = {2019},
  pages     = {2442–2448},
  publisher = {AAAI Press},
  series    = {IJCAI'19},
  abstract  = {The sample inefficiency of standard deep reinforcement learning methods precludes their application to many real-world problems. Methods which leverage human demonstrations require fewer samples but have been researched less. As demonstrated in the computer vision and natural language processing communities, large-scale datasets have the capacity to facilitate research by serving as an experimental and benchmarking platform for new methods. However, existing datasets compatible with reinforcement learning simulators do not have sufficient scale, structure, and quality to enable the further development and evaluation of methods focused on using human examples. Therefore, we introduce a comprehensive, large-scale, simulator-paired dataset of human demonstrations: MineRL. The dataset consists of over 60 million automatically annotated state-action pairs across a variety of related tasks in Minecraft, a dynamic, 3D, open-world environment. We present a novel data collection scheme which allows for the ongoing introduction of new tasks and the gathering of complete state information suitable for a variety of methods. We demonstrate the hierarchality, diversity, and scale of the MineRL dataset. Further, we show the difficulty of the Minecraft domain along with the potential of MineRL in developing techniques to solve key research challenges within it.},
  file      = {:Guss2019_MineRLDataset - MineRL_ a Large Scale Dataset of Minecraft Demonstrations.pdf:PDF},
  groups    = {Environments},
  isbn      = {9780999241141},
  location  = {Macao, China},
  numpages  = {7},
  ranking   = {rank2},
}

@PhdThesis{Laud2004_Shaping,
  author  = {Laud, Adam Daniel},
  school  = {University of Illinois at Urbana-Champaign},
  title   = {Theory and Application of Reward Shaping in Reinforcement Learning},
  year    = {2004},
  file    = {:Theory and Application of Reward Shaping in Reinforcement Learning.pdf:PDF},
  groups  = {RL, Reward Shaping},
  ranking = {rank2},
}

@InProceedings{Tsitsiklis1996,
  author    = {Tsitsiklis, John and Van Roy, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Analysis of Temporal-Diffference Learning with Function Approximation},
  year      = {1996},
  editor    = {M.C. Mozer and M. Jordan and T. Petsche},
  publisher = {MIT Press},
  volume    = {9},
  file      = {:Tsitsiklis1996 - Analysis of Temporal Diffference Learning with Function Approximation.pdf:PDF},
  groups    = {RL, Value based},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1996/file/e00406144c1e7e35240afed70f34166a-Paper.pdf},
}

@Book{Busoniu2017_FuncApprox,
  author    = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart and Ernst, Damien},
  publisher = {CRC press},
  title     = {Reinforcement learning and dynamic programming using function approximators},
  year      = {2017},
  file      = {:busoniu2017reinforcement - Reinforcement Learning and Dynamic Programming Using Function Approximators.pdf:PDF},
  groups    = {RL},
  ranking   = {rank5},
  url       = {https://orbi.uliege.be/bitstream/2268/27963/1/book-FA-RL-DP.pdf},
}

@Book{Bishop2006_Pattern,
  author    = {Bishop, Christopher M.},
  publisher = {Springer New York, NY},
  title     = {Pattern recognition and machine learning},
  year      = {2006},
  isbn      = {978-0-387-31073-2},
  file      = {:Bishop2006_Pattern - Pattern Recognition and Machine Learning.pdf:PDF},
  groups    = {Machine Learning},
  ranking   = {rank5},
  url       = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
}

@InProceedings{Bagnell2001_Helicopter,
  author    = {Bagnell, J.A. and Schneider, J.G.},
  booktitle = {Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)},
  title     = {Autonomous helicopter control using reinforcement learning policy search methods},
  year      = {2001},
  pages     = {1615-1620 vol.2},
  volume    = {2},
  doi       = {10.1109/ROBOT.2001.932842},
  file      = {:file.pdf:PDF},
  groups    = {Model based, RL, POMDP, RL in Robotics, Navigation},
  keywords  = {Helicopters;Search methods;Robots;Robust control;Learning systems;Optimal control;Uncertainty;Control system synthesis;Artificial intelligence;Computer crashes},
  ranking   = {rank3},
}

@InProceedings{Abbeel2006,
  author    = {Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {An Application of Reinforcement Learning to Aerobatic Helicopter Flight},
  year      = {2006},
  editor    = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
  publisher = {MIT Press},
  volume    = {19},
  file      = {:Abbeel2006 - An Application of Reinforcement Learning to Aerobatic Helicopter Flight.pdf:PDF},
  groups    = {RL, RL in Robotics, Navigation},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper/2006/hash/98c39996bf1543e974747a2549b3107c-Abstract.html},
}

@InProceedings{Sumers2022a,
  author    = {Sumers, Theodore and Hawkins, Robert and Ho, Mark K and Griffiths, Tom and Hadfield-Menell, Dylan},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {How to talk so AI will learn: Instructions, descriptions, and autonomy},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {34762--34775},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Sumers2022a - How to Talk so AI Will Learn_ Instructions, Descriptions, and Autonomy.pdf:PDF},
  groups    = {Language-Augmented RL},
  priority  = {prio1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e0cfde0ff720fa9674bb976e7f1b99d4-Paper-Conference.pdf},
}

@Article{Sigaud2023,
  author        = {Sigaud, Olivier and Baldassarre, Gianluca and Colas, Cedric and Doncieux, Stephane and Duro, Richard and Perrin-Gilbert, Nicolas and Santucci, Vieri Giuliano},
  title         = {A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents},
  year          = {2023},
  month         = nov,
  abstract      = {A lot of recent machine learning research papers have ``open-ended learning'' in their title. But very few of them attempt to define what they mean when using the term. Even worse, when looking more closely there seems to be no consensus on what distinguishes open-ended learning from related concepts such as continual learning, lifelong learning or autotelic learning. In this paper, we contribute to fixing this situation. After illustrating the genealogy of the concept and more recent perspectives about what it truly means, we outline that open-ended learning is generally conceived as a composite notion encompassing a set of diverse properties. In contrast with previous approaches, we propose to isolate a key elementary property of open-ended processes, which is to produce elements from time to time (e.g., observations, options, reward functions, and goals), over an infinite horizon, that are considered novel from an observer's perspective. From there, we build the notion of open-ended learning problems and focus in particular on the subset of open-ended goal-conditioned reinforcement learning problems in which agents can learn a growing repertoire of goal-driven skills. Finally, we highlight the work that remains to be performed to fill the gap between our elementary definition and the more involved notions of open-ended learning that developmental AI researchers may have in mind.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2311.00344},
  eprint        = {2311.00344},
  file          = {:Sigaud2023 - A Definition of Open Ended Learning Problems for Goal Conditioned Agents.pdf:PDF},
  groups        = {Open-Ended Learning},
  keywords      = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@InProceedings{Christiano2017_RLHF,
  author    = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Deep Reinforcement Learning from Human Preferences},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  file      = {:Christiano2017 - Deep Reinforcement Learning from Human Preferences.pdf:PDF},
  groups    = {RLHF},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
}

@Article{Leike2018_RewardModel,
  author        = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  title         = {Scalable agent alignment via reward modeling: a research direction},
  year          = {2018},
  month         = nov,
  abstract      = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
  archiveprefix = {arXiv},
  comment       = {Reward modelling},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.1811.07871},
  eprint        = {1811.07871},
  file          = {:Leike2018 - Scalable Agent Alignment Via Reward Modeling_ a Research Direction.pdf:PDF},
  groups        = {RLHF},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank3},
}

@InProceedings{Stiennon2020,
  author    = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning to summarize with human feedback},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {3008--3021},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  file      = {:Stiennon2020 - Learning to Summarize with Human Feedback.pdf:PDF},
  groups    = {RLHF},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf},
}

@Article{Casper2023,
  author        = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, Jérémy and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and Wang, Tony and Marks, Samuel and Segerie, Charbel-Raphaël and Carroll, Micah and Peng, Andi and Christoffersen, Phillip and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and Bıyık, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and Hadfield-Menell, Dylan},
  title         = {Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
  year          = {2023},
  month         = jul,
  abstract      = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2307.15217},
  eprint        = {2307.15217},
  file          = {:Casper2023 - Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.pdf:PDF},
  groups        = {RLHF},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  ranking       = {rank2},
}

@Article{Wu2021,
  author        = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  title         = {Recursively Summarizing Books with Human Feedback},
  year          = {2021},
  month         = sep,
  abstract      = {A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2109.10862},
  eprint        = {2109.10862},
  file          = {:Wu2021 - Recursively Summarizing Books with Human Feedback.pdf:PDF},
  groups        = {RLHF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
  ranking       = {rank2},
}

@Article{Menick2022,
  author        = {Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and McAleese, Nat},
  title         = {Teaching language models to support answers with verified quotes},
  year          = {2022},
  month         = mar,
  abstract      = {Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2203.11147},
  eprint        = {2203.11147},
  file          = {:Menick2022 - Teaching Language Models to Support Answers with Verified Quotes.pdf:PDF},
  groups        = {RLHF},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
  ranking       = {rank2},
}

@Article{Nakano2021,
  author        = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  title         = {WebGPT: Browser-assisted question-answering with human feedback},
  year          = {2021},
  month         = dec,
  abstract      = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2112.09332},
  eprint        = {2112.09332},
  file          = {:Nakano2021 - WebGPT_ Browser Assisted Question Answering with Human Feedback.pdf:PDF},
  groups        = {RLHF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
  ranking       = {rank3},
}

@InProceedings{Gao2023_RewardModel,
  author    = {Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {Scaling Laws for Reward Model Overoptimization},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {10835--10866},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart’s law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed “gold-standard” reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.},
  file      = {:Gao2023_RewardModel - Scaling Laws for Reward Model Overoptimization.pdf:PDF},
  groups    = {RLHF},
  pdf       = {https://proceedings.mlr.press/v202/gao23h/gao23h.pdf},
  ranking   = {rank2},
  url       = {https://proceedings.mlr.press/v202/gao23h.html},
}

@Article{Charpentier2021_Economics,
  author    = {Charpentier, Arthur and Élie, Romuald and Remlinger, Carl},
  journal   = {Computational Economics},
  title     = {Reinforcement Learning in Economics and Finance},
  year      = {2021},
  issn      = {1572-9974},
  month     = apr,
  number    = {1},
  pages     = {425--462},
  volume    = {62},
  doi       = {10.1007/s10614-021-10119-4},
  groups    = {RL in Economics},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank2},
}

@Article{Arthur1991_Economics,
  author    = {W. Brian Arthur},
  journal   = {The American Economic Review},
  title     = {Designing Economic Agents that Act like Human Agents: A Behavioral Approach to Bounded Rationality},
  year      = {1991},
  issn      = {00028282},
  number    = {2},
  pages     = {353--359},
  volume    = {81},
  groups    = {RL in Economics},
  publisher = {American Economic Association},
  ranking   = {rank3},
  url       = {http://www.jstor.org/stable/2006884},
  urldate   = {2024-02-29},
}

@Article{Teng2023,
  author    = {Teng, Siyu and Hu, Xuemin and Deng, Peng and Li, Bai and Li, Yuchen and Ai, Yunfeng and Yang, Dongsheng and Li, Lingxi and Xuanyuan, Zhe and Zhu, Fenghua and Chen, Long},
  journal   = {IEEE Transactions on Intelligent Vehicles},
  title     = {Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives},
  year      = {2023},
  issn      = {2379-8858},
  month     = jun,
  number    = {6},
  pages     = {3692--3711},
  volume    = {8},
  doi       = {10.1109/tiv.2023.3274536},
  file      = {:Teng2023 - Motion Planning for Autonomous Driving_ the State of the Art and Future Perspectives.pdf:PDF},
  groups    = {Autonomous Driving},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank2},
}

@Article{Chen2023_EndToEndDriving,
  author        = {Chen, Li and Wu, Penghao and Chitta, Kashyap and Jaeger, Bernhard and Geiger, Andreas and Li, Hongyang},
  title         = {End-to-end Autonomous Driving: Challenges and Frontiers},
  year          = {2023},
  month         = jun,
  abstract      = {The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. To facilitate future research, we maintain an active repository that contains up-to-date links to relevant literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2306.16927},
  eprint        = {2306.16927},
  file          = {:https___doi.org_10.48550_arxiv.2306.16927 - End to End Autonomous Driving_ Challenges and Frontiers.pdf:PDF},
  groups        = {Autonomous Driving},
  keywords      = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
  ranking       = {rank1},
}

@Article{Kiran2022_DRLDriving,
  author    = {Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Perez, Patrick},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  title     = {Deep Reinforcement Learning for Autonomous Driving: A Survey},
  year      = {2022},
  issn      = {1558-0016},
  month     = jun,
  number    = {6},
  pages     = {4909--4926},
  volume    = {23},
  doi       = {10.1109/tits.2021.3054625},
  groups    = {Autonomous Driving},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank4},
}

@InProceedings{Bengio2009_Curriculum,
  author     = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
  booktitle  = {Proceedings of the 26th Annual International Conference on Machine Learning},
  title      = {Curriculum learning},
  year       = {2009},
  month      = jun,
  publisher  = {ACM},
  series     = {ICML ’09},
  collection = {ICML ’09},
  doi        = {10.1145/1553374.1553380},
  file       = {:Bengio2009 - Curriculum Learning.pdf:PDF},
  groups     = {Curriculum Learning},
  ranking    = {rank5},
}

@Article{Colas2022,
  author     = {Colas, Cédric and Karch, Tristan and Moulin-Frier, Clément and Oudeyer, Pierre-Yves},
  journal    = {Nature Machine Intelligence},
  title      = {Language and culture internalization for human-like autotelic AI},
  year       = {2022},
  issn       = {2522-5839},
  month      = dec,
  number     = {12},
  pages      = {1068--1076},
  volume     = {4},
  abstract   = {Building autonomous agents able to grow open-ended repertoires of skills across their lives is a fundamental goal of artificial intelligence (AI). A promising developmental approach recommends the design of intrinsically motivated agents that learn new skills by generating and pursuing their own goals--autotelic agents. But despite recent progress, existing algorithms still show serious limitations in terms of goal diversity, exploration, generalization or skill composition. This Perspective calls for the immersion of autotelic agents into rich socio-cultural worlds, an immensely important attribute of our environment that shapes human cognition but is mostly omitted in modern AI. Inspired by the seminal work of Vygotsky, we propose Vygotskian autotelic agents--agents able to internalize their interactions with others and turn them into cognitive tools. We focus on language and show how its structure and informational content may support the development of new cognitive functions in artificial agents as it does in humans. We justify the approach by uncovering several examples of new artificial cognitive functions emerging from interactions between language and embodiment in recent works at the intersection of deep reinforcement learning and natural language processing. Looking forward, we highlight future opportunities and challenges for Vygotskian autotelic AI research, including the use of language models as cultural models supporting artificial cognitive development.},
  doi        = {10.1038/s42256-022-00591-4},
  file       = {:Colas2022a - Language and Culture Internalization for Human like Autotelic AI.pdf:PDF;:Colas2022 - Vygotskian Autotelic Artificial Intelligence_ Language and Culture Internalization for Human like AI.pdf:PDF},
  groups     = {Language-Augmented RL},
  priority   = {prio1},
  publisher  = {Springer Science and Business Media LLC},
  readstatus = {read},
  refid      = {Colas2022},
  url        = {https://doi.org/10.1038/s42256-022-00591-4},
}

@InProceedings{Todorov2012,
  author    = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title     = {MuJoCo: A physics engine for model-based control},
  year      = {2012},
  month     = oct,
  publisher = {IEEE},
  doi       = {10.1109/iros.2012.6386109},
  file      = {:MuJoCo-Aphysicsengineformodel-basedcontrol_Todorov2012.pdf:PDF},
  groups    = {Environments},
  ranking   = {rank5},
}

@Article{Tassa2018_DeepmindControl,
  author        = {Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and Lillicrap, Timothy and Riedmiller, Martin},
  title         = {DeepMind Control Suite},
  year          = {2018},
  month         = jan,
  abstract      = {The DeepMind Control Suite is a set of continuous control tasks with a standardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and modify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at https://www.github.com/deepmind/dm_control . A video summary of all tasks is available at http://youtu.be/rAai4QzcYbs .},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1801.00690},
  eprint        = {1801.00690},
  file          = {:Tassa2018 - DeepMind Control Suite.pdf:PDF},
  groups        = {Environments},
  keywords      = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  ranking       = {rank3},
}

@Article{Bellman1957_MDP,
  author    = {Richard Bellman},
  journal   = {Journal of Mathematics and Mechanics},
  title     = {A Markovian Decision Process},
  year      = {1957},
  issn      = {00959057, 19435274},
  number    = {5},
  pages     = {679--684},
  volume    = {6},
  groups    = {RL},
  publisher = {Indiana University Mathematics Department},
  ranking   = {rank5},
  url       = {http://www.jstor.org/stable/24900506},
  urldate   = {2024-03-04},
}

@Article{Vandenhende2021,
  author    = {Vandenhende, Simon and Georgoulis, Stamatios and Van Gansbeke, Wouter and Proesmans, Marc and Dai, Dengxin and Van Gool, Luc},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Multi-Task Learning for Dense Prediction Tasks: A Survey},
  year      = {2021},
  issn      = {1939-3539},
  pages     = {1--1},
  doi       = {10.1109/tpami.2021.3054719},
  file      = {:Vandenhende2021 - Multi Task Learning for Dense Prediction Tasks_ a Survey.pdf:PDF},
  groups    = {Multi-Task Learning},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank3},
}

@PhdThesis{Sutton1984_ActorCritic,
  author    = {Sutton, Richard Stuart},
  title     = {Temporal credit assignment in reinforcement learning},
  year      = {1984},
  comment   = {actor-critic},
  groups    = {RL},
  publisher = {University of Massachusetts Amherst},
  ranking   = {rank5},
}

@Article{Sutton1991_Dyna,
  author    = {Sutton, Richard S.},
  journal   = {ACM SIGART Bulletin},
  title     = {Dyna, an integrated architecture for learning, planning, and reacting},
  year      = {1991},
  issn      = {0163-5719},
  month     = jul,
  number    = {4},
  pages     = {160--163},
  volume    = {2},
  doi       = {10.1145/122344.122377},
  file      = {:Sutton1991 - Dyna, an Integrated Architecture for Learning, Planning, and Reacting.pdf:PDF},
  groups    = {Model based},
  publisher = {Association for Computing Machinery (ACM)},
  ranking   = {rank4},
}

@InProceedings{Sutton2008_DynaLinearReg,
  author    = {Sutton, Richard S. and Szepesv\'{a}ri, Csaba and Geramifard, Alborz and Bowling, Michael},
  booktitle = {Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence},
  title     = {Dyna-style planning with linear function approximation and prioritized sweeping},
  year      = {2008},
  address   = {Arlington, Virginia, USA},
  pages     = {528–536},
  publisher = {AUAI Press},
  series    = {UAI'08},
  abstract  = {We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world. This paper develops an explicitly model-based approach extending the Dyna architecture to linear function approximation. Dyna-style planning proceeds by generating imaginary experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our main results are to prove that linear Dyna-style planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting, we prove that the limit point is the least-squares (LSTD) solution. An implication of our results is that prioritized-sweeping can be soundly extended to the linear approximation case, backing up to preceding features rather than to preceding states. We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems.},
  groups    = {Model based},
  isbn      = {0974903949},
  location  = {Helsinki, Finland},
  numpages  = {9},
  ranking   = {rank2},
}

@Article{Narendra1990_NNSystemIdent,
  author    = {Narendra, K.S. and Parthasarathy, K.},
  journal   = {IEEE Transactions on Neural Networks},
  title     = {Identification and control of dynamical systems using neural networks},
  year      = {1990},
  issn      = {1045-9227},
  month     = mar,
  number    = {1},
  pages     = {4--27},
  volume    = {1},
  doi       = {10.1109/72.80202},
  file      = {:Narendra1990 - Identification and Control of Dynamical Systems Using Neural Networks.pdf:PDF},
  groups    = {Model based},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank4},
}

@InProceedings{Oh2015_DeepModel,
  author    = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Action-Conditional Video Prediction using Deep Networks in Atari Games},
  year      = {2015},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {28},
  file      = {:Oh2015 - Action Conditional Video Prediction Using Deep Networks in Atari Games.pdf:PDF},
  groups    = {Model based},
  ranking   = {rank4},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/6ba3af5d7b2790e73f0de32e5c8c1798-Paper.pdf},
}

@InProceedings{Hester2012,
  author    = {Hester, Todd and Stone, Peter},
  booktitle = {2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL)},
  title     = {Intrinsically motivated model learning for a developing curious agent},
  year      = {2012},
  month     = nov,
  publisher = {IEEE},
  doi       = {10.1109/devlrn.2012.6400802},
  file      = {:Hester2012 - Intrinsically Motivated Model Learning for a Developing Curious Agent.pdf:PDF},
  groups    = {Curiosity},
  ranking   = {rank1},
}

@Article{Hester2012_TEXPLORE,
  author    = {Hester, Todd and Stone, Peter},
  journal   = {Machine Learning},
  title     = {TEXPLORE: real-time sample-efficient reinforcement learning for robots},
  year      = {2012},
  issn      = {1573-0565},
  month     = oct,
  number    = {3},
  pages     = {385--429},
  volume    = {90},
  doi       = {10.1007/s10994-012-5322-7},
  file      = {:Hester2012a - TEXPLORE_ Real Time Sample Efficient Reinforcement Learning for Robots.pdf:PDF},
  groups    = {Model based},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank1},
}

@InProceedings{Silver2014_DPG,
  author     = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle  = {Proceedings of the 31st International Conference on Machine Learning},
  title      = {Deterministic Policy Gradient Algorithms},
  year       = {2014},
  address    = {Bejing, China},
  editor     = {Xing, Eric P. and Jebara, Tony},
  month      = {22--24 Jun},
  number     = {1},
  pages      = {387--395},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {32},
  abstract   = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.},
  file       = {:Silver2014_DPG - Deterministic Policy Gradient Algorithms.pdf:PDF;:Silver2014_DPG - supp mat.pdf:PDF},
  groups     = {Policy based, Actor Critic},
  pdf        = {http://proceedings.mlr.press/v32/silver14.pdf},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://proceedings.mlr.press/v32/silver14.html},
}

@InProceedings{Levine2014_GPSModel,
  author    = {Levine, Sergey and Abbeel, Pieter},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {27},
  file      = {:Levine2014 - Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics.pdf:PDF},
  groups    = {Model based},
  ranking   = {rank3},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf},
}

@InProceedings{Levine2013_GPS,
  author    = {Levine, Sergey and Koltun, Vladlen},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  title     = {Guided Policy Search},
  year      = {2013},
  address   = {Atlanta, Georgia, USA},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  month     = {17--19 Jun},
  number    = {3},
  pages     = {1--9},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {28},
  abstract  = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.},
  groups    = {Policy based},
  pdf       = {http://proceedings.mlr.press/v28/levine13.pdf},
  ranking   = {rank5},
  url       = {https://proceedings.mlr.press/v28/levine13.html},
}

@InBook{Kocsis2006_MCTS,
  author    = {Kocsis, Levente and Szepesvári, Csaba},
  pages     = {282--293},
  publisher = {Springer Berlin Heidelberg},
  title     = {Bandit Based Monte-Carlo Planning},
  year      = {2006},
  isbn      = {9783540460565},
  booktitle = {Machine Learning: ECML 2006},
  doi       = {10.1007/11871842_29},
  groups    = {Model based},
  issn      = {1611-3349},
  ranking   = {rank5},
}

@InProceedings{Tesauro1996_MonteCarloSearch,
  author    = {Tesauro, Gerald and Galperin, Gregory},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {On-line Policy Improvement using Monte-Carlo Search},
  year      = {1996},
  editor    = {M.C. Mozer and M. Jordan and T. Petsche},
  publisher = {MIT Press},
  volume    = {9},
  groups    = {Model based},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1996/file/996009f2374006606f4c0b0fda878af1-Paper.pdf},
}

@InProceedings{Engstrom2020_PPOImplement,
  author    = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  title     = {Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO},
  year      = {2020},
  publisher = {OpenReview.net},
  abstract  = {We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of "code-level optimizations:" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty and importance of attributing performance gains in deep reinforcement learning. Code for reproducing our results is available at https://github.com/MadryLab/implementation-matters .},
  file      = {:https___doi.org_10.48550_arxiv.2005.12729 - Implementation Matters in Deep Policy Gradients_ a Case Study on PPO and TRPO.pdf:PDF},
  groups    = {TRPO-PPO, Implementation tricks in RL},
  ranking   = {rank3},
  url       = {https://openreview.net/forum?id=r1etN1rtPB},
}

@InProceedings{Nagabandi2018,
  author    = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning},
  year      = {2018},
  month     = may,
  publisher = {IEEE},
  doi       = {10.1109/icra.2018.8463189},
  groups    = {Model based},
  ranking   = {rank5},
}

@Book{Thorndike1911_AnimalIntell,
  author    = {Thorndike, Edward L.},
  publisher = {The Macmillan Company},
  title     = {Animal intelligence; experimental studies},
  year      = {1911},
  doi       = {10.5962/bhl.title.55072},
  groups    = {Artificial Intelligence},
  ranking   = {rank5},
}

@Article{Walter1950_MechTortoise,
  author    = {W. Grey Walter},
  journal   = {Scientific American},
  title     = {An Imitation Of Life},
  year      = {1950},
  issn      = {00368733, 19467087},
  number    = {5},
  pages     = {42--45},
  volume    = {182},
  groups    = {Artificial Intelligence},
  publisher = {Scientific American, a division of Nature America, Inc.},
  ranking   = {rank3},
  url       = {http://www.jstor.org/stable/24967456},
  urldate   = {2024-03-24},
}

@Article{Bellman1966,
  author    = {Bellman, Richard},
  journal   = {Science},
  title     = {Dynamic Programming},
  year      = {1966},
  issn      = {1095-9203},
  month     = jul,
  number    = {3731},
  pages     = {34--37},
  volume    = {153},
  doi       = {10.1126/science.153.3731.34},
  groups    = {Artificial Intelligence},
  publisher = {American Association for the Advancement of Science (AAAS)},
  ranking   = {rank5},
}

@Article{Garcia2015_SafeRL,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  journal = {Journal of Machine Learning Research},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  year    = {2015},
  number  = {42},
  pages   = {1437--1480},
  volume  = {16},
  groups  = {Safe RL},
  ranking = {rank5},
  url     = {http://jmlr.org/papers/v16/garcia15a.html},
}

@InProceedings{Coulom2006_MCTS,
  author    = {Coulom, Rémi},
  booktitle = {Computers and Games},
  title     = {Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search},
  year      = {2006},
  address   = {Berlin, Heidelberg},
  editor    = {van den Herik, H. Jaap and Ciancarini, Paolo and Donkers, H. H. L. M. (Jeroen)},
  pages     = {72--83},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9{\texttimes}9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
  doi       = {10.1007/978-3-540-75538-8_7},
  groups    = {Model based},
  isbn      = {978-3-540-75538-8},
  issn      = {1611-3349},
  ranking   = {rank5},
}

@InProceedings{Finnsson2008_GamePlaying,
  author    = {Finnsson, Hilmar and Bj\"{o}rnsson, Yngvi},
  booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 1},
  title     = {Simulation-based approach to general game playing},
  year      = {2008},
  pages     = {259–264},
  publisher = {AAAI Press},
  series    = {AAAI'08},
  abstract  = {The aim of General Game Playing (GGP) is to create intelligent agents that automatically learn how to play many different games at an expert level without any human intervention. The most successful GGP agents in the past have used traditional game-tree search combined with an automatically learned heuristic function for evaluating game states. In this paper we describe a GGP agent that instead uses a Monte Carlo/UCT simulation technique for action selection, an approach recently popularized in computer Go. Our GGP agent has proven its effectiveness by winning last year s AAAI GGP Competition. Furthermore, we introduce and empirically evaluate a new scheme for automatically learning search-control knowledge for guiding the simulation playouts, showing that it offers significant benefits for a variety of games.},
  groups    = {Model based},
  isbn      = {9781577353683},
  location  = {Chicago, Illinois},
  numpages  = {6},
  ranking   = {rank2},
}

@Article{Rosenblatt1958_Perceptron,
  author    = {Rosenblatt, F.},
  journal   = {Psychological Review},
  title     = {The perceptron: A probabilistic model for information storage and organization in the brain.},
  year      = {1958},
  issn      = {0033-295X},
  number    = {6},
  pages     = {386--408},
  volume    = {65},
  doi       = {10.1037/h0042519},
  groups    = {Artificial Intelligence},
  publisher = {American Psychological Association (APA)},
  ranking   = {rank5},
}

@InProceedings{Glorot2011_ReLU,
  author    = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  title     = {Deep Sparse Rectifier Neural Networks},
  year      = {2011},
  address   = {Fort Lauderdale, FL, USA},
  editor    = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  month     = {11--13 Apr},
  pages     = {315--323},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {15},
  abstract  = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  groups    = {Deep Learning},
  pdf       = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  ranking   = {rank5},
  url       = {https://proceedings.mlr.press/v15/glorot11a.html},
}

@InProceedings{Tolstikhin2021_MLPMixer,
  author    = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {MLP-Mixer: An all-MLP Architecture for Vision},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {24261--24272},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  groups    = {Computer Vision},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf},
}

@Article{Hochreiter1997_LSTM,
  author    = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal   = {Neural Computation},
  title     = {Long Short-Term Memory},
  year      = {1997},
  issn      = {1530-888X},
  month     = nov,
  number    = {8},
  pages     = {1735--1780},
  volume    = {9},
  doi       = {10.1162/neco.1997.9.8.1735},
  groups    = {NLP},
  publisher = {MIT Press},
  ranking   = {rank5},
}

@Article{Chung2014_GRU,
  author        = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  title         = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  year          = {2014},
  month         = dec,
  abstract      = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1412.3555},
  eprint        = {1412.3555},
  file          = {:https___doi.org_10.48550_arxiv.1412.3555 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:PDF},
  groups        = {NLP},
  keywords      = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.NE},
  publisher     = {arXiv},
  ranking       = {rank5},
}

@InProceedings{Claus1998_JAL,
  author    = {Claus, Caroline and Boutilier, Craig},
  booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
  title     = {The dynamics of reinforcement learning in cooperative multiagent systems},
  year      = {1998},
  address   = {USA},
  pages     = {746–752},
  publisher = {American Association for Artificial Intelligence},
  series    = {AAAI '98/IAAI '98},
  abstract  = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multi agent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-leaming in cooperative multi agent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.},
  file      = {:Claus1998_JAL - The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf:PDF},
  groups    = {Centralised Training and Execution},
  isbn      = {0262510987},
  location  = {Madison, Wisconsin, USA},
  numpages  = {7},
  priority  = {prio1},
  ranking   = {rank5},
}

@InProceedings{Lake2018_Generalization,
  author    = {Lake, Brenden and Baroni, Marco},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  title     = {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  month     = {10--15 Jul},
  pages     = {2873--2882},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.},
  file      = {:Lake2018_Generalization - Generalization without Systematicity_ on the Compositional Skills of Sequence to Sequence Recurrent Networks.pdf:PDF:http\://proceedings.mlr.press/v80/lake18a/lake18a.pdf},
  groups    = {NLP, Language},
  pdf       = {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf},
  priority  = {prio1},
  ranking   = {rank4},
  url       = {https://proceedings.mlr.press/v80/lake18a.html},
}

@InProceedings{Lee2018_EmergentTranslation,
  author     = {Jason Lee and Kyunghyun Cho and Jason Weston and Douwe Kiela},
  booktitle  = {International Conference on Learning Representations},
  title      = {Emergent Translation in Multi-Agent Communication},
  year       = {2018},
  file       = {:Lee2018_Emergent - Emergent Translation in Multi Agent Communication.pdf:PDF:https\://openreview.net/pdf?id=H1vEXaxA-},
  groups     = {Communication, Language-Grounded Communication},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://openreview.net/forum?id=H1vEXaxA-},
}

@Article{Steels1997,
  author     = {Steels, Luc},
  journal    = {Evolution of Communication},
  title      = {The Synthetic Modeling of Language Origins},
  year       = {1997},
  issn       = {1569-9757},
  month      = jan,
  number     = {1},
  pages      = {1--34},
  volume     = {1},
  doi        = {10.1075/eoc.1.1.02ste},
  file       = {:Steels1997 - The Synthetic Modeling of Language Origins.pdf:PDF:https\://digital.csic.es/bitstream/10261/128074/1/language%20origins.pdf},
  groups     = {Communication, Language origins and evolution},
  publisher  = {John Benjamins Publishing Company},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@InProceedings{Chaabouni2019_AntiEfficient,
  author     = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Anti-efficient encoding in emergent communication},
  year       = {2019},
  editor     = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {32},
  file       = {:Chaabouni2019 - Anti Efficient Encoding in Emergent Communication.pdf:PDF:https\://proceedings.neurips.cc/paper/2019/file/31ca0ca71184bbdb3de7b20a51e88e90-Paper.pdf},
  groups     = {Communication, Speaker-Listener},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2019/file/31ca0ca71184bbdb3de7b20a51e88e90-Paper.pdf},
}

@InProceedings{Mu2023_EC2,
  author     = {Mu, Yao and Yao, Shunyu and Ding, Mingyu and Luo, Ping and Gan, Chuang},
  booktitle  = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {EC2: Emergent Communication for Embodied Control},
  year       = {2023},
  month      = {June},
  pages      = {6704-6714},
  comment    = {Their "emergent communication" part is basically VQ-VAE: they use a speaker-listener archi. with discrete symbols to reconstruct part of videos},
  file       = {:Mu2023_EC2 - EC2_ Emergent Communication for Embodied Control.pdf:PDF:https\://openaccess.thecvf.com/content/CVPR2023/papers/Mu_EC2_Emergent_Communication_for_Embodied_Control_CVPR_2023_paper.pdf},
  groups     = {Communication},
  readstatus = {skimmed},
  url        = {https://openaccess.thecvf.com/content/CVPR2023/html/Mu_EC2_Emergent_Communication_for_Embodied_Control_CVPR_2023_paper.html},
}

@InProceedings{LeCun1989_CNN,
  author    = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Handwritten Digit Recognition with a Back-Propagation Network},
  year      = {1989},
  editor    = {D. Touretzky},
  publisher = {Morgan-Kaufmann},
  volume    = {2},
  groups    = {Computer Vision, Deep Learning},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
}

@InProceedings{Simonyan2015_VGG,
  author    = {Simonyan, K and Zisserman, A},
  title     = {Very deep convolutional networks for large-scale image recognition},
  year      = {2015},
  pages     = {1-14},
  publisher = {Computational and Biological Learning Society},
  groups    = {Computer Vision},
  journal   = {3rd International Conference on Learning Representations (ICLR 2015)},
  ranking   = {rank5},
  url       = {https://arxiv.org/abs/1409.1556},
}

@InProceedings{Krizhevsky2012_AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  groups    = {Computer Vision},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
}

@InProceedings{Vaswani2017_Transformer,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Attention is All you Need},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  groups    = {NLP},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
}

@Article{Baumli2023,
  author        = {Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Chan, Harris and Comanici, Gheorghe and Flennerhag, Sebastian and Gazeau, Maxime and Holsheimer, Kristian and Horgan, Dan and Laskin, Michael and Lyle, Clare and Masoom, Hussain and McKinney, Kay and Mnih, Volodymyr and Neitz, Alexander and Pardo, Fabio and Parker-Holder, Jack and Quan, John and Rocktäschel, Tim and Sahni, Himanshu and Schaul, Tom and Schroecker, Yannick and Spencer, Stephen and Steigerwald, Richie and Wang, Luyu and Zhang, Lei},
  title         = {Vision-Language Models as a Source of Rewards},
  year          = {2023},
  month         = dec,
  abstract      = {Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2312.09187},
  eprint        = {2312.09187},
  file          = {:Baumli2023 - Vision Language Models As a Source of Rewards.pdf:PDF:http\://arxiv.org/pdf/2312.09187v2},
  groups        = {LA Reward shaping},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@PhdThesis{Lin1992_ExperienceReplay,
  author    = {Lin, Long-Ji},
  title     = {Reinforcement learning for robots using neural networks},
  year      = {1992},
  groups    = {RL},
  publisher = {Carnegie Mellon University},
  ranking   = {rank5},
  url       = {https://apps.dtic.mil/sti/tr/pdf/ADA261434.pdf},
}

@Article{Kaplan2020_ScalingLaws,
  author        = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title         = {Scaling Laws for Neural Language Models},
  year          = {2020},
  month         = jan,
  abstract      = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2001.08361},
  eprint        = {2001.08361},
  file          = {:Kaplan2020 - Scaling Laws for Neural Language Models.pdf:PDF:http\://arxiv.org/pdf/2001.08361v1},
  groups        = {Deep Learning},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank5},
}

@Article{Villalobos2022_ModelSizes,
  author        = {Villalobos, Pablo and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Ho, Anson and Hobbhahn, Marius},
  title         = {Machine Learning Model Sizes and the Parameter Gap},
  year          = {2022},
  month         = jul,
  abstract      = {We study trends in model size of notable machine learning systems over time using a curated dataset. From 1950 to 2018, model size in language models increased steadily by seven orders of magnitude. The trend then accelerated, with model size increasing by another five orders of magnitude in just 4 years from 2018 to 2022. Vision models grew at a more constant pace, totaling 7 orders of magnitude of growth between 1950 and 2022. We also identify that, since 2020, there have been many language models below 20B parameters, many models above 70B parameters, but a scarcity of models in the 20-70B parameter range. We refer to that scarcity as the parameter gap. We provide some stylized facts about the parameter gap and propose a few hypotheses to explain it. The explanations we favor are: (a) increasing model size beyond 20B parameters requires adopting different parallelism techniques, which makes mid-sized models less cost-effective, (b) GPT-3 was one order of magnitude larger than previous language models, and researchers afterwards primarily experimented with bigger models to outperform it. While these dynamics likely exist, and we believe they play some role in generating the gap, we don't have high confidence that there are no other, more important dynamics at play.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2207.02852},
  eprint        = {2207.02852},
  file          = {:Villalobos2022 - Machine Learning Model Sizes and the Parameter Gap.pdf:PDF:http\://arxiv.org/pdf/2207.02852v1},
  groups        = {Deep Learning},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank1},
}

@InProceedings{Hill2021_Grounded,
  author    = {Felix Hill and Olivier Tieleman and Tamara von Glehn and Nathaniel Wong and Hamza Merzic and Stephen Clark},
  booktitle = {International Conference on Learning Representations},
  title     = {Grounded Language Learning Fast and Slow},
  year      = {2021},
  file      = {:Hill2021_Grounded - Grounded Language Learning Fast and Slow.pdf:PDF:https\://openreview.net/pdf?id=wpSWuz_hyqA},
  groups    = {Language-Augmented RL},
  priority  = {prio1},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=wpSWuz_hyqA},
}

@InProceedings{Rita2022_GenOverf,
  author     = {Rita, Mathieu and Tallec, Corentin and Michel, Paul and Grill, Jean-Bastien and Pietquin, Olivier and Dupoux, Emmanuel and Strub, Florian},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Emergent Communication: Generalization and Overfitting in Lewis Games},
  year       = {2022},
  editor     = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages      = {1389--1404},
  publisher  = {Curran Associates, Inc.},
  volume     = {35},
  file       = {:Rita2022 - Emergent Communication_ Generalization and Overfitting in Lewis Games.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2022/file/093b08a7ad6e6dd8d34b9cc86bb5f07c-Paper-Conference.pdf},
  groups     = {Communication, Speaker-Listener},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/093b08a7ad6e6dd8d34b9cc86bb5f07c-Abstract-Conference.html},
}

@Article{Chaabouni2021,
  author    = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Communicating artificial neural networks develop efficient color-naming systems},
  year      = {2021},
  issn      = {1091-6490},
  month     = mar,
  number    = {12},
  volume    = {118},
  doi       = {10.1073/pnas.2016569118},
  file      = {:chaabouni-et-al-2021-communicating-artificial-neural-networks-develop-efficient-color-naming-systems.pdf:PDF},
  groups    = {Communication, Discrete language},
  priority  = {prio1},
  publisher = {Proceedings of the National Academy of Sciences},
  ranking   = {rank1},
}

@InProceedings{Hasselt2010_DoubleQlearning,
  author    = {Hasselt, Hado},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Double Q-learning},
  year      = {2010},
  editor    = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
  publisher = {Curran Associates, Inc.},
  volume    = {23},
  groups    = {Value based},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf},
}

@InProceedings{Plappert2018_Noise,
  author    = {Matthias Plappert and Rein Houthooft and Prafulla Dhariwal and Szymon Sidor and Richard Y. Chen and Xi Chen and Tamim Asfour and Pieter Abbeel and Marcin Andrychowicz},
  booktitle = {International Conference on Learning Representations},
  title     = {Parameter Space Noise for Exploration},
  year      = {2018},
  groups    = {RL},
  ranking   = {rank4},
  url       = {https://openreview.net/forum?id=ByBAl2eAZ},
}

@InProceedings{Hausknecht2015d_DRQN,
  author     = {Hausknecht, Matthew and Stone, Peter},
  booktitle  = {2015 AAAI Fall Symposium Series},
  title      = {Deep Recurrent Q-Learning for Partially Observable MDPs},
  year       = {2015},
  file       = {:Hausknecht2015d_DRQN - Deep Recurrent Q Learning for Partially Observable Mdps.pdf:PDF:https\://cdn.aaai.org/ocs/11673/11673-51288-1-PB.pdf},
  groups     = {Value based},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://cdn.aaai.org/ocs/11673/11673-51288-1-PB.pdf},
}

@Article{Serban2017_VHRED,
  author    = {Serban, Iulian and Sordoni, Alessandro and Lowe, Ryan and Charlin, Laurent and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues},
  year      = {2017},
  issn      = {2159-5399},
  month     = feb,
  number    = {1},
  volume    = {31},
  doi       = {10.1609/aaai.v31i1.10983},
  file      = {:10983-Article Text-14511-1-2-20201228.pdf:PDF},
  groups    = {Dialogue agents},
  priority  = {prio1},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  ranking   = {rank4},
}

@InProceedings{Battaglia2016,
  author    = {Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Jimenez Rezende, Danilo and kavukcuoglu, koray},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Interaction Networks for Learning about Objects, Relations and Physics},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  groups    = {Interaction Networks},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf},
}

@InProceedings{Jang2017_GumbelSoftmax,
  author    = {Eric Jang and Shixiang Gu and Ben Poole},
  booktitle = {International Conference on Learning Representations},
  title     = {Categorical Reparameterization with Gumbel-Softmax},
  year      = {2017},
  file      = {:Jang2017_GumbelSoftmax - Categorical Reparameterization with Gumbel Softmax.pdf:PDF:https\://openreview.net/pdf?id=rkE3y85ee},
  groups    = {Deep Learning},
  ranking   = {rank5},
  url       = {https://openreview.net/forum?id=rkE3y85ee},
}

@Article{Yoshida2008,
  author    = {Yoshida, Wako and Dolan, Ray J. and Friston, Karl J.},
  journal   = {PLoS Computational Biology},
  title     = {Game Theory of Mind},
  year      = {2008},
  issn      = {1553-7358},
  month     = dec,
  number    = {12},
  pages     = {e1000254},
  volume    = {4},
  doi       = {10.1371/journal.pcbi.1000254},
  editor    = {Behrens, Tim},
  file      = {:Yoshida2008 - Game Theory of Mind.pdf:PDF:https\://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1000254&type=printable},
  groups    = {Agent Modelling, Theory of Mind},
  priority  = {prio1},
  publisher = {Public Library of Science (PLoS)},
  ranking   = {rank3},
}

@Article{Steels2003Social,
  author    = {Steels, Luc},
  journal   = {The future of learning},
  title     = {Social language learning},
  year      = {2003},
  pages     = {133--162},
  file      = {:Steels2003Social - Social Language Learning.pdf:PDF:https\://ai.vub.ac.be/sites/default/files/steels-03f.pdf},
  groups    = {Language, Language origins and evolution},
  publisher = {IOS Press},
  ranking   = {rank1},
  url       = {https://ai.vub.ac.be/sites/default/files/steels-03f.pdf},
}

@InProceedings{Li2023_CAMEL,
  author    = {Li, Guohao and Hammoud, Hasan and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society},
  year      = {2023},
  editor    = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {51991--52008},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Li2023 - CAMEL_ Communicative Agents for _Mind_ Exploration of Large Language Model Society.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2023/file/a3621ee907def47c1b952ade25c67698-Paper-Conference.pdf},
  groups    = {Communication, Comm with LLMs, LLM towns},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a3621ee907def47c1b952ade25c67698-Paper-Conference.pdf},
}

@Article{Zhu2024_CognitiveLLMs,
  author    = {Zhu, Feiyu and Simmons, Reid},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Bootstrapping Cognitive Agents with a Large Language Model},
  year      = {2024},
  issn      = {2159-5399},
  month     = mar,
  number    = {1},
  pages     = {655--663},
  volume    = {38},
  doi       = {10.1609/aaai.v38i1.27822},
  file      = {:27822-Article Text-31876-1-2-20240324-1.pdf:PDF},
  groups    = {Agent-based LLMs},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
}

@Article{Liu2020a,
  author    = {Liu, Yong and Wang, Weixun and Hu, Yujing and Hao, Jianye and Chen, Xingguo and Gao, Yang},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Multi-Agent Game Abstraction via Graph Attention Neural Network},
  year      = {2020},
  issn      = {2159-5399},
  month     = apr,
  number    = {05},
  pages     = {7211--7218},
  volume    = {34},
  doi       = {10.1609/aaai.v34i05.6211},
  groups    = {Graphs},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  ranking   = {rank2},
}

@Article{Nowak1999_LanguageEvo,
  author    = {Nowak, Martin A. and Krakauer, David C.},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {The evolution of language},
  year      = {1999},
  issn      = {1091-6490},
  month     = jul,
  number    = {14},
  pages     = {8028--8033},
  volume    = {96},
  doi       = {10.1073/pnas.96.14.8028},
  file      = {:nowak-krakauer-1999-the-evolution-of-language.pdf:PDF},
  groups    = {Language origins and evolution},
  priority  = {prio2},
  publisher = {Proceedings of the National Academy of Sciences},
  ranking   = {rank4},
}

@InProceedings{Oord2017_VQ-VAE,
  author     = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Neural Discrete Representation Learning},
  year       = {2017},
  editor     = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {30},
  file       = {:Oord2017 - Neural Discrete Representation Learning.pdf:PDF:https\://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
  groups     = {Deep Learning, Auto-Encoding},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
}

@InProceedings{Iqbal2019_MAAC,
  author    = {Iqbal, Shariq and Sha, Fei},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month     = {09--15 Jun},
  pages     = {2961--2970},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.},
  file      = {:Iqbal2019_MAAC - Actor Attention Critic for Multi Agent Reinforcement Learning.pdf:PDF:http\://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  groups    = {Multi-agent RL},
  pdf       = {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  priority  = {prio1},
  ranking   = {rank4},
  url       = {https://proceedings.mlr.press/v97/iqbal19a.html},
}

@InProceedings{Tishby2015,
  author     = {Tishby, Naftali and Zaslavsky, Noga},
  booktitle  = {2015 IEEE Information Theory Workshop (ITW)},
  title      = {Deep learning and the information bottleneck principle},
  year       = {2015},
  month      = apr,
  publisher  = {IEEE},
  doi        = {10.1109/itw.2015.7133169},
  file       = {:Tishby2015 - Deep Learning and the Information Bottleneck Principle.pdf:PDF:https\://arxiv.org/pdf/1503.02406},
  groups     = {Deep Learning, Auto-Encoding},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@InProceedings{Yang2023,
  author    = {Yang, Dingkang and Yang, Kun and Wang, Yuzheng and Liu, Jing and Xu, Zhi and Yin, Rongbin and Zhai, Peng and Zhang, Lihua},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {How2comm: Communication-Efficient and Collaboration-Pragmatic Multi-Agent Perception},
  year      = {2023},
  editor    = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {25151--25164},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  groups    = {Applied MAC},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4f31327e046913c7238d5b671f5d820e-Paper-Conference.pdf},
}

@InProceedings{Guo2023,
  author    = {Guo, Yuxuan and Hao, Yifan and Zhang, Rui and Zhou, Enshuai and Du, Zidong and zhang, xishan and Song, Xinkai and Wen, Yuanbo and Zhao, Yongwei and Zhou, Xuehai and Guo, Jiaming and Yi, Qi and Peng, Shaohui and Huang, Di and Chen, Ruizhi and Guo, Qi and Chen, Yunji},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Emergent Communication for Rules Reasoning},
  year      = {2023},
  editor    = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {68655--68672},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Guo2023 - Emergent Communication for Rules Reasoning.pdf:PDF:https\://papers.nips.cc/paper_files/paper/2023/file/d8ace30c68b085556ccce04ed4ae4ebb-Paper-Conference.pdf},
  groups    = {Communication, Speaker-Listener},
  priority  = {prio3},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/d8ace30c68b085556ccce04ed4ae4ebb-Paper-Conference.pdf},
}

@InProceedings{Ye2023,
  author    = {and Ye, Deheng and Lu, Zongqing},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Mutual-Information Regularized Multi-Agent Policy Iteration},
  year      = {2023},
  editor    = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {2617--2635},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Ye2023 - Mutual Information Regularized Multi Agent Policy Iteration.pdf:PDF:https\://papers.nips.cc/paper_files/paper/2023/file/0799492e7be38b66d10ead5e8809616d-Paper-Conference.pdf},
  groups    = {Intrinsic rewards in MARL},
  priority  = {prio2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0799492e7be38b66d10ead5e8809616d-Paper-Conference.pdf},
}

@InProceedings{Li2023a,
  author    = {Li, Jiahui and Kuang, Kun and Wang, Baoxiang and Li, Xingchen and Wu, Fei and Xiao, Jun and Chen, Long},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Two Heads are Better Than One: A Simple Exploration Framework for Efficient Multi-Agent Reinforcement Learning},
  year      = {2023},
  editor    = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {20038--20053},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Li2023a - Two Heads Are Better Than One_ a Simple Exploration Framework for Efficient Multi Agent Reinforcement Learning.pdf:PDF:https\://papers.nips.cc/paper_files/paper/2023/file/3fa2d2b637122007845a2fbb7c21453b-Paper-Conference.pdf},
  groups    = {Intrinsic rewards in MARL, Exploration in MARL},
  priority  = {prio2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3fa2d2b637122007845a2fbb7c21453b-Paper-Conference.pdf},
}

@InProceedings{Shen2023,
  author    = {Shen, Siqi and Ma, Chennan and Li, Chao and Liu, Weiquan and Fu, Yongquan and Mei, Songzhu and Liu, Xinwang and Wang, Cheng},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization},
  year      = {2023},
  editor    = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {34791--34825},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Shen2023 - RiskQ_ Risk Sensitive Multi Agent Reinforcement Learning Value Factorization.pdf:PDF:https\://papers.nips.cc/paper_files/paper/2023/file/6d3040941a2d57ead4043556a70dd728-Paper-Conference.pdf},
  groups    = {Multi-agent RL},
  priority  = {prio3},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6d3040941a2d57ead4043556a70dd728-Paper-Conference.pdf},
}

@Article{Farrell1996,
  author    = {Farrell, Joseph and Rabin, Matthew},
  journal   = {Journal of Economic Perspectives},
  title     = {Cheap Talk},
  year      = {1996},
  issn      = {0895-3309},
  month     = aug,
  number    = {3},
  pages     = {103--118},
  volume    = {10},
  doi       = {10.1257/jep.10.3.103},
  file      = {:farrell-rabin-2011-cheap-talk.pdf:PDF},
  groups    = {Communication},
  priority  = {prio3},
  publisher = {American Economic Association},
  ranking   = {rank5},
}

@Article{Wagner2003_EC,
  author    = {Wagner, Kyle and Reggia, James A. and Uriagereka, Juan and Wilkinson, Gerald S.},
  journal   = {Adaptive Behavior},
  title     = {Progress in the Simulation of Emergent Communication and Language},
  year      = {2003},
  issn      = {1741-2633},
  month     = mar,
  number    = {1},
  pages     = {37--69},
  volume    = {11},
  doi       = {10.1177/10597123030111003},
  file      = {:Wagner2003 - Progress in the Simulation of Emergent Communication and Language.pdf:PDF:https\://langev.com/pdf/wagner03adaptiveBehavior.pdf},
  groups    = {Communication},
  priority  = {prio3},
  publisher = {SAGE Publications},
  ranking   = {rank2},
}

@Article{Bogin2018,
  author        = {Bogin, Ben and Geva, Mor and Berant, Jonathan},
  title         = {Emergence of Communication in an Interactive World with Consistent Speakers},
  year          = {2018},
  month         = sep,
  abstract      = {Training agents to communicate with one another given task-based supervision only has attracted considerable attention recently, due to the growing interest in developing models for human-agent interaction. Prior work on the topic focused on simple environments, where training using policy gradient was feasible despite the non-stationarity of the agents during training. In this paper, we present a more challenging environment for testing the emergence of communication from raw pixels, where training using policy gradient fails. We propose a new model and training algorithm, that utilizes the structure of a learned representation space to produce more consistent speakers at the initial phases of training, which stabilizes learning. We empirically show that our algorithm substantially improves performance compared to policy gradient. We also propose a new alignment-based metric for measuring context-independence in emerged communication and find our method increases context-independence compared to policy gradient and other competitive baselines.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1809.00549},
  eprint        = {1809.00549},
  file          = {:Bogin2018 - Emergence of Communication in an Interactive World with Consistent Speakers.pdf:PDF:http\://arxiv.org/pdf/1809.00549v2},
  groups        = {Communication},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  priority      = {prio2},
  publisher     = {arXiv},
}

@InProceedings{Park2023_LLMtown,
  author    = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  title     = {Generative Agents: Interactive Simulacra of Human Behavior},
  year      = {2023},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {UIST '23},
  abstract  = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  articleno = {2},
  doi       = {10.1145/3586183.3606763},
  file      = {:Park2013_LLMTownrohtua - Generative Agents_ Interactive Simulacra of Human Behavior.pdf:PDF:https\://dl.acm.org/doi/pdf/10.1145/3586183.3606763},
  groups    = {LLM towns},
  isbn      = {9798400701320},
  keywords  = {Human-AI interaction, agents, generative AI, large language models},
  location  = {, San Francisco, CA, USA,},
  numpages  = {22},
  ranking   = {rank4},
  url       = {https://doi.org/10.1145/3586183.3606763},
}

@InProceedings{Wei2022_ChainOfThought,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {24824--24837},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Wei2022 - Chain of Thought Prompting Elicits Reasoning in Large Language Models.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  groups    = {Large Language Models},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
}

@InProceedings{Kojima2022,
  author    = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Large Language Models are Zero-Shot Reasoners},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {22199--22213},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Kojima2022 - Large Language Models Are Zero Shot Reasoners.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
  groups    = {Large Language Models},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
}

@InProceedings{Tishby1999_IB,
  author     = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  booktitle  = {Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing},
  title      = {The information bottleneck method},
  year       = {1999},
  pages      = {368-377},
  abstract   = {We define the relevant information in a signal $x\in X$ as being the information that this signal provides about another signal $y\in \Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\X$ that preserves the maximum information about $\Y$. That is, we squeeze the information that $\X$ provides about $\Y$ through a `bottleneck' formed by a limited set of codewords $\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\x)$ emerges from the joint statistics of $\X$ and $\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \to \tX$ and $\tX \to \Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  doi        = {10.48550/ARXIV.PHYSICS/0004057},
  file       = {:Tishby2000 - The Information Bottleneck Method.pdf:PDF:http\://arxiv.org/pdf/physics/0004057v1},
  groups     = {Auto-Encoding},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@InProceedings{Rezende2014_VAE,
  author    = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  title     = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  year      = {2014},
  address   = {Bejing, China},
  editor    = {Xing, Eric P. and Jebara, Tony},
  month     = {22--24 Jun},
  number    = {2},
  pages     = {1278--1286},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {32},
  abstract  = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  file      = {:Rezende2014_VAE - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:PDF:http\://proceedings.mlr.press/v32/rezende14.pdf},
  groups    = {Auto-Encoding},
  pdf       = {http://proceedings.mlr.press/v32/rezende14.pdf},
  ranking   = {rank5},
  url       = {https://proceedings.mlr.press/v32/rezende14.html},
}

@InProceedings{Kingma2013,
  author    = {Kingma, Diederik P and Welling, Max},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Auto-Encoding Variational Bayes},
  year      = {2014},
  abstract  = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  doi       = {10.48550/ARXIV.1312.6114},
  file      = {:Kingma2013 - Auto Encoding Variational Bayes.pdf:PDF:http\://arxiv.org/pdf/1312.6114v11},
  groups    = {Auto-Encoding},
  ranking   = {rank5},
}

@InProceedings{Alemi2017,
  author    = {Alexander A. Alemi and Ian Fischer and Joshua V. Dillon and Kevin Murphy},
  booktitle = {International Conference on Learning Representations},
  title     = {Deep Variational Information Bottleneck},
  year      = {2017},
  file      = {:Alemi2017 - Deep Variational Information Bottleneck.pdf:PDF:https\://openreview.net/pdf?id=HyxQzBceg},
  groups    = {Auto-Encoding},
  ranking   = {rank5},
  url       = {https://openreview.net/forum?id=HyxQzBceg},
}

@Article{Smith2003_IteratedLearning,
  author    = {Smith, Kenny and Kirby, Simon and Brighton, Henry},
  journal   = {Artificial Life},
  title     = {Iterated Learning: A Framework for the Emergence of Language},
  year      = {2003},
  issn      = {1530-9185},
  month     = oct,
  number    = {4},
  pages     = {371--386},
  volume    = {9},
  doi       = {10.1162/106454603322694825},
  file      = {:Smith2003 - Iterated Learning_ a Framework for the Emergence of Language.pdf:PDF:https\://cse.iitk.ac.in/users/hk/cs784/papers/iteratedLearningFrameworkForEmergenceOfLang.pdf},
  groups    = {Language origins and evolution},
  priority  = {prio1},
  publisher = {MIT Press - Journals},
  ranking   = {rank3},
}

@Article{Brighton2006_TopographicMapping,
  author     = {Brighton, Henry and Kirby, Simon},
  journal    = {Artificial Life},
  title      = {Understanding Linguistic Evolution by Visualizing the Emergence of Topographic Mappings},
  year       = {2006},
  issn       = {1530-9185},
  month      = jan,
  number     = {2},
  pages      = {229--242},
  volume     = {12},
  doi        = {10.1162/artl.2006.12.2.229},
  file       = {:Brighton2006 - Understanding Linguistic Evolution by Visualizing the Emergence of Topographic Mappings.pdf:PDF:https\://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6a54db36afce548b5e4deefb3a4df0428a13a813},
  groups     = {Communication, Language origins and evolution},
  publisher  = {MIT Press - Journals},
  ranking    = {rank1},
  readstatus = {skimmed},
}

@InProceedings{Lewis2017,
  author    = {Lewis, Mike and Yarats, Denis and Dauphin, Yann and Parikh, Devi and Batra, Dhruv},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  title     = {Deal or No Deal? End-to-End Learning of Negotiation Dialogues},
  year      = {2017},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/d17-1259},
  groups    = {Dialogue agents},
  ranking   = {rank3},
}

@Article{Montague1970,
  author    = {Montague, Richard},
  journal   = {Theoria},
  title     = {Universal grammar},
  year      = {1970},
  issn      = {1755-2567},
  month     = dec,
  number    = {3},
  pages     = {373--398},
  volume    = {36},
  doi       = {10.1111/j.1755-2567.1970.tb00434.x},
  groups    = {Language, Language origins and evolution},
  publisher = {Wiley},
  ranking   = {rank5},
}

@Article{Karten2023a,
  author        = {Karten, Seth and Kailas, Siva and Li, Huao and Sycara, Katia},
  title         = {On the Role of Emergent Communication for Social Learning in Multi-Agent Reinforcement Learning},
  year          = {2023},
  month         = feb,
  abstract      = {Explicit communication among humans is key to coordinating and learning. Social learning, which uses cues from experts, can greatly benefit from the usage of explicit communication to align heterogeneous policies, reduce sample complexity, and solve partially observable tasks. Emergent communication, a type of explicit communication, studies the creation of an artificial language to encode a high task-utility message directly from data. However, in most cases, emergent communication sends insufficiently compressed messages with little or null information, which also may not be understandable to a third-party listener. This paper proposes an unsupervised method based on the information bottleneck to capture both referential complexity and task-specific utility to adequately explore sparse social communication scenarios in multi-agent reinforcement learning (MARL). We show that our model is able to i) develop a natural-language-inspired lexicon of messages that is independently composed of a set of emergent concepts, which span the observations and intents with minimal bits, ii) develop communication to align the action policies of heterogeneous agents with dissimilar feature models, and iii) learn a communication policy from watching an expert's action policy, which we term `social shadowing'.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2302.14276},
  eprint        = {2302.14276},
  file          = {:Karten2023a - On the Role of Emergent Communication for Social Learning in Multi Agent Reinforcement Learning.pdf:PDF:http\://arxiv.org/pdf/2302.14276v1},
  groups        = {Communication},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  priority      = {prio1},
  publisher     = {arXiv},
}

@InProceedings{Karten2023_CompoConcept,
  author    = {Karten, Seth and Kailas, Siva and Sycara, Katia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  title     = {Emergent Compositional Concept Communication through Mutual Information in Multi-Agent Teams},
  year      = {2023},
  address   = {Richland, SC},
  pages     = {2391–2393},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '23},
  abstract  = {In multi-agent reinforcement learning (MARL) with communication, coordination information (ordinal) is often required in addition to referential info about one's observations. The information bottleneck defines a trade-off between complexity and utility, which loses structure of latent information when compressed solely for utility. Thus, in this work, we use information theory to introduce information-rich, variational compositional communication to adequately embed referential information and to provide a contrastive objective to ground communication in intent-specific features without relying on reward. Each message is composed of a set of emergent concepts, which we show span the observations and intents. Messages are naturally compressed to the least number of bits.},
  file      = {:Karten2023_CompoConcept - Emergent Compositional Concept Communication through Mutual Information in Multi Agent Teams.pdf:PDF:https\://www.ifaamas.org/Proceedings/aamas2023/pdfs/p2391.pdf},
  groups    = {Communication},
  isbn      = {9781450394321},
  keywords  = {concept whitening, emergent communication, information theory, multi-agent reinforcement learning, sparse communication},
  location  = {, London, United Kingdom,},
  numpages  = {3},
  priority  = {prio2},
}

@Article{Zaslavsky2018_ColorNamingEvo,
  author    = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Efficient compression in color naming and its evolution},
  year      = {2018},
  issn      = {1091-6490},
  month     = jul,
  number    = {31},
  pages     = {7937--7942},
  volume    = {115},
  doi       = {10.1073/pnas.1800521115},
  file      = {:zaslavsky-et-al-2018-efficient-compression-in-color-naming-and-its-evolution.pdf:PDF},
  groups    = {Language origins and evolution},
  publisher = {Proceedings of the National Academy of Sciences},
  ranking   = {rank3},
}

@InProceedings{Higgins2017_BetaVAE,
  author    = {Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle = {International Conference on Learning Representations},
  title     = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
  year      = {2017},
  file      = {:Higgins2017_BetaVAE - Beta VAE_ Learning Basic Visual Concepts with a Constrained Variational Framework.pdf:PDF:https\://openreview.net/pdf?id=Sy2fzU9gl},
  groups    = {Auto-Encoding},
  ranking   = {rank5},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
}

@Book{Lewis1969_Convention,
  author    = {David Lewis},
  publisher = {Harvard University Press},
  title     = {Convention: A Philosophical Study},
  year      = {1969},
  groups    = {Communication},
  priority  = {prio1},
  ranking   = {rank5},
  url       = {https://eclass.upatras.gr/modules/document/file.php/CEID1184/0 Γενικά/David Lewis - Convention_ A Philosophical Study (2002, Wiley-Blackwell).pdf},
}

@Article{Lan2023,
  author        = {Lan, Yihuai and Hu, Zhiqiang and Wang, Lei and Wang, Yang and Ye, Deheng and Zhao, Peilin and Lim, Ee-Peng and Xiong, Hui and Wang, Hao},
  title         = {LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay},
  year          = {2023},
  month         = oct,
  abstract      = {This paper aims to investigate the open research problem of uncovering the social behaviors of LLM-based agents. To achieve this goal, we adopt Avalon, a representative communication game, as the environment and use system prompts to guide LLM agents to play the game. While previous studies have conducted preliminary investigations into gameplay with LLM agents, there lacks research on their social behaviors. In this paper, we present a novel framework designed to seamlessly adapt to Avalon gameplay. The core of our proposed framework is a multi-agent system that enables efficient communication and interaction among agents. We evaluate the performance of our framework based on metrics from two perspectives: winning the game and analyzing the social behaviors of LLM agents. Our results demonstrate the effectiveness of our framework in generating adaptive and intelligent agents and highlight the potential of LLM-based agents in addressing the challenges associated with dynamic social environment interaction. By analyzing the social behaviors of LLM agents from the aspects of both collaboration and confrontation, we provide insights into the research and applications of this domain. Our code is publicly available at https://github.com/3DAgentWorld/LLM-Game-Agent},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2310.14985},
  eprint        = {2310.14985},
  file          = {:Lan2023 - LLM Based Agent Society Investigation_ Collaboration and Confrontation in Avalon Gameplay.pdf:PDF:http\://arxiv.org/pdf/2310.14985v3},
  groups        = {LLM towns},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@InProceedings{Zhang2024_CoELA,
  author    = {Hongxin Zhang and Weihua Du and Jiaming Shan and Qinhong Zhou and Yilun Du and Joshua B. Tenenbaum and Tianmin Shu and Chuang Gan},
  booktitle = {International Conference on Learning Representations},
  title     = {Building Cooperative Embodied Agents Modularly with Large Language Models},
  year      = {2024},
  file      = {:Zhang2024 - Building Cooperative Embodied Agents Modularly with Large Language Models.pdf:PDF:https\://openreview.net/pdf?id=EnXJfQqy0K},
  groups    = {Comm with LLMs, Agent-based LLMs},
  url       = {https://openreview.net/forum?id=EnXJfQqy0K},
}

@Article{Perez2024_CulturalEvo,
  author        = {Perez, Jérémy and Léger, Corentin and Ovando-Tellez, Marcela and Foulon, Chris and Dussauld, Joan and Oudeyer, Pierre-Yves and Moulin-Frier, Clément},
  title         = {Cultural evolution in populations of Large Language Models},
  year          = {2024},
  month         = mar,
  abstract      = {Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake. Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution. We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed. The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2403.08882},
  eprint        = {2403.08882},
  file          = {:Perez2024 - Cultural Evolution in Populations of Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2403.08882v1},
  groups        = {LLM towns},
  keywords      = {Multiagent Systems (cs.MA), Artificial Intelligence (cs.AI), Populations and Evolution (q-bio.PE), FOS: Computer and information sciences, FOS: Biological sciences, I.2.7, 68T50},
  primaryclass  = {cs.MA},
  priority      = {prio2},
  publisher     = {arXiv},
}

@InProceedings{Lyu2021_DecentralisedCritic,
  author    = {Lyu, Xueguang and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
  booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  title     = {Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning},
  year      = {2021},
  address   = {Richland, SC},
  pages     = {844–852},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  series    = {AAMAS '21},
  abstract  = {Centralized Training for Decentralized Execution, where agents are trained offline using centralized information but execute in a decentralized manner online, has gained popularity in the multi-agent reinforcement learning community. In particular, actor-critic methods with a centralized critic and decentralized actors are a common instance of this idea. However, the implications of using a centralized critic in this context are not fully discussed and understood even though it is the standard choice of many algorithms. We therefore formally analyze centralized and decentralized critic approaches, providing a deeper understanding of the implications of critic choice. Because our theory makes unrealistic assumptions, we also empirically compare the centralized and decentralized critic methods over a wide set of environments to validate our theories and to provide practical advice. We show that there exist misconceptions regarding centralized critics in the current literature and show that the centralized critic design is not strictly beneficial, but rather both centralized and decentralized critics have different pros and cons that should be taken into account by algorithm designers.},
  file      = {:Lyu2021_DecentralisedCritic - Contrasting Centralized and Decentralized Critics in Multi Agent Reinforcement Learning.pdf:PDF:https\://www.ifaamas.org/Proceedings/aamas2021/pdfs/p844.pdf},
  groups    = {Independent Learning},
  keywords  = {reinforcement learning, policy gradient, multi-agent system},
  location  = {Virtual Event, United Kingdom},
  numpages  = {9},
  priority  = {prio1},
  ranking   = {rank2},
  url       = {https://dl.acm.org/doi/10.5555/3463952.3464053},
}

@Article{Brinkmann2023,
  author    = {Brinkmann, Levin and Baumann, Fabian and Bonnefon, Jean-François and Derex, Maxime and Müller, Thomas F. and Nussberger, Anne-Marie and Czaplicka, Agnieszka and Acerbi, Alberto and Griffiths, Thomas L. and Henrich, Joseph and Leibo, Joel Z. and McElreath, Richard and Oudeyer, Pierre-Yves and Stray, Jonathan and Rahwan, Iyad},
  journal   = {Nature Human Behaviour},
  title     = {Machine culture},
  year      = {2023},
  issn      = {2397-3374},
  month     = nov,
  number    = {11},
  pages     = {1855--1868},
  volume    = {7},
  doi       = {10.1038/s41562-023-01742-2},
  file      = {:Brinkmann2023 - Machine Culture.pdf:PDF:https\://arxiv.org/pdf/2311.11388},
  groups    = {Artificial Intelligence},
  priority  = {prio1},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Acerbi2023_LLMsBias,
  author    = {Acerbi, Alberto and Stubbersfield, Joseph M.},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Large language models show human-like content biases in transmission chain experiments},
  year      = {2023},
  issn      = {1091-6490},
  month     = oct,
  number    = {44},
  volume    = {120},
  doi       = {10.1073/pnas.2313790120},
  file      = {:acerbi-stubbersfield-2023-large-language-models-show-human-like-content-biases-in-transmission-chain-experiments-1.pdf:PDF},
  groups    = {Large Language Models},
  priority  = {prio2},
  publisher = {Proceedings of the National Academy of Sciences},
}

@Article{Vezhnevets2023_Concordia,
  author        = {Vezhnevets, Alexander Sasha and Agapiou, John P. and Aharon, Avia and Ziv, Ron and Matyas, Jayd and Duéñez-Guzmán, Edgar A. and Cunningham, William A. and Osindero, Simon and Karmon, Danny and Leibo, Joel Z.},
  title         = {Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia},
  year          = {2023},
  month         = dec,
  abstract      = {Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act "reasonably", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2312.03664},
  eprint        = {2312.03664},
  file          = {:Vezhnevets2023 - Generative Agent Based Modeling with Actions Grounded in Physical, Social, or Digital Space Using Concordia.pdf:PDF:http\://arxiv.org/pdf/2312.03664v2},
  groups        = {LLM towns, Agent-based LLMs},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@Article{Hinton2006_Autoencoder,
  author    = {Hinton, G. E. and Salakhutdinov, R. R.},
  journal   = {Science},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  year      = {2006},
  issn      = {1095-9203},
  month     = jul,
  number    = {5786},
  pages     = {504--507},
  volume    = {313},
  doi       = {10.1126/science.1127647},
  groups    = {Auto-Encoding},
  publisher = {American Association for the Advancement of Science (AAAS)},
  ranking   = {rank5},
}

@InProceedings{Hunt2024_LLMMultiRobot,
  author    = {Hunt, William and Godfrey, Toby and Soorati, Mohammad D.},
  booktitle = {Demonstration at International Conference on Autonomous Agents and Multi-Agent Systems},
  title     = {Conversational Language Models for Human-in-the-Loop Multi-Robot Coordination},
  year      = {2024},
  abstract  = {With the increasing prevalence and diversity of robots interacting in the real world, there is need for flexible, on-the-fly planning and cooperation. Large Language Models are starting to be explored in a multimodal setup for communication, coordination, and planning in robotics. Existing approaches generally use a single agent building a plan, or have multiple homogeneous agents coordinating for a simple task. We present a decentralised, dialogical approach in which a team of agents with different abilities plans solutions through peer-to-peer and human-robot discussion. We suggest that argument-style dialogues are an effective way to facilitate adaptive use of each agent's abilities within a cooperative team. Two robots discuss how to solve a cleaning problem set by a human, define roles, and agree on paths they each take. Each step can be interrupted by a human advisor and agents check their plans with the human. Agents then execute this plan in the real world, collecting rubbish from people in each room. Our implementation uses text at every step, maintaining transparency and effective human-multi-robot interaction.},
  file      = {:Hunt2024 - Conversational Language Models for Human in the Loop Multi Robot Coordination.pdf:PDF:http\://arxiv.org/pdf/2402.19166v1},
  groups    = {Comm with LLMs},
  keywords  = {Robotics (cs.RO), FOS: Computer and information sciences},
  url       = {https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p2809.pdf},
}

@InProceedings{Ekila2024_LingConvent,
  author     = {Botoko Ekila, J\'{e}r\^{o}me},
  booktitle  = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
  title      = {Emergence of Linguistic Conventions In Multi-Agent Systems Through Situated Communicative Interactions},
  year       = {2024},
  address    = {Richland, SC},
  pages      = {2725–2727},
  publisher  = {International Foundation for Autonomous Agents and Multiagent Systems},
  series     = {AAMAS '24},
  abstract   = {The field of emergent communication investigates the emergence of shared linguistic conventions among autonomous agents engaged in cooperative tasks that require communication. Conventions that arise through self-organisation are known to be more robust, flexible, and adaptive, and it removes the need for hand-crafting communication protocols. In my PhD research, I investigate how artificial agents can co-construct such conventions of linguistic structures in reference-based tasks. This problem is tackled using the language game experimental paradigm which aims to model the processes underlying the emergence and evolution of human languages. My primary contribution thus far introduces a novel methodology for the language game paradigm in the emergent setting. Using the methodology, agents can establish through self-organisation an emergent language that enables them to refer to arbitrary entities in their environment using single-word utterances. For the first time, the methodology is directly applicable to any dataset that describes entities in terms of continuously-valued features. The next phase in my research is to move from single-word utterances to multi-word utterances through the emergence of grammatical structures.},
  file       = {:Ekila2024_LingConvent - Emergence of Linguistic Conventions in Multi Agent Systems through Situated Communicative Interactions.pdf:PDF:https\://arxiv.org/pdf/2401.08461},
  groups     = {Discrete language, Language origins and evolution},
  keywords   = {autonomous agents, emergent communication, language emergence, language evolution, multi-agent systems, self-organisation},
  location   = {Auckland, New Zealand},
  numpages   = {3},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/10.5555/3635637.3663267},
}

@InProceedings{Liu2024_HLA,
  author    = {Liu, Jijia and Yu, Chao and Gao, Jiaxuan and Xie, Yuqing and Liao, Qingmin and Wu, Yi and Wang, Yu},
  booktitle = {International Conference on Autonomous Agents and Multiagent Systems},
  title     = {LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination},
  year      = {2024},
  abstract  = {AI agents powered by Large Language Models (LLMs) have made significant advances, enabling them to assist humans in diverse complex tasks and leading to a revolution in human-AI coordination. LLM-powered agents typically require invoking LLM APIs and employing artificially designed complex prompts, which results in high inference latency. While this paradigm works well in scenarios with minimal interactive demands, such as code generation, it is unsuitable for highly interactive and real-time applications, such as gaming. Traditional gaming AI often employs small models or reactive policies, enabling fast inference but offering limited task completion and interaction abilities. In this work, we consider Overcooked as our testbed where players could communicate with natural language and cooperate to serve orders. We propose a Hierarchical Language Agent (HLA) for human-AI coordination that provides both strong reasoning abilities while keeping real-time execution. In particular, HLA adopts a hierarchical framework and comprises three modules: a proficient LLM, referred to as Slow Mind, for intention reasoning and language interaction, a lightweight LLM, referred to as Fast Mind, for generating macro actions, and a reactive policy, referred to as Executor, for transforming macro actions into atomic actions. Human studies show that HLA outperforms other baseline agents, including slow-mind-only agents and fast-mind-only agents, with stronger cooperation abilities, faster responses, and more consistent language communications.},
  file      = {:Liu2023 - LLM Powered Hierarchical Language Agent for Real Time Human AI Coordination.pdf:PDF:http\://arxiv.org/pdf/2312.15224v2},
  groups    = {Comm with LLMs},
  keywords  = {Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences},
  priority  = {prio1},
  url       = {https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p1219.pdf},
}

@InProceedings{Choi2018_CompComm,
  author    = {Edward Choi and Angeliki Lazaridou and Nando de Freitas},
  booktitle = {International Conference on Learning Representations},
  title     = {Multi-Agent Compositional Communication Learning from Raw Visual Input},
  year      = {2018},
  file      = {:Choi2018_CompComm - Multi Agent Compositional Communication Learning from Raw Visual Input.pdf:PDF:https\://openreview.net/pdf?id=rknt2Be0-},
  groups    = {Communication},
  ranking   = {rank1},
  url       = {https://openreview.net/forum?id=rknt2Be0-},
}

@Article{Zhu2024a,
  author    = {Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
  journal   = {Autonomous Agents and Multi-Agent Systems},
  title     = {A survey of multi-agent deep reinforcement learning with communication},
  year      = {2024},
  issn      = {1573-7454},
  month     = jan,
  number    = {1},
  volume    = {38},
  doi       = {10.1007/s10458-023-09633-6},
  file      = {:s10458-023-09633-6.pdf:PDF},
  groups    = {Communication},
  priority  = {prio1},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Gupta2023,
  author    = {Gupta, Nikunj and Srinivasaraghavan, G. and Mohalik, Swarup and Kumar, Nishant and Taylor, Matthew E.},
  journal   = {Neural Computing and Applications},
  title     = {HAMMER: Multi-level coordination of reinforcement learning agents via learned messaging},
  year      = {2023},
  issn      = {1433-3058},
  month     = oct,
  doi       = {10.1007/s00521-023-09096-6},
  file      = {:Gupta2023 - HAMMER_ Multi Level Coordination of Reinforcement Learning Agents Via Learned Messaging.pdf:PDF:https\://arxiv.org/pdf/2102.00824},
  groups    = {Communication},
  priority  = {prio1},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Tsitsiklis1994,
  author    = {Tsitsiklis, John N.},
  journal   = {Machine Learning},
  title     = {Asynchronous stochastic approximation and Q-learning},
  year      = {1994},
  issn      = {1573-0565},
  month     = sep,
  number    = {3},
  pages     = {185--202},
  volume    = {16},
  comment   = {Proof of convergence of Q-learning},
  doi       = {10.1007/bf00993306},
  groups    = {Value based},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank5},
}

@Article{Swarup2009,
  author     = {Swarup, Samarth and Gasser, Les},
  journal    = {Adaptive Behavior},
  title      = {The Iterated Classification Game: A New Model of the Cultural Transmission of Language},
  year       = {2009},
  issn       = {1741-2633},
  month      = jun,
  number     = {3},
  pages      = {213--235},
  volume     = {17},
  doi        = {10.1177/1059712309105818},
  file       = {:Swarup2009 - The Iterated Classification Game_ a New Model of the Cultural Transmission of Language.pdf:PDF:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC2828955/pdf/nihms170010.pdf},
  groups     = {Language origins and evolution},
  publisher  = {SAGE Publications},
  readstatus = {read},
}

@Article{Ji2023_Hallucinations,
  author    = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal   = {ACM Computing Surveys},
  title     = {Survey of Hallucination in Natural Language Generation},
  year      = {2023},
  issn      = {1557-7341},
  month     = mar,
  number    = {12},
  pages     = {1--38},
  volume    = {55},
  doi       = {10.1145/3571730},
  groups    = {Large Language Models},
  publisher = {Association for Computing Machinery (ACM)},
  ranking   = {rank5},
}

@Article{Hong2023_MetaGPT,
  author        = {Hong, Sirui and Zhuge, Mingchen and Chen, Jonathan and Zheng, Xiawu and Cheng, Yuheng and Zhang, Ceyao and Wang, Jinlin and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin and Schmidhuber, Jürgen},
  title         = {MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework},
  year          = {2023},
  month         = aug,
  abstract      = {Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2308.00352},
  eprint        = {2308.00352},
  file          = {:Hong2023 - MetaGPT_ Meta Programming for a Multi Agent Collaborative Framework.pdf:PDF:http\://arxiv.org/pdf/2308.00352v5},
  groups        = {LLM towns},
  keywords      = {Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@Article{Vogt2005_EmergenceCompo,
  author     = {Vogt, Paul},
  journal    = {Artificial Intelligence},
  title      = {The emergence of compositional structures in perceptually grounded language games},
  year       = {2005},
  issn       = {0004-3702},
  month      = sep,
  number     = {1–2},
  pages      = {206--242},
  volume     = {167},
  doi        = {10.1016/j.artint.2005.04.010},
  file       = {:1-s2.0-S0004370205001001-main.pdf:PDF},
  groups     = {Language origins and evolution, Symbol grounding},
  publisher  = {Elsevier BV},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{Nagel1974,
  author    = {Nagel, Thomas},
  journal   = {The Philosophical Review},
  title     = {What Is It Like to Be a Bat?},
  year      = {1974},
  issn      = {0031-8108},
  month     = oct,
  number    = {4},
  pages     = {435},
  volume    = {83},
  doi       = {10.2307/2183914},
  file      = {:Nagel1974 - What Is It like to Be a Bat_.pdf:PDF:https\://www.sas.upenn.edu/~cavitch/pdf-library/Nagel_Bat.pdf},
  groups    = {Conciousness},
  publisher = {JSTOR},
}

@Article{Searle1980,
  author    = {Searle, John R.},
  journal   = {Behavioral and Brain Sciences},
  title     = {Minds, brains, and programs},
  year      = {1980},
  issn      = {1469-1825},
  month     = sep,
  number    = {3},
  pages     = {417--424},
  volume    = {3},
  doi       = {10.1017/s0140525x00005756},
  file      = {:Searle1980 - Minds, Brains, and Programs.pdf:PDF:https\://cse.buffalo.edu/~rapaport/Papers/Papers.by.Others/Searle/searle80-MindsBrainsProgs-BBS.pdf},
  groups    = {Artificial Intelligence},
  publisher = {Cambridge University Press (CUP)},
  ranking   = {rank5},
}

@Article{Hinton1987_LearningEvolution,
  author  = {Hinton, Geoffrey E and Nowlan, Steven J and others},
  journal = {Complex systems},
  title   = {How learning can guide evolution},
  year    = {1987},
  number  = {3},
  pages   = {495--502},
  volume  = {1},
  file    = {:Hinton1987_LearningEvolution - How Learning Can Guide Evolution.pdf:PDF:https\://icts.res.in/sites/default/files/How%20Learning%20Can%20Guide%20Evolution%20-1987.pdf},
  groups  = {Evolution},
  ranking = {rank4},
  url     = {https://icts.res.in/sites/default/files/How Learning Can Guide Evolution -1987.pdf},
}

@Article{Baldwin1896_BaldwinEffect,
  author    = {Baldwin, J. Mark},
  journal   = {The American Naturalist},
  title     = {A New Factor in Evolution},
  year      = {1896},
  issn      = {1537-5323},
  month     = jun,
  number    = {354},
  pages     = {441--451},
  volume    = {30},
  doi       = {10.1086/276408},
  file      = {:Baldwin1896_BaldwinEffect - A New Factor in Evolution.pdf:PDF:https\://web.archive.org/web/20180721170610id_/http\://www.diacronia.ro/en/journal/issue/7/A112/en/pdf},
  groups    = {Evolution},
  publisher = {University of Chicago Press},
  ranking   = {rank5},
}

@Book{Steels2015,
  author    = {Steels, Luc L.},
  publisher = {Language Science Press},
  title     = {{The} {Talking} {Heads} experiment},
  year      = {2015},
  address   = {Berlin},
  number    = {1},
  series    = {Computational Models of Language Evolution},
  doi       = {10.17169/FUDOCS_document_000000022455},
  groups    = {Language origins and evolution},
  ranking   = {rank2},
  subtitle  = {{Origins} of words and meanings},
}

@Article{Li2024,
  author        = {Li, Junkai and Wang, Siyu and Zhang, Meng and Li, Weitao and Lai, Yunghwei and Kang, Xinhui and Ma, Weizhi and Liu, Yang},
  title         = {Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents},
  year          = {2024},
  month         = may,
  abstract      = {In this paper, we introduce a simulacrum of hospital called Agent Hospital that simulates the entire process of treating illness. All patients, nurses, and doctors are autonomous agents powered by large language models (LLMs). Our central goal is to enable a doctor agent to learn how to treat illness within the simulacrum. To do so, we propose a method called MedAgent-Zero. As the simulacrum can simulate disease onset and progression based on knowledge bases and LLMs, doctor agents can keep accumulating experience from both successful and unsuccessful cases. Simulation experiments show that the treatment performance of doctor agents consistently improves on various tasks. More interestingly, the knowledge the doctor agents have acquired in Agent Hospital is applicable to real-world medicare benchmarks. After treating around ten thousand patients (real-world doctors may take over two years), the evolved doctor agent achieves a state-of-the-art accuracy of 93.06% on a subset of the MedQA dataset that covers major respiratory diseases. This work paves the way for advancing the applications of LLM-powered agent techniques in medical scenarios.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2405.02957},
  eprint        = {2405.02957},
  file          = {:http\://arxiv.org/pdf/2405.02957v1:PDF},
  groups        = {LLM towns},
  keywords      = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
}

@TechReport{Rummery1994_SARSA,
  author      = {Rummery, G. A. and Niranjan, M.},
  institution = {Engineering Department, Cambridge University},
  title       = {On-line Q-learning using connectionnist systems},
  year        = {1994},
  booktitle   = {Technical Report CUED/F-INFENG/TR 166},
  groups      = {Value based},
  url         = {https://www.researchgate.net/profile/Mahesan-Niranjan/publication/2500611_On-Line_Q-Learning_Using_Connectionist_Systems/links/5438d5db0cf204cab1d6db0f/On-Line-Q-Learning-Using-Connectionist-Systems.pdf?_sg[0]=HYd0h230b7WOR6m4hj5yx01K97aS61Z0DufUURMQr9ZqMqcEVZ0dNpG84h6uCfRl_M40FNkXgRX-GnpnxH31Ww.jBF3fgrlhaJYs3bDEaHQU22nRpKP0zKeF_oOsqh7WddL8pfxAomPSbeANzdmLP9YPB26HbLeSaEJqhFgzIxvWQ&_sg[1]=CZtZhHTEMgSwBZrpZU_7BACd8RH04JUKiITdXRQJ6MQ9SFS27jreZmcsuNcqYYWRoxcwBE-xBMbrfl1QobmEZ65bmkmpzonq5JoLRIIUKXne.jBF3fgrlhaJYs3bDEaHQU22nRpKP0zKeF_oOsqh7WddL8pfxAomPSbeANzdmLP9YPB26HbLeSaEJqhFgzIxvWQ&_iepl=},
}

@InProceedings{Gupta2022,
  author    = {Gupta, Abhishek and Pacchiano, Aldo and Zhai, Yuexiang and Kakade, Sham and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {15281--15295},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Gupta2022 - Unpacking Reward Shaping_ Understanding the Benefits of Reward Engineering on Sample Complexity.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2022/file/6255f22349da5f2126dfc0b007075450-Paper-Conference.pdf},
  groups    = {Reward Shaping},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/6255f22349da5f2126dfc0b007075450-Paper-Conference.pdf},
}

@InProceedings{Kakade2002ApproximatelyOA,
  author    = {Kakade, Sham and Langford, John},
  booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
  title     = {Approximately Optimal Approximate Reinforcement Learning},
  year      = {2002},
  address   = {San Francisco, CA, USA},
  pages     = {267–274},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series    = {ICML '02},
  comment   = {basis of TRPO's surrogate objective},
  file      = {:Kakade2002ApproximatelyOA - Approximately Optimal Approximate Reinforcement Learning.pdf:PDF:https\://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf},
  groups    = {TRPO-PPO},
  isbn      = {1558608737},
  numpages  = {8},
  url       = {https://dl.acm.org/doi/10.5555/645531.656005},
}

@PhdThesis{Ziebart2010_MaxEntropyRL,
  author    = {Ziebart, Brian D},
  title     = {Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  year      = {2010},
  groups    = {Soft Actor Critic},
  publisher = {Carnegie Mellon University},
  url       = {http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/bziebart/publications/thesis-bziebart.pdf},
}

@Article{Haarnoja2018_SAC2,
  author        = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  title         = {Soft Actor-Critic Algorithms and Applications},
  year          = {2018},
  month         = dec,
  abstract      = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1812.05905},
  eprint        = {1812.05905},
  file          = {:Haarnoja2018 - Soft Actor Critic Algorithms and Applications.pdf:PDF:http\://arxiv.org/pdf/1812.05905v2},
  groups        = {Soft Actor Critic},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank5},
}

@InProceedings{Steels1997_Grounding,
  author    = {L. Steels and P.A. Vogt},
  booktitle = {Advances in artificial life},
  title     = {Grounding adaptive language games in robotic agents},
  year      = {1997},
  editor    = {P. Husbands and I. Harvey},
  pages     = {474--482},
  publisher = {MIT Press},
  series    = {Complex adaptive systems},
  volume    = {4},
  file      = {:Steels1997_Grounding - Grounding Adaptive Language Games in Robotic Agents.pdf:PDF:https\://pure.uvt.nl/ws/portalfiles/portal/752834/groundingad.pdf},
  groups    = {Language origins and evolution, Symbol grounding},
  isbn      = {0262581574},
  language  = {English},
  ranking   = {rank2},
}

@Article{Vogt2002_PhysicalGrounding,
  author    = {Vogt, Paul},
  journal   = {Cognitive Systems Research},
  title     = {The physical symbol grounding problem},
  year      = {2002},
  issn      = {1389-0417},
  month     = sep,
  number    = {3},
  pages     = {429--457},
  volume    = {3},
  doi       = {10.1016/s1389-0417(02)00051-7},
  file      = {:1-s2.0-S1389041702000517-main.pdf:PDF},
  groups    = {Language origins and evolution, Symbol grounding},
  publisher = {Elsevier BV},
  ranking   = {rank2},
}

@Article{Beckner2009,
  author    = {Beckner, Clay and Blythe, Richard and Bybee, Joan and Christiansen, Morten H. and Croft, William and Ellis, Nick C. and Holland, John and Ke, Jinyun and Larsen‐Freeman, Diane and Schoenemann, Tom},
  journal   = {Language Learning},
  title     = {Language Is a Complex Adaptive System: Position Paper},
  year      = {2009},
  issn      = {1467-9922},
  month     = nov,
  number    = {s1},
  pages     = {1--26},
  volume    = {59},
  doi       = {10.1111/j.1467-9922.2009.00533.x},
  file      = {:Language Learning - 2009 - - Language Is a Complex Adaptive System Position Paper.pdf:PDF},
  groups    = {Language origins and evolution},
  priority  = {prio1},
  publisher = {Wiley},
  ranking   = {rank5},
}

@Article{steels2005,
  author    = {Steels, Luc and Belpaeme, Tony},
  journal   = {Behavioral and Brain Sciences},
  title     = {Coordinating Perceptually Grounded Categories Through Language: A Case Study for Colour},
  year      = {2005},
  issn      = {1469-1825},
  month     = aug,
  number    = {4},
  pages     = {469--489},
  volume    = {28},
  doi       = {10.1017/s0140525x05000087},
  file      = {:steels2005 - Coordinating Perceptually Grounded Categories through Language_ a Case Study for Colour.pdf:PDF:https\://digital.csic.es/bitstream/10261/128299/1/Coordinating%20Perceptually.pdf},
  groups    = {Language origins and evolution},
  publisher = {Cambridge University Press (CUP)},
  ranking   = {rank3},
}

@Article{Beuls2013_GrammaticalAgreement,
  author    = {Beuls, Katrien and Steels, Luc},
  journal   = {PLoS ONE},
  title     = {Agent-Based Models of Strategies for the Emergence and Evolution of Grammatical Agreement},
  year      = {2013},
  issn      = {1932-6203},
  month     = mar,
  number    = {3},
  pages     = {e58960},
  volume    = {8},
  doi       = {10.1371/journal.pone.0058960},
  editor    = {Solé, Ricard V.},
  file      = {:Beuls2013 - Agent Based Models of Strategies for the Emergence and Evolution of Grammatical Agreement.pdf:PDF:https\://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0058960&type=printable},
  groups    = {Language origins and evolution},
  priority  = {prio1},
  publisher = {Public Library of Science (PLoS)},
  ranking   = {rank2},
}

@Article{Nevens2020,
  author   = {Nevens, Jens and Van Eecke, Paul and Beuls, Katrien},
  journal  = {Frontiers in Robotics and AI},
  title    = {From Continuous Observations to Symbolic Concepts: A Discrimination-Based Strategy for Grounded Concept Learning},
  year     = {2020},
  issn     = {2296-9144},
  volume   = {7},
  abstract = {Autonomous agents perceive the world through streams of continuous sensori-motor data. Yet, in order to reason and communicate about their environment, agents need to be able to distill meaningful concepts from their raw observations. Most current approaches that bridge between the continuous and symbolic domain are using deep learning techniques. While these approaches often achieve high levels of accuracy, they rely on large amounts of training data, and the resulting models lack transparency, generality, and adaptivity. In this paper, we introduce a novel methodology for grounded concept learning. In a tutor-learner scenario, the method allows an agent to construct a conceptual system in which meaningful concepts are formed by discriminative combinations of prototypical values on human-interpretable feature channels. We evaluate our approach on the CLEVR dataset, using features that are either simulated or extracted using computer vision techniques. Through a range of experiments, we show that our method allows for incremental learning, needs few data points, and that the resulting concepts are general enough to be applied to previously unseen objects and can be combined compositionally. These properties make the approach well-suited to be used in robotic agents as the module that maps from continuous sensory input to grounded, symbolic concepts that can then be used for higher-level reasoning tasks.},
  doi      = {10.3389/frobt.2020.00084},
  file     = {:Nevens2020 - From Continuous Observations to Symbolic Concepts_ a Discrimination Based Strategy for Grounded Concept Learning.pdf:PDF:https\://pure.unamur.be/ws/portalfiles/portal/83862520/frobt_07_00084.pdf},
  groups   = {Symbol grounding},
  url      = {https://www.frontiersin.org/articles/10.3389/frobt.2020.00084},
}

@Article{Wellens2008_FlexibleWordMeaning,
  author    = {Wellens, Peter and Loetzsch, Martin and Steels, Luc},
  journal   = {Connection Science},
  title     = {Flexible word meaning in embodied agents},
  year      = {2008},
  issn      = {1360-0494},
  month     = sep,
  number    = {2–3},
  pages     = {173--191},
  volume    = {20},
  doi       = {10.1080/09540090802091966},
  file      = {:Flexible word meaning in embodied agents.pdf:PDF},
  groups    = {Language origins and evolution},
  priority  = {prio1},
  publisher = {Informa UK Limited},
  ranking   = {rank1},
}

@Article{Kirby2001_ILM,
  author    = {Kirby, S.},
  journal   = {IEEE Transactions on Evolutionary Computation},
  title     = {Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity},
  year      = {2001},
  issn      = {1089-778X},
  month     = apr,
  number    = {2},
  pages     = {102--110},
  volume    = {5},
  doi       = {10.1109/4235.918430},
  file      = {:Kirby2001 - Spontaneous Evolution of Linguistic Structure an Iterated Learning Model of the Emergence of Regularity and Irregularity.pdf:PDF:https\://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e5bc94f7f841b83012d3895839f539bf85270532},
  groups    = {Language origins and evolution},
  priority  = {prio1},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank3},
}

@Book{Steels1999_TalkingHeads1,
  author   = {Luc Steels},
  editor   = {Special pre-edition for LABORATORIUM},
  title    = {The Talking Heads Experiment. Volume I. Words and Meanings},
  year     = {1999},
  groups   = {Language origins and evolution},
  priority = {prio1},
  ranking  = {rank2},
}

@InProceedings{Zuidema2002,
  author    = {Zuidema, Willem},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {How the Poverty of the Stimulus Solves the Poverty of the Stimulus},
  year      = {2002},
  editor    = {S. Becker and S. Thrun and K. Obermayer},
  publisher = {MIT Press},
  volume    = {15},
  file      = {:Zuidema2002 - How the Poverty of the Stimulus Solves the Poverty of the Stimulus.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2002/file/04ad5632029cbfbed8e136e5f6f7ddfa-Paper.pdf},
  groups    = {Language origins and evolution},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2002/file/04ad5632029cbfbed8e136e5f6f7ddfa-Paper.pdf},
}

@Book{Albrecht2024,
  author     = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  publisher  = {MIT Press},
  title      = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  year       = {2024},
  file       = {:Albrecht2024 - Multi Agent Reinforcement Learning_ Foundations and Modern Approaches.pdf:PDF:https\://www.marl-book.com/download/marl-book.pdf},
  groups     = {MAS Reviews},
  ranking    = {rank1},
  readstatus = {read},
  url        = {https://www.marl-book.com},
}

@Book{Shoham2008_Multiagent,
  author    = {Shoham, Yoav and Leyton-Brown, Kevin},
  publisher = {Cambridge University Press},
  title     = {Multiagent systems: Algorithmic, game-theoretic, and logical foundations},
  year      = {2008},
  groups    = {MAS Reviews},
  ranking   = {rank5},
  url       = {http://eecs.harvard.edu/cs286r/courses/fall08/files/SLB.pdf},
}

@Article{Albrecht2018_AgentModelling,
  author    = {Albrecht, Stefano V. and Stone, Peter},
  journal   = {Artificial Intelligence},
  title     = {Autonomous agents modelling other agents: A comprehensive survey and open problems},
  year      = {2018},
  issn      = {0004-3702},
  month     = may,
  pages     = {66--95},
  volume    = {258},
  doi       = {10.1016/j.artint.2018.01.002},
  groups    = {Agent Modelling},
  priority  = {prio1},
  publisher = {Elsevier BV},
  ranking   = {rank3},
}

@Book{Owen2013_GameTheory,
  author    = {Owen, Guillermo},
  publisher = {Emerald Group Publishing},
  title     = {Game theory},
  year      = {2013},
  groups    = {MAS Reviews, Game theory},
  ranking   = {rank5},
  url       = {https://books.google.fr/books?hl=en&lr=&id=yeVbAAAAQBAJ&oi=fnd&pg=PP2&ots=Yp1-UuOAie&sig=1IaP-ESJcbpI7rs7FjB2KmkQdsQ&redir_esc=y#v=onepage&q&f=false},
}

@Book{Harsanyi1988_EqSec,
  author    = {John C. Harsanyi and Reinhard Selten},
  publisher = {The MIT Press},
  title     = {{A General Theory of Equilibrium Selection in Games}},
  year      = {1988},
  isbn      = {ARRAY(0x56531e08)},
  month     = {December},
  number    = {0262582384},
  series    = {MIT Press Books},
  volume    = {1},
  abstract  = {The authors, two of the most prominent game theorists of this generation, have devoted a number of years to the development of the theory presented here, and to its economic applications. They propose rational criteria for selecting one particular uniformly perfect equilibrium point as the solution of any noncooperative game. And, because any cooperative game can be remodelled as a noncooperative bargaining game, their theory defines a one-point solution for any cooperative game as well. By providing solutions - based on the same principles of rational behavior - for all classes of games, both cooperative and noncooperative, both those with complete and with incomplete information, Harsanyi and Selten's approach achieves a remarkable degree of theoretical unification for game theory as a whole and provides a deeper insight into the nature of game-theoretic rationality. The book applies this theory to a number of specific game classes, such as unanimity games; bargaining with transaction costs; trade involving one seller and several buyers; two-person bargaining with incomplete information on one side, and on both sides. The last chapter discusses the relationship of the authors' theory to other recently proposed solution concepts, particularly the Kohberg-Mertens stability theory.},
  groups    = {MAS Reviews, Equilibrium selection},
  keywords  = {game theory; equlibrium selection},
  url       = {https://ideas.repec.org/b/mtp/titles/0262582384.html},
}

@Article{Nash1950,
  author    = {Nash, John F.},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Equilibrium points in n -person games},
  year      = {1950},
  issn      = {1091-6490},
  month     = jan,
  number    = {1},
  pages     = {48--49},
  volume    = {36},
  doi       = {10.1073/pnas.36.1.48},
  groups    = {Game theory},
  publisher = {Proceedings of the National Academy of Sciences},
  ranking   = {rank5},
}

@Article{Fudenberg1993,
  author    = {Fudenberg, Drew and Levine, David K.},
  journal   = {Econometrica},
  title     = {Steady State Learning and Nash Equilibrium},
  year      = {1993},
  issn      = {0012-9682},
  month     = may,
  number    = {3},
  pages     = {547},
  volume    = {61},
  doi       = {10.2307/2951717},
  groups    = {Agent Modelling},
  publisher = {JSTOR},
  ranking   = {rank2},
}

@Article{Lanctot2023RockPaperScissors,
  author   = {Marc Lanctot and John Schultz and Neil Burch and Max Olan Smith and Daniel Hennes and Thomas Anthony and Julien Perolat},
  journal  = {Transactions on Machine Learning Research},
  title    = {Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning},
  year     = {2023},
  issn     = {2835-8856},
  file     = {:Lanctot2023RockPaperScissors - Population Based Evaluation in Repeated Rock Paper Scissors As a Benchmark for Multiagent Reinforcement Learning.pdf:PDF:https\://openreview.net/pdf?id=gQnJ7ODIAx},
  groups   = {Multi-Agent Environments, Agent Modelling},
  priority = {prio1},
  url      = {https://openreview.net/forum?id=gQnJ7ODIAx},
}

@InProceedings{Papoudakis2021_LIAM,
  author    = {Papoudakis, Georgios and Christianos, Filippos and Albrecht, Stefano},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Agent Modelling under Partial Observability for Deep Reinforcement Learning},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {19210--19222},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  file      = {:Papoudakis2021_LIAM - Agent Modelling under Partial Observability for Deep Reinforcement Learning.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2021/file/a03caec56cd82478bf197475b48c05f9-Paper.pdf},
  groups    = {Agent Modelling},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/a03caec56cd82478bf197475b48c05f9-Paper.pdf},
}

@Article{Graves2016_DNC,
  author    = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  journal   = {Nature},
  title     = {Hybrid computing using a neural network with dynamic external memory},
  year      = {2016},
  issn      = {1476-4687},
  month     = oct,
  number    = {7626},
  pages     = {471--476},
  volume    = {538},
  comment   = {Differentiable Neural Computer},
  doi       = {10.1038/nature20101},
  file      = {:Graves2016 - Hybrid Computing Using a Neural Network with Dynamic External Memory.pdf:PDF:https\://clgiles.ist.psu.edu/IST597/materials/papers-computing/DL-computing/2016-graves.pdf},
  groups    = {Memory},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank5},
}

@InProceedings{Brown1951_FictitiousPlay,
  author    = {George W. Brown},
  booktitle = {Proceedingsof the Conference on Activity Analysis of Production and Allocation, Cowles CommissionMonograph 13},
  title     = {Iterative solution of games by fictitious play},
  year      = {1951},
  pages     = {374-376},
  groups    = {Agent Modelling, Fictituous Play},
  ranking   = {rank5},
  url       = {https://econwpa.ub.uni-muenchen.de/econ-wp/game/papers/0503/0503008.pdf},
}

@Article{Robinson1951_FictitiousPlay,
  author    = {Robinson, Julia},
  journal   = {The Annals of Mathematics},
  title     = {An Iterative Method of Solving a Game},
  year      = {1951},
  issn      = {0003-486X},
  month     = sep,
  number    = {2},
  pages     = {296},
  volume    = {54},
  doi       = {10.2307/1969530},
  groups    = {Agent Modelling, Fictituous Play},
  publisher = {JSTOR},
  ranking   = {rank5},
}

@Article{Fudenberg1995_FictitiousPlay,
  author    = {Fudenberg, Drew and Levine, David K.},
  journal   = {Journal of Economic Dynamics and Control},
  title     = {Consistency and cautious fictitious play},
  year      = {1995},
  issn      = {0165-1889},
  month     = jul,
  number    = {5–7},
  pages     = {1065--1089},
  volume    = {19},
  doi       = {10.1016/0165-1889(94)00819-4},
  groups    = {Fictituous Play},
  publisher = {Elsevier BV},
  ranking   = {rank3},
}

@Article{Hofbauer2002_FictitiousPlay,
  author    = {Hofbauer, Josef and Sandholm, William H.},
  journal   = {Econometrica},
  title     = {On the Global Convergence of Stochastic Fictitious Play},
  year      = {2002},
  issn      = {1468-0262},
  month     = nov,
  number    = {6},
  pages     = {2265--2294},
  volume    = {70},
  doi       = {10.1111/j.1468-0262.2002.00440.x},
  groups    = {Fictituous Play},
  publisher = {The Econometric Society},
  ranking   = {rank3},
}

@InProceedings{Heinrich2015_FictituousPlay,
  author    = {Heinrich, Johannes and Lanctot, Marc and Silver, David},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  title     = {Fictitious Self-Play in Extensive-Form Games},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Bach, Francis and Blei, David},
  month     = {07--09 Jul},
  pages     = {805--813},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  abstract  = {Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.},
  groups    = {Fictituous Play},
  pdf       = {http://proceedings.mlr.press/v37/heinrich15.pdf},
  ranking   = {rank3},
  url       = {https://proceedings.mlr.press/v37/heinrich15.html},
}

@Article{Jordan1991_Bayesian,
  author    = {Jordan, J.S},
  journal   = {Games and Economic Behavior},
  title     = {Bayesian learning in normal form games},
  year      = {1991},
  issn      = {0899-8256},
  month     = feb,
  number    = {1},
  pages     = {60--81},
  volume    = {3},
  doi       = {10.1016/0899-8256(91)90005-y},
  groups    = {Bayesian learning},
  publisher = {Elsevier BV},
  ranking   = {rank3},
}

@Article{Kalai1993_Rational,
  author    = {Kalai, Ehud and Lehrer, Ehud},
  journal   = {Econometrica},
  title     = {Rational Learning Leads to Nash Equilibrium},
  year      = {1993},
  issn      = {0012-9682},
  month     = sep,
  number    = {5},
  pages     = {1019},
  volume    = {61},
  doi       = {10.2307/2951492},
  groups    = {Bayesian learning},
  publisher = {JSTOR},
  ranking   = {rank4},
}

@Article{VanEecke2022,
  author    = {Van Eecke, Paul and Beuls, Katrien and Botoko Ekila, Jérôme and Rădulescu, Roxana},
  journal   = {Journal of Language Evolution},
  title     = {Language games meet multi-agent reinforcement learning: A case study for the naming game},
  year      = {2022},
  issn      = {2058-458X},
  month     = jul,
  number    = {2},
  pages     = {213--223},
  volume    = {7},
  doi       = {10.1093/jole/lzad001},
  file      = {:VanEecke2022 - Language Games Meet Multi Agent Reinforcement Learning_ a Case Study for the Naming Game.pdf:PDF:https\://watermark.silverchair.com/lzad001.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA0owggNGBgkqhkiG9w0BBwagggM3MIIDMwIBADCCAywGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMM220hCvjm2XLuqhNAgEQgIIC_SUkOhWKKuTB8JAc3EJ0je-6GDrJln5RQFqMFyPpVtUiXTBwKexY_fQBihdRzRHP4KZ76PJnFWlEX7L0VRX4akXGUaThsbVDXnV3t2B19udpkeg9C900VyeW414HFQFdJdEq6wzvwe8KrFXXFRq_5L9P-SINJExTnWHGRnqkaFbs1-ZsIXKFxicj6YiqgjfqiPj3sSTtG4jKMUdqACjY54YaPi-drrpRZVsfu47S_Wdjx3ZZYr9kLaKUO749QqqgYSd0lhPwI71Nk-FDNijUTn6Oc6H2t6I4jsI3sU_4KuW2FIJ7-DjaXGG8BqdFx64k4PVVslhcnSR9CHGYWEyeN2XZ2cgo-f1EDCIwmAGhM_nbGVrQAnC2RsKQoP73Mmw-u0yh_MvVx6gYLVde0CREhhogplKWbMY0IRi5LXiQTIdjmCrpmZfJOr_UqePf9O3V6vuhb9lvYXYJZ8S_KZp5Bq7DEWkdRUcrVzgHrs3VeCxhfBriaApuC_KaXR_IJNgSt3XVj2zq-sVTyE85p96SI9hpW7cpjWrQFntp9iK0olw-igviwymywbWL7G5zgd7j8OtNH9u3OH2oeW7SkK3gs9bpmrxDGr0bjskaHGFGleu3lkIxkE0ulDEPLDdieEp73lC5liqdIox-7RnzYw-7egwygZ9k3UXxUe-wbYycBWieDq6ixDAdyVhhNwwyDhDGybhrTP-74nKUnNml2UMgq9sRuUtSOuyXy7Ar3GSxfenYj_2Mxo6Q4YWj6NxhUg_K9yB5pZEtdi-Bm5q-v69BHGQguq2yDD6Eo1-Ly8N-TTbK06Zksu2bhlC3AZtVb3H5qM0cRHINM8SYo-wouf-YgbWIb4tZ4mBBVj-kZ7ov7d5sqDPhVOasQ8qBsjsQreVqdd50nbAMxCpOiKy8qgv-TGiuvx1RMniVb03VaY9_nkHIgz2QjSbCDCbk7dxsWYFx-r65IAk1izvLfF0Dw9LwTXfe9woJqkywY4yHGbaZmASbTzZ0cQvb4K-9_5F-lg},
  groups    = {Language origins and evolution, Communication, Discrete language},
  priority  = {prio1},
  publisher = {Oxford University Press (OUP)},
}

@InBook{Mikolov2018,
  author    = {Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
  pages     = {29--61},
  publisher = {Springer International Publishing},
  title     = {A Roadmap Towards Machine Intelligence},
  year      = {2018},
  isbn      = {9783319754772},
  booktitle = {Lecture Notes in Computer Science},
  doi       = {10.1007/978-3-319-75477-2_2},
  file      = {:ARoadmaptowardsMachineIntelligence.pdf:PDF},
  groups    = {Communication},
  issn      = {1611-3349},
  priority  = {prio1},
  ranking   = {rank2},
}

@Article{Minaee2024,
  author        = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  title         = {Large Language Models: A Survey},
  year          = {2024},
  month         = feb,
  abstract      = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2402.06196},
  eprint        = {2402.06196},
  file          = {:Minaee2024 - Large Language Models_ a Survey.pdf:PDF:http\://arxiv.org/pdf/2402.06196v2},
  groups        = {Large Language Models},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
  ranking       = {rank1},
}

@Book{Weiss1999_MAS,
  author    = {Weiss, Gerhard},
  editor    = {Weiss, Gerhard},
  publisher = {MIT Press},
  title     = {Multiagent systems: a modern approach to distributed artificial intelligence},
  year      = {1999},
  address   = {Cambridge, MA, USA},
  isbn      = {0262232030},
  groups    = {MAS Reviews},
  ranking   = {rank5},
}

@InProceedings{Ellis2023,
  author    = {Ellis, Benjamin and Cook, Jonathan and Moalla, Skander and Samvelyan, Mikayel and Sun, Mingfei and Mahajan, Anuj and Foerster, Jakob and Whiteson, Shimon},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning},
  year      = {2023},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {37567--37593},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  file      = {:Ellis2023 - SMACv2_ an Improved Benchmark for Cooperative Multi Agent Reinforcement Learning.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2023/file/764c18ad230f9e7bf6a77ffc2312c55e-Paper-Datasets_and_Benchmarks.pdf},
  groups    = {Multi-Agent Environments},
  ranking   = {rank1},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/764c18ad230f9e7bf6a77ffc2312c55e-Paper-Datasets_and_Benchmarks.pdf},
}

@InProceedings{octo_2023,
  author    = {{Octo Model Team} and Dibya Ghosh and Homer Walke and Karl Pertsch and Kevin Black and Oier Mees and Sudeep Dasari and Joey Hejna and Charles Xu and Jianlan Luo and Tobias Kreiman and {You Liang} Tan and Lawrence Yunliang Chen and Pannag Sanketi and Quan Vuong and Ted Xiao and Dorsa Sadigh and Chelsea Finn and Sergey Levine},
  booktitle = {Proceedings of Robotics: Science and Systems},
  title     = {Octo: An Open-Source Generalist Robot Policy},
  year      = {2024},
  address   = {Delft, Netherlands},
  groups    = {Vision Language Action Models},
}

@Article{Kim2024,
  author        = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
  title         = {OpenVLA: An Open-Source Vision-Language-Action Model},
  year          = {2024},
  month         = jun,
  abstract      = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2406.09246},
  eprint        = {2406.09246},
  file          = {:Kim2024 - OpenVLA_ an Open Source Vision Language Action Model.pdf:PDF:http\://arxiv.org/pdf/2406.09246v1},
  groups        = {Vision Language Action Models},
  keywords      = {Robotics (cs.RO), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
}

@Article{Fudenberg1993a,
  author    = {Fudenberg, Drew and Kreps, David M.},
  journal   = {Games and Economic Behavior},
  title     = {Learning Mixed Equilibria},
  year      = {1993},
  issn      = {0899-8256},
  month     = jul,
  number    = {3},
  pages     = {320--367},
  volume    = {5},
  doi       = {10.1006/game.1993.1021},
  groups    = {Fictituous Play},
  publisher = {Elsevier BV},
  ranking   = {rank3},
}

@InProceedings{Sen1994_CoordQlearning,
  author    = {Sen, Sandip and Sekaran, Mahendra and Hale, John},
  booktitle = {Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence},
  title     = {Learning to coordinate without sharing information},
  year      = {1994},
  pages     = {426–431},
  publisher = {AAAI Press},
  series    = {AAAI'94},
  abstract  = {Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes.},
  groups    = {Multi-agent RL, Independent Learning},
  location  = {Seattle, Washington},
  numpages  = {6},
  ranking   = {rank3},
  url       = {https://dl.acm.org/doi/abs/10.5555/2891730.2891796},
}

@Book{LeytonBrown2008_GameTheory,
  author    = {Leyton-Brown, Kevin and Shoham, Yoav},
  publisher = {Springer International Publishing},
  title     = {Essentials of Game Theory: A Concise, Multidisciplinary Introduction},
  year      = {2008},
  isbn      = {9783031015458},
  doi       = {10.1007/978-3-031-01545-8},
  groups    = {Game theory},
  journal   = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  ranking   = {rank3},
}

@InProceedings{Li2023,
  author    = {Li, Huao and Chong, Yu and Stepputtis, Simon and Campbell, Joseph and Hughes, Dana and Lewis, Charles and Sycara, Katia},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  title     = {Theory of Mind for Multi-Agent Collaboration via Large Language Models},
  year      = {2023},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.emnlp-main.13},
  file      = {:Li2023 - Theory of Mind for Multi Agent Collaboration Via Large Language Models.pdf:PDF:https\://aclanthology.org/2023.emnlp-main.13.pdf},
  groups    = {Agent-based LLMs, Theory of Mind},
  ranking   = {rank1},
}

@InProceedings{Guo2024_LLMsMA,
  author       = {Guo, T and Chen, X and Wang, Y and Chang, R and Pei, S and Chawla, NV and Wiest, O and Zhang, X},
  booktitle    = {33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)},
  title        = {Large Language Model based Multi-Agents: A Survey of Progress and Challenges.},
  year         = {2024},
  abstractnote = {Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.},
  file         = {:Guo2024_LLMsMA - Large Language Model Based Multi Agents_ a Survey of Progress and Challenges..pdf:PDF:https\://arxiv.org/pdf/2402.01680},
  groups       = {Agent-based LLMs},
  place        = {Country unknown/Code not available},
  priority     = {prio2},
  url          = {https://par.nsf.gov/biblio/10508149},
}

@Article{Orr2023_MADRLRobots,
  author    = {Orr, James and Dutta, Ayan},
  journal   = {Sensors},
  title     = {Multi-Agent Deep Reinforcement Learning for Multi-Robot Applications: A Survey},
  year      = {2023},
  issn      = {1424-8220},
  month     = mar,
  number    = {7},
  pages     = {3625},
  volume    = {23},
  doi       = {10.3390/s23073625},
  file      = {:Orr2023 - Multi Agent Deep Reinforcement Learning for Multi Robot Applications_ a Survey.pdf:PDF:https\://pdfs.semanticscholar.org/2581/887cd2848fc28a78019360953e2046368326.pdf},
  groups    = {MADRL in Robotics},
  priority  = {prio1},
  publisher = {MDPI AG},
  ranking   = {rank1},
}

@InProceedings{Huang2020,
  author    = {Huang, Yixin and Wu, Shufan and Mu, Zhongcheng and Long, Xiangyu and Chu, Sunhao and Zhao, Guohong},
  booktitle = {2020 6th International Conference on Control, Automation and Robotics (ICCAR)},
  title     = {A Multi-agent Reinforcement Learning Method for Swarm Robots in Space Collaborative Exploration},
  year      = {2020},
  month     = apr,
  publisher = {IEEE},
  doi       = {10.1109/iccar49639.2020.9107997},
  file      = {:https\://www.researchgate.net/profile/Yixin-Huang-10/publication/341918517_A_Multi-agent_Reinforcement_Learning_Method_for_Swarm_Robots_in_Space_Collaborative_Exploration/links/60580407299bf173675c2fe7/A-Multi-agent-Reinforcement-Learning-Method-for-Swarm-Robots-in-Space-Collaborative-Exploration.pdf:},
  groups    = {Space Exploration},
  ranking   = {rank1},
}

@Article{Dutta2021,
  author    = {Dutta, Ayan and Roy, Swapnoneel and Kreidl, O. Patrick and Boloni, Ladislau},
  journal   = {IEEE Access},
  title     = {Multi-Robot Information Gathering for Precision Agriculture: Current State, Scope, and Challenges},
  year      = {2021},
  issn      = {2169-3536},
  pages     = {161416--161430},
  volume    = {9},
  doi       = {10.1109/access.2021.3130900},
  groups    = {Multi-robot, Information gathering},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank1},
}

@Article{Zhou2022,
  author    = {Zhou, Ziye and Liu, Jincun and Yu, Junzhi},
  journal   = {IEEE/CAA Journal of Automatica Sinica},
  title     = {A Survey of Underwater Multi-Robot Systems},
  year      = {2022},
  issn      = {2329-9274},
  month     = jan,
  number    = {1},
  pages     = {1--18},
  volume    = {9},
  doi       = {10.1109/jas.2021.1004269},
  groups    = {Multi-robot},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  ranking   = {rank1},
}

@Article{Yliniemi2014,
  author    = {Yliniemi, Logan and Agogino, Adrian K. and Tumer, Kagan},
  journal   = {AI Magazine},
  title     = {Multirobot Coordination for Space Exploration},
  year      = {2014},
  issn      = {2371-9621},
  month     = dec,
  number    = {4},
  pages     = {61--74},
  volume    = {35},
  doi       = {10.1609/aimag.v35i4.2556},
  groups    = {Multi-robot, Space Exploration},
  publisher = {Wiley},
  ranking   = {rank1},
}

@Article{Galceran2013,
  author    = {Galceran, Enric and Carreras, Marc},
  journal   = {Robotics and Autonomous Systems},
  title     = {A survey on coverage path planning for robotics},
  year      = {2013},
  issn      = {0921-8890},
  month     = dec,
  number    = {12},
  pages     = {1258--1276},
  volume    = {61},
  doi       = {10.1016/j.robot.2013.09.004},
  groups    = {Coverage Path Planning},
  publisher = {Elsevier BV},
  ranking   = {rank4},
}

@Article{Oroojlooy2022,
  author    = {Oroojlooy, Afshin and Hajinezhad, Davood},
  journal   = {Applied Intelligence},
  title     = {A review of cooperative multi-agent deep reinforcement learning},
  year      = {2022},
  issn      = {1573-7497},
  month     = oct,
  number    = {11},
  pages     = {13677--13722},
  volume    = {53},
  doi       = {10.1007/s10489-022-04105-y},
  file      = {:Oroojlooy2022 - A Review of Cooperative Multi Agent Deep Reinforcement Learning.pdf:PDF:https\://arxiv.org/pdf/1908.03963},
  groups    = {MAS Reviews},
  priority  = {prio1},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank3},
}

@Article{Wong2022,
  author    = {Wong, Annie and Bäck, Thomas and Kononova, Anna V. and Plaat, Aske},
  journal   = {Artificial Intelligence Review},
  title     = {Deep multiagent reinforcement learning: challenges and directions},
  year      = {2022},
  issn      = {1573-7462},
  month     = oct,
  number    = {6},
  pages     = {5023--5056},
  volume    = {56},
  doi       = {10.1007/s10462-022-10299-x},
  file      = {:s10462-022-10299-x.pdf:PDF},
  groups    = {MAS Reviews},
  priority  = {prio1},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{moulinfrier:hal-03051029,
  author      = {Moulin-Frier, Cl{\'e}ment and Oudeyer, Pierre-Yves},
  booktitle   = {{COMARL AAAI 2020-2021 - Challenges and Opportunities for Multi-Agent Reinforcement Learning, AAAI Spring Symposium Series}},
  title       = {{Multi-Agent Reinforcement Learning as a Computational Tool for Language Evolution Research: Historical Context and Future Challenges}},
  year        = {2021},
  address     = {Palo Alto, California / Virtual, United States},
  month       = Feb,
  series      = {COMARL AAAI 2020-2021 - Challenges and Opportunities for Multi-Agent Reinforcement Learning, AAAI Spring Symposium Series},
  groups      = {MAS Reviews, Communication},
  hal_id      = {hal-03051029},
  hal_version = {v1},
  pdf         = {https://hal.science/hal-03051029/file/2002.08878.pdf},
  url         = {https://hal.science/hal-03051029},
}

@Article{Papoudakis2019,
  author        = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V.},
  title         = {Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning},
  year          = {2019},
  month         = jun,
  abstract      = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent's policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1906.04737},
  eprint        = {1906.04737},
  file          = {:Papoudakis2019 - Dealing with Non Stationarity in Multi Agent Deep Reinforcement Learning.pdf:PDF:http\://arxiv.org/pdf/1906.04737v1},
  groups        = {MAS Reviews},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  ranking       = {rank2},
}

@Article{Tuyls2012,
  author    = {Tuyls, Karl and Weiss, Gerhard},
  journal   = {AI Magazine},
  title     = {Multiagent Learning: Basics, Challenges, and Prospects},
  year      = {2012},
  issn      = {2371-9621},
  month     = sep,
  number    = {3},
  pages     = {41--52},
  volume    = {33},
  doi       = {10.1609/aimag.v33i3.2426},
  file      = {:2426-Article Text-4046-1-10-20121011.pdf:PDF},
  groups    = {MAS Reviews},
  publisher = {Wiley},
  ranking   = {rank3},
}

@Article{Doran1995_EOS,
  author  = {Doran, Jim and Palmer, Mike},
  journal = {Artificial Societies: The ComputerSimulation of Social Life},
  title   = {The EOS project: integrating two models of Palaeolithic social change},
  year    = {1995},
  pages   = {103--125},
  groups  = {MAS in Social Sciences},
  ranking = {rank1},
  url     = {https://www.taylorfrancis.com/chapters/oa-edit/10.4324/9780203993699-13/eos-project-integrating-two-models-palaeolithic-social-change-jim-doran-mike-palmer},
}

@Article{Barreteau2000_SHADOC,
  author    = {Barreteau, Olivier and Bousquet, François},
  journal   = {Annals of Operations Research},
  title     = {SHADOC: a multi‐agent model to tackle viability of irrigated systems},
  year      = {2000},
  issn      = {0254-5330},
  number    = {1/4},
  pages     = {139--162},
  volume    = {94},
  doi       = {10.1023/a:1018908931155},
  groups    = {MAS in Economics},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank3},
}

@Article{Bousquet2004_MASEcosystem,
  author    = {Bousquet, F and Le Page, C},
  journal   = {Ecological Modelling},
  title     = {Multi-agent simulations and ecosystem management: a review},
  year      = {2004},
  issn      = {0304-3800},
  month     = sep,
  number    = {3–4},
  pages     = {313--332},
  volume    = {176},
  doi       = {10.1016/j.ecolmodel.2004.01.011},
  groups    = {MAS in Social Sciences},
  publisher = {Elsevier BV},
  ranking   = {rank4},
}

@Book{Hamill2015_ABMEconomics,
  author    = {Hamill, L and Gilbert, N},
  publisher = {John Wiley & Sons},
  title     = {Agent-Based Modelling in Economics},
  year      = {2015},
  isbn      = {1118456076},
  abstract  = {New methods of economic modelling have been sought as a result of the global economic downturn in 2008. This unique book highlights the benefits of an agent-based modelling (ABM) approach. It demonstrates how ABM can easily handle complexity: heterogeneous people, households and firms interacting dynamically. Unlike traditional methods, ABM does not require people or firms to optimise or economic systems to reach equilibrium. ABM offers a way to link micro foundations directly to the macro situation.   Key features: •         Introduces the concept of agent-based modelling and shows how it differs from existing approaches.  •         Provides a theoretical and methodological rationale for using ABM in economics, along with practical advice on how to design and create the models. •         Starts each chapter with a short summary of the relevant economic theory and then shows how to apply ABM.  •         Explores both topics covered in basic economics textbooks and current important policy themes; unemployment, exchange rates, banking and environmental issues. •         Describes the models in pseudocode, enabling the reader to develop programs in their chosen language. •         Is supported by a website featuring the NetLogo models described in the book. Agent-based Modelling in Economics provides students and researchers with the skills to design, implement, and analyze agent-based models. Third year undergraduate, master and doctoral students, faculty and professional economists will find this book an invaluable resource.},
  groups    = {MAS in Economics, MAS Reviews},
  keywords  = {Economics;computer simulation;Economics";" computer simulation},
  pages     = {272},
  ranking   = {rank2},
}

@Book{Ben-Ari2006_DistribProg,
  author    = {Ben-Ari, M.},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  title     = {Principles of Concurrent and Distributed Programming (2nd Edition)},
  year      = {2006},
  isbn      = {032131283X},
  groups    = {MAS in Software Engineering},
}

@Book{Rosenschein1994_Encounter,
  author    = {Jeffrey S. Rosenschein and Gilad Zlotkin},
  publisher = {pub-mit},
  title     = {Rules of Encounter},
  year      = {1994},
  isbn      = {0262181592},
  abstract  = {Rules of Encounter applies the general approach and
                  the mathematical tools of game theory in a formal
                  analysis of rules (or protocols) governing the
                  high-level behavior of interacting heterogeneous
                  computer systems. It describes a theory of
                  high-level protocol design that can be used to
                  constrain manipulation and harness the potential of
                  automated negotiation and coordination strategies to
                  attain more effective interaction among machines
                  that have been programmed by different entities to
                  pursue different goals. While game theoretic ideas
                  have been used to answer the question of how a
                  computer should be programmed to act in a given
                  specific interaction, here they are used in a new
                  way, to address the question of how to design the
                  rules of interaction themselves for automated
                  agents. Rules of Encounter provides a unified,
                  coherent account of machine interaction at the level
                  of the machine designers (the society of designers)
                  and the level of the machine interaction itself (the
                  resulting artificial society). Taking into account
                  such attributes of the artificial society as
                  efficiency, and the self-interest of each member in
                  the society of designers, it analyzes what kinds of
                  rules should be instituted to govern interaction
                  among these autonomous agents. The authors point out
                  that adjusting the rules of public behavior -- or
                  the rules of the game -- by which the programs must
                  interact can influence the private strategies that
                  designers set up in their machines, shaping design
                  choices and run-time behavior, as well as social
                  behavior. Artificial Intelligence series.},
  googleid  = {mD24BWOMbgYJ:scholar.google.com/},
  groups    = {MAS in Economics, MAS Reviews, Game theory},
  ranking   = {rank5},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Artificial Intelligence\;0\;1\;\;\;\;;
1 StaticGroup:Biology\;0\;1\;\;\;\;;
2 StaticGroup:Conciousness\;0\;1\;\;\;\;;
2 StaticGroup:Evolution\;0\;1\;\;\;\;;
1 StaticGroup:Data Augmentation\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Developmental psychology\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Evolution Strategy\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Evolutionary RL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Imitation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Language\;2\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Dialogue agents\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Language for memory\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Language generation with RL\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Large Language Models\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Agent-based LLMs\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:LLM towns\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Question Answering\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Text-based environments\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Visual-Language Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Language origins and evolution\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Symbol grounding\;0\;1\;0x7c7c7cff\;\;\;;
1 StaticGroup:Machine Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Computer Vision\;0\;0\;\;\;\;;
2 StaticGroup:Confidence\;0\;1\;0x7c7c7cff\;\;\;;
2 StaticGroup:Contrastive Learning\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Deep Learning\;0\;0\;\;\;\;;
2 StaticGroup:Multi-Task Learning\;0\;1\;0x838383ff\;\;\;;
2 StaticGroup:NLP\;0\;0\;0x838383ff\;\;\;;
2 StaticGroup:Auto-Encoding\;0\;0\;0x838383ff\;\;\;;
1 StaticGroup:Multi-agent Systems\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:MAS Reviews\;2\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-Agent Environments\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-agent learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:MAS in Social Sciences\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:MAS in Economics\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:MAS in Software Engineering\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Shared Mental Models\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-agent RL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Agent Modelling\;2\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:Fictituous Play\;2\;0\;0x7c7c7cff\;\;\;;
4 StaticGroup:Bayesian learning\;0\;1\;0x7c7c7cff\;\;\;;
4 StaticGroup:Theory of Mind\;0\;1\;0x7c7c7cff\;\;\;;
3 StaticGroup:Centralised Training and Execution\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Independent Learning\;0\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:IDQN\;0\;0\;0x7c7c7cff\;\;\;;
3 StaticGroup:Communication\;2\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:Discrete language\;2\;1\;0x8a8a8aff\;\;\;;
5 StaticGroup:Natural language\;0\;0\;0x8a8a8aff\;\;\;;
4 StaticGroup:Speaker-Listener\;0\;0\;0x8a8a8aff\;\;\;;
4 StaticGroup:Language-Grounded Communication\;0\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:Comm with LLMs\;0\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:Applied MAC\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Credit Assignment\;2\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:Shapley value\;2\;0\;0x8a8a8aff\;\;\;;
4 StaticGroup:Wonderful Life Utility\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Equilibrium selection\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Exploration in MARL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Game theory\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Graphs\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Heterogeneous MADRL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Interaction Networks\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Intrinsic rewards in MARL\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Joint Action Learning\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Memory in MARL\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Multi-task\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Population-based training\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Social Learning\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Value factorisation\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Memory\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Meta-Learning\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Curriculum Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:ZPD\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:RL\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Continual RL\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Exploration\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Environments\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Hierarchical RL\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Macro-actions\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Intrinsic goals\;2\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Adversary Guidance\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Curiosity\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Disagreement\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Diversity\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Random Exploration\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Visitation count\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Language-Augmented RL\;2\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:LA Reward shaping\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Model based\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Latent Dynamics Model\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Model-based exploration\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Policy based\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Actor Critic\;2\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:DDPG\;0\;0\;0x8a8a8aff\;\;\;;
4 StaticGroup:Soft Actor Critic\;0\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:TRPO-PPO\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Value based\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Goal-conditioned RL\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:POMDP\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:RL in Biology and Neuroscience\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:RL in Economics\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:RL in Robotics\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Offline RL\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Open-Ended Learning\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:RLHF\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Safe RL\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Transformers in RL\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Reward Shaping\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Implementation tricks in RL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Robotics\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Autonomous Driving\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Coverage Path Planning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Information gathering\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Interactivity\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-robot\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Heterogeneous\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Multi-Robot Communication\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:MADRL in Robotics\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Multi-task robotics\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Navigation\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Platforms\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Sim to Real\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Space Exploration\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Visual-and-Language Navigation\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:Vision Language Action Models\;0\;1\;0x8a8a8aff\;\;\;;
}
