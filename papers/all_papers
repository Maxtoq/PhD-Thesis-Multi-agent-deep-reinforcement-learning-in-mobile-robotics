% Encoding: UTF-8

@Article{Haarnoja2018,
  author        = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  year          = {2018},
  month         = jan,
  abstract      = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  comment       = {Soft Actor Critic},
  eprint        = {1801.01290},
  file          = {:Haarnoja2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF},
  groups        = {Policy based, Actor Critic},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Fujimoto2018,
  author        = {Scott Fujimoto and Herke van Hoof and David Meger},
  title         = {Addressing Function Approximation Error in Actor-Critic Methods},
  year          = {2018},
  month         = feb,
  abstract      = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  comment       = {Twin Delayed DDPG},
  eprint        = {1802.09477},
  file          = {:Fujimoto2018 - Addressing Function Approximation Error in Actor Critic Methods.pdf:PDF},
  groups        = {Policy based, Actor Critic, DDPG},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
  readstatus    = {read},
}

@Article{Schulman2017,
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  comment       = {PPO},
  eprint        = {1707.06347},
  file          = {:Schulman2017 - Proximal Policy Optimization Algorithms.pdf:PDF},
  groups        = {Policy based, Actor Critic, TRPO},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Schulman2015,
  author        = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  title         = {Trust Region Policy Optimization},
  year          = {2015},
  month         = feb,
  abstract      = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  comment       = {TRPO},
  eprint        = {1502.05477},
  file          = {:Schulman2015 - Trust Region Policy Optimization.pdf:PDF},
  groups        = {Policy based, Actor Critic, TRPO},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Schrittwieser2019,
  author        = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
  title         = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  year          = {2019},
  month         = nov,
  abstract      = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  comment       = {Muzero},
  doi           = {10.1038/s41586-020-03051-4},
  eprint        = {1911.08265},
  file          = {:Schrittwieser2019 - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hafner2019,
  author        = {Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
  title         = {Dream to Control: Learning Behaviors by Latent Imagination},
  year          = {2019},
  month         = dec,
  abstract      = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  archiveprefix = {arXiv},
  comment       = {Dreamer},
  eprint        = {1912.01603},
  file          = {:Hafner2019 - Dream to Control_ Learning Behaviors by Latent Imagination.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Pathak2017,
  author        = {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  title         = {Curiosity-driven Exploration by Self-supervised Prediction},
  year          = {2017},
  month         = may,
  abstract      = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  archiveprefix = {arXiv},
  comment       = {Intrinsinc Curiosity Module},
  eprint        = {1705.05363},
  file          = {:Pathak2017 - Curiosity Driven Exploration by Self Supervised Prediction.pdf:PDF},
  groups        = {Intrinsic goals, Curiosity},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Burda2018,
  author        = {Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
  title         = {Exploration by Random Network Distillation},
  year          = {2018},
  month         = oct,
  abstract      = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  archiveprefix = {arXiv},
  eprint        = {1810.12894},
  file          = {:Burda2018 - Exploration by Random Network Distillation.pdf:PDF},
  groups        = {Intrinsic goals, Curiosity},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Lillicrap2015,
  author        = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  title         = {Continuous control with deep reinforcement learning},
  year          = {2015},
  month         = sep,
  abstract      = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  comment       = {Deep Deterministic Policy Gradient},
  eprint        = {1509.02971},
  file          = {:Lillicrap2015 - Continuous Control with Deep Reinforcement Learning.pdf:PDF},
  groups        = {Policy based, Actor Critic, DDPG},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Misc{Williams1992,
  author     = {Ronald J. Williams},
  title      = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  year       = {1992},
  comment    = {Reinforce},
  doi        = {10.1007/978-1-4615-3618-5_2},
  file       = {:Williams1992_Article_SimpleStatisticalGradient-foll.pdf:PDF},
  groups     = {Policy based},
  pages      = {5-32},
  readstatus = {read},
}

@Article{Zhang2018,
  author        = {Marvin Zhang and Sharad Vikram and Laura Smith and Pieter Abbeel and Matthew J. Johnson and Sergey Levine},
  title         = {SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning},
  year          = {2018},
  month         = aug,
  abstract      = {Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.},
  archiveprefix = {arXiv},
  eprint        = {1808.09105},
  file          = {:Zhang2018 - SOLAR_ Deep Structured Representations for Model Based Reinforcement Learning.pdf:PDF},
  groups        = {Model based},
  keywords      = {cs.LG, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Sekar2020,
  author        = {Ramanan Sekar and Oleh Rybkin and Kostas Daniilidis and Pieter Abbeel and Danijar Hafner and Deepak Pathak},
  title         = {Planning to Explore via Self-Supervised World Models},
  year          = {2020},
  month         = may,
  abstract      = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/},
  archiveprefix = {arXiv},
  eprint        = {2005.05960},
  file          = {:Sekar2020 - Planning to Explore Via Self Supervised World Models.pdf:PDF},
  groups        = {Model based, Disagreement},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.NE, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Pathak2019,
  author        = {Deepak Pathak and Dhiraj Gandhi and Abhinav Gupta},
  title         = {Self-Supervised Exploration via Disagreement},
  year          = {2019},
  month         = jun,
  abstract      = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/},
  archiveprefix = {arXiv},
  eprint        = {1906.04161},
  file          = {:Pathak2019 - Self Supervised Exploration Via Disagreement.pdf:PDF},
  groups        = {Disagreement},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Hessel2017,
  author        = {Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
  title         = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  year          = {2017},
  month         = oct,
  abstract      = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archiveprefix = {arXiv},
  eprint        = {1710.02298},
  file          = {:Hessel2017 - Rainbow_ Combining Improvements in Deep Reinforcement Learning.pdf:PDF},
  groups        = {Value based},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
}

@Article{Vecerik2017,
  author        = {Mel Vecerik and Todd Hester and Jonathan Scholz and Fumin Wang and Olivier Pietquin and Bilal Piot and Nicolas Heess and Thomas Rothörl and Thomas Lampe and Martin Riedmiller},
  title         = {Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sampling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Normalized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kinesthetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.},
  archiveprefix = {arXiv},
  eprint        = {1707.08817},
  file          = {:Vecerik2017 - Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards.pdf:PDF},
  groups        = {DDPG, Robotics},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Eysenbach2018,
  author        = {Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
  title         = {Diversity is All You Need: Learning Skills without a Reward Function},
  year          = {2018},
  month         = feb,
  abstract      = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  archiveprefix = {arXiv},
  eprint        = {1802.06070},
  file          = {:Eysenbach2018 - Diversity Is All You Need_ Learning Skills without a Reward Function.pdf:PDF},
  groups        = {Diversity},
  keywords      = {cs.AI, cs.RO},
  primaryclass  = {cs.AI},
  readstatus    = {skimmed},
}

@Article{Burda2018a,
  author        = {Yuri Burda and Harri Edwards and Deepak Pathak and Amos Storkey and Trevor Darrell and Alexei A. Efros},
  title         = {Large-Scale Study of Curiosity-Driven Learning},
  year          = {2018},
  month         = aug,
  abstract      = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  archiveprefix = {arXiv},
  eprint        = {1808.04355},
  file          = {:Burda2018a - Large Scale Study of Curiosity Driven Learning.pdf:PDF},
  groups        = {Curiosity},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Achiam2017,
  author        = {Joshua Achiam and Shankar Sastry},
  title         = {Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},
  year          = {2017},
  month         = mar,
  abstract      = {Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.},
  archiveprefix = {arXiv},
  eprint        = {1703.01732},
  file          = {:Achiam2017 - Surprise Based Intrinsic Motivation for Deep Reinforcement Learning.pdf:PDF},
  groups        = {Curiosity},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {skimmed},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Policy based\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Actor Critic\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:TRPO\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:DDPG\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Model based\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Intrinsic goals\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Curiosity\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Disagreement\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Diversity\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Value based\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Robotics\;0\;1\;0x8a8a8aff\;\;\;;
}
