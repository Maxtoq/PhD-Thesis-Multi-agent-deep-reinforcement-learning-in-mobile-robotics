% Encoding: UTF-8

@Article{Tesauro1995,
  author    = {Gerald Tesauro},
  journal   = {Commun. {ACM}},
  title     = {Temporal Difference Learning and TD-Gammon},
  year      = {1995},
  number    = {3},
  pages     = {58--68},
  volume    = {38},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/cacm/Tesauro95.bib},
  doi       = {10.1145/203330.203343},
  groups    = {Introduction, TD Learning},
}

@Article{Sutton1988,
  author = {Richard S. Sutton},
  title  = {Learning to predict by the methods of temporal differences},
  year   = {1988},
  issn   = {0885-6125},
  pages  = {9-44},
  volume = {3},
  doi    = {10.1007/bf00115009},
  groups = {Introduction, TD Learning},
}

@Misc{Tanner2005,
  author   = {Brian Tanner and Richard S. Sutton},
  title    = {TD(λ) networks},
  year     = {2005},
  doi      = {10.1145/1102351.1102463},
  groups   = {Introduction, TD Learning},
  subtitle = {temporal-difference networks with eligibility traces},
}

@Article{Silver2012,
  author = {David Silver and Richard S. Sutton and Martin Müller},
  title  = {Temporal-difference search in computer Go},
  year   = {2012},
  issn   = {0885-6125},
  pages  = {183-219},
  volume = {87},
  doi    = {10.1007/s10994-012-5280-0},
  groups = {Introduction, TD Learning},
}

@InProceedings{Kohl2004,
  author    = {Nate Kohl and Peter Stone},
  booktitle = {Proceedings of the 2004 {IEEE} International Conference on Robotics and Automation, {ICRA} 2004, April 26 - May 1, 2004, New Orleans, LA, {USA}},
  title     = {Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion},
  year      = {2004},
  pages     = {2619--2624},
  publisher = {{IEEE}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/icra/KohlS04.bib},
  doi       = {10.1109/ROBOT.2004.1307456},
  groups    = {Policy Gradient},
}

@InProceedings{Ng2004,
  author    = {Andrew Y. Ng and Adam Coates and Mark Diel and Varun Ganapathi and Jamie Schulte and Ben Tse and Eric Berger and Eric Liang},
  booktitle = {Experimental Robotics IX, The 9th International Symposium on Experimental Robotics {[ISER} 2004, Singapore, 18.-21. June 2004]},
  title     = {Autonomous Inverted Helicopter Flight via Reinforcement Learning},
  year      = {2004},
  editor    = {Marcelo H. Ang Jr. and Oussama Khatib},
  pages     = {363--372},
  publisher = {Springer},
  series    = {Springer Tracts in Advanced Robotics},
  volume    = {21},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iser/NgCDGSTBL04.bib},
  doi       = {10.1007/11552246\_35},
  groups    = {Policy Gradient},
  url       = {https://doi.org/10.1007/11552246_35},
}

@Article{Williams1992,
  author    = {Ronald J. Williams},
  journal   = {Mach. Learn.},
  title     = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  year      = {1992},
  pages     = {229--256},
  volume    = {8},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ml/Williams92.bib},
  doi       = {10.1007/BF00992696},
  groups    = {Policy Gradient},
}

@InProceedings{Sutton1990,
  author    = {Richard S. Sutton},
  booktitle = {Machine Learning, Proceedings of the Seventh International Conference on Machine Learning, Austin, Texas, USA, June 21-23, 1990},
  title     = {Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming},
  year      = {1990},
  editor    = {Bruce W. Porter and Raymond J. Mooney},
  pages     = {216--224},
  publisher = {Morgan Kaufmann},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/icml/Sutton90.bib},
  doi       = {10.1016/b978-1-55860-141-3.50030-4},
  groups    = {Model-based},
}

@Article{Watkins1992a,
  author = {Christopher J. C. H. Watkins and Peter Dayan},
  title  = {Q-learning},
  year   = {1992},
  issn   = {0885-6125},
  pages  = {279-292},
  volume = {8},
  doi    = {10.1007/bf00992698},
  groups = {Q Learning},
}

@Book{Rummery1994,
  author    = {G. A. Rummery and M. Niranjan},
  publisher = {Univ. of Cambridge, Department of Engineering},
  title     = {On-line q-learning using connectionist systems},
  year      = {1994},
  address   = {Cambridge},
  number    = {166},
  series    = {CUED/F-INFENG/TR},
  comment   = {SARSA},
  groups    = {Q Learning},
  pagetotal = {20},
  ppn_gvk   = {181088983},
}

@Article{Bengio2013,
  author    = {Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
  journal   = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  title     = {Representation Learning: {A} Review and New Perspectives},
  year      = {2013},
  number    = {8},
  pages     = {1798--1828},
  volume    = {35},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/pami/BengioCV13.bib},
  doi       = {10.1109/TPAMI.2013.50},
  groups    = {Representation Learning},
}

@Article{Mnih2015,
  author    = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and Andreas Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  journal   = {Nat.},
  title     = {Human-level control through deep reinforcement learning},
  year      = {2015},
  number    = {7540},
  pages     = {529--533},
  volume    = {518},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/MnihKSRVBGRFOPB15.bib},
  comment   = {DQN
- PROBLEM:
	Efficient representation learning from high-dimensional sensory inputs
	Generalize well to new situations
	Leverage recent advances in deep neural networks to train artificial agents
- SOLUTION:
	DQN
- RESULTS:
	Using only pixels as input, surpasses all previous algos and achieve professional human level on 49 Atari games, with the same architecture},
  doi       = {10.1038/nature14236},
  groups    = {Deep Q-Network, Value based},
}

@Article{Silver2016,
  author    = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  journal   = {Nat.},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  year      = {2016},
  number    = {7587},
  pages     = {484--489},
  volume    = {529},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  doi       = {10.1038/nature16961},
  groups    = {Model based},
}

@Article{Silver2017,
  author    = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy P. Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  journal   = {Nat.},
  title     = {Mastering the game of Go without human knowledge},
  year      = {2017},
  number    = {7676},
  pages     = {354--359},
  volume    = {550},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/nature/SilverSSAHGHBLB17.bib},
  doi       = {10.1038/nature24270},
  groups    = {Model based},
}

@Article{Silver2017a,
  author        = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy P. Lillicrap and Karen Simonyan and Demis Hassabis},
  journal       = {CoRR},
  title         = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  year          = {2017},
  volume        = {abs/1712.01815},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  eprint        = {1712.01815},
  groups        = {Model based},
  url           = {http://arxiv.org/abs/1712.01815},
}

@Article{Lillicrap2015,
  author        = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  title         = {Continuous control with deep reinforcement learning},
  year          = {2015},
  month         = sep,
  abstract      = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  comment       = {DDPG},
  eprint        = {1509.02971},
  file          = {:Lillicrap2015 - Continuous Control with Deep Reinforcement Learning.pdf:PDF},
  groups        = {Policy based},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Fujimoto2018,
  author        = {Scott Fujimoto and Herke van Hoof and David Meger},
  title         = {Addressing Function Approximation Error in Actor-Critic Methods},
  year          = {2018},
  month         = feb,
  abstract      = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  comment       = {Twin Delayed DDPG},
  eprint        = {1802.09477},
  file          = {:Fujimoto2018 - Addressing Function Approximation Error in Actor Critic Methods.pdf:PDF},
  groups        = {Policy based, Actor Critic, DDPG},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
  readstatus    = {read},
}

@Article{Schulman2015,
  author        = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
  title         = {Trust Region Policy Optimization},
  year          = {2015},
  month         = feb,
  abstract      = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  comment       = {TRPO},
  eprint        = {1502.05477},
  file          = {:Schulman2015 - Trust Region Policy Optimization.pdf:PDF},
  groups        = {Policy based, Actor Critic, TRPO},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Schulman2017,
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  comment       = {PPO},
  eprint        = {1707.06347},
  file          = {:Schulman2017 - Proximal Policy Optimization Algorithms.pdf:PDF},
  groups        = {Policy based, Actor Critic, TRPO},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Haarnoja2018,
  author        = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  year          = {2018},
  month         = jan,
  abstract      = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  comment       = {Soft Actor Critic},
  eprint        = {1801.01290},
  file          = {:Haarnoja2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF},
  groups        = {Policy based, Actor Critic, Soft Actor Critic},
  keywords      = {cs.LG, cs.AI, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Mnih2013,
  author        = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  title         = {Playing Atari with Deep Reinforcement Learning},
  year          = {2013},
  month         = dec,
  abstract      = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Learning to control agents directly from high-dimensional sensory inputs
- SOLUTION:
	CNN to analyse raw pixels and output the value of states},
  eprint        = {1312.5602},
  file          = {:Mnih2013 - Playing Atari with Deep Reinforcement Learning.pdf:PDF},
  groups        = {Deep Q-Network},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@InProceedings{Hasselt2016,
  author    = {Hado van Hasselt and Arthur Guez and David Silver},
  booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {USA}},
  title     = {Deep Reinforcement Learning with Double Q-Learning},
  year      = {2016},
  editor    = {Dale Schuurmans and Michael P. Wellman},
  pages     = {2094--2100},
  publisher = {{AAAI} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aaai/HasseltGS16.bib},
  comment   = {- PROBLEM:
	(Deep)Q-learning overestimates action values sometimes
- SOLUTION:
	Double DQN: two models are train for selecting and evaluating actions
- RESULT: 
	Reduces overestimation problem
	Increases performance on several games},
  groups    = {Value based, Double DQN},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389},
}

@InProceedings{Schaul2016,
  author    = {Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  title     = {Prioritized Experience Replay},
  year      = {2016},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/SchaulQAS15.bib},
  comment   = {- PROBLEM: 
	Classic experience replay simply replays transitions at the same frequency that they were originally experienced
	Sample efficiency
- SOLUTION:
	PER: more frequently replay transitions with high expected learning progress (high TD-Error)
- RESULT:
	Learning sped up by a factor 2
	New sota performance on Atari benchmark},
  groups    = {PER},
  url       = {http://arxiv.org/abs/1511.05952},
}

@InProceedings{Wang2016,
  author    = {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado van Hasselt and Marc Lanctot and Nando de Freitas},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  year      = {2016},
  editor    = {Maria{-}Florina Balcan and Kilian Q. Weinberger},
  pages     = {1995--2003},
  publisher = {JMLR.org},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/icml/WangSHHLF16.bib},
  comment   = {- PROBLEM:
	Find better suited neural network architectures for model-free RL
- SOLUTION:
	Dueling DQN: Separately estimate state-value and advantage for each action
- RESULTS:
	Less variance in traing
	Increased performance},
  groups    = {Dueling DQN},
  url       = {http://proceedings.mlr.press/v48/wangf16.html},
}

@InProceedings{Bellemare2017,
  author    = {Marc G. Bellemare and Will Dabney and R{\'e}mi Munos},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {A Distributional Perspective on Reinforcement Learning},
  year      = {2017},
  address   = {International Convention Centre, Sydney, Australia},
  editor    = {Doina Precup and Yee Whye Teh},
  month     = {06--11 Aug},
  pages     = {449--458},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  abstract  = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  comment   = {- PROBLEM:
	When learning the expected (average) return of an action, we lose some information about possible outcomes
- SOLUTION:
	Learn a distribution of random return for each action (value distribution)
- RESULTS:
	Greatly increased performance on Atari benchmark
	Learning a value distribution matters, even when using a policy which aims to maximize expected return},
  file      = {:/home/doctoq/Desktop/THESE/papers/bellemare17a.pdf:PDF},
  groups    = {Value based, Distributional RL},
  pdf       = {http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
  url       = {http://proceedings.mlr.press/v70/bellemare17a.html},
}

@Article{Fortunato2017,
  author        = {Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
  title         = {Noisy Networks for Exploration},
  year          = {2017},
  month         = jun,
  abstract      = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
  archiveprefix = {arXiv},
  comment       = {- PROBLEM:
	Most exploration strategies rely on random perturbations of the agent's policy (e.g. epsilon-greedy, entropy regularisation) to induce novel behaviors, thus they are unlikely to lead to large-scale efficient exploration
- SOLUTION:
	Parametric noise added to model's weights to induce stochasticity in agent's policy and efficient exploration
	Noise parameters learned with gradient descent concurrently with model's weights
- RESULTS: 
	Way better results than with other exploration strategies},
  eprint        = {1706.10295},
  file          = {:Fortunato2017 - Noisy Networks for Exploration.pdf:PDF},
  groups        = {Value based, Noisy DQN},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Schrittwieser2019,
  author        = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
  title         = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  year          = {2019},
  month         = nov,
  abstract      = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  comment       = {Muzero},
  doi           = {10.1038/s41586-020-03051-4},
  eprint        = {1911.08265},
  file          = {:Schrittwieser2019 - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf:PDF},
  groups        = {Model based, Tree-based},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
  readstatus    = {read},
}

@Article{Hessel2017,
  author        = {Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
  title         = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  year          = {2017},
  month         = oct,
  abstract      = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archiveprefix = {arXiv},
  eprint        = {1710.02298},
  file          = {:/home/doctoq/Desktop/THESE/papers/Hessel2017 - Rainbow_ Combining Improvements in Deep Reinforcement Learning.pdf:PDF},
  groups        = {Value based},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Introduction\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:TD Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Q Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Policy Gradient\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Model-based\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Representation Learning\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Deep Reinforcement Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Value based\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Deep Q-Network\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Double DQN\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:PER\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Dueling DQN\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Distributional RL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Noisy DQN\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Model based\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Policy based\;0\;1\;0x8a8a8aff\;\;\;;
}
